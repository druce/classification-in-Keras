{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equity premium prediction with python and Keras\n",
    "\n",
    "* An exploratory data analysis and a demo of classification using Keras\n",
    "* Using [dataset](http://www.hec.unil.ch/agoyal/docs/PredictorData2016.xlsx) from [Prof. Amit Goyal](http://www.hec.unil.ch/agoyal/), attempt to predict quarterly equity outperformance based on fundamental data like interest rates, valuation.\n",
    "* Train binary classifier to predict whether next quarter's equity premium will be above or below long term average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1732\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from pprint import pprint\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#use a fixed seed for reproducibility\n",
    "seed = np.random.randint(10000, size=1)[0]\n",
    "print(seed)\n",
    "#seed = 6016\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>D12</th>\n",
       "      <th>E12</th>\n",
       "      <th>b.m</th>\n",
       "      <th>tbl</th>\n",
       "      <th>AAA</th>\n",
       "      <th>BAA</th>\n",
       "      <th>lty</th>\n",
       "      <th>ntis</th>\n",
       "      <th>infl</th>\n",
       "      <th>...</th>\n",
       "      <th>ltr.diff</th>\n",
       "      <th>corpr.diff</th>\n",
       "      <th>svar.diff</th>\n",
       "      <th>tbl.lagdiff</th>\n",
       "      <th>AAA.lagdiff</th>\n",
       "      <th>BAA.lagdiff</th>\n",
       "      <th>lty.lagdiff</th>\n",
       "      <th>ltr.lagdiff</th>\n",
       "      <th>corpr.lagdiff</th>\n",
       "      <th>EqPremResponse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.770</td>\n",
       "      <td>1.110</td>\n",
       "      <td>0.374689</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.050313</td>\n",
       "      <td>0.058913</td>\n",
       "      <td>0.037313</td>\n",
       "      <td>0.076475</td>\n",
       "      <td>-0.005713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012577</td>\n",
       "      <td>-0.004409</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>-0.0035</td>\n",
       "      <td>-0.011438</td>\n",
       "      <td>-0.012138</td>\n",
       "      <td>-0.012838</td>\n",
       "      <td>0.030167</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.790</td>\n",
       "      <td>1.177</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>0.044430</td>\n",
       "      <td>0.053030</td>\n",
       "      <td>0.031530</td>\n",
       "      <td>0.054364</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020049</td>\n",
       "      <td>-0.005564</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.016741</td>\n",
       "      <td>0.015941</td>\n",
       "      <td>0.015041</td>\n",
       "      <td>0.007711</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.810</td>\n",
       "      <td>1.245</td>\n",
       "      <td>0.370300</td>\n",
       "      <td>0.0392</td>\n",
       "      <td>0.045530</td>\n",
       "      <td>0.055330</td>\n",
       "      <td>0.032430</td>\n",
       "      <td>0.050299</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011019</td>\n",
       "      <td>-0.022453</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>-0.0013</td>\n",
       "      <td>-0.029141</td>\n",
       "      <td>-0.028641</td>\n",
       "      <td>-0.028441</td>\n",
       "      <td>-0.038657</td>\n",
       "      <td>-0.012230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.830</td>\n",
       "      <td>1.312</td>\n",
       "      <td>0.328166</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>0.040150</td>\n",
       "      <td>0.049950</td>\n",
       "      <td>0.028650</td>\n",
       "      <td>0.027980</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014276</td>\n",
       "      <td>0.019109</td>\n",
       "      <td>-0.003478</td>\n",
       "      <td>-0.0039</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>0.010261</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>0.011038</td>\n",
       "      <td>0.014918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.850</td>\n",
       "      <td>1.380</td>\n",
       "      <td>0.259667</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0.051744</td>\n",
       "      <td>0.061644</td>\n",
       "      <td>0.039644</td>\n",
       "      <td>0.063069</td>\n",
       "      <td>-0.005644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034818</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>-0.006513</td>\n",
       "      <td>-0.006713</td>\n",
       "      <td>-0.007113</td>\n",
       "      <td>0.012577</td>\n",
       "      <td>-0.004409</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>1.438</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.064443</td>\n",
       "      <td>0.075343</td>\n",
       "      <td>0.055143</td>\n",
       "      <td>0.079805</td>\n",
       "      <td>-0.017443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055116</td>\n",
       "      <td>-0.014560</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>-0.005882</td>\n",
       "      <td>-0.005882</td>\n",
       "      <td>-0.005782</td>\n",
       "      <td>-0.020049</td>\n",
       "      <td>-0.005564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>0.910</td>\n",
       "      <td>1.495</td>\n",
       "      <td>0.253581</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.088744</td>\n",
       "      <td>0.100444</td>\n",
       "      <td>0.077744</td>\n",
       "      <td>0.116197</td>\n",
       "      <td>-0.041044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060579</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>-0.011019</td>\n",
       "      <td>-0.022453</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>0.940</td>\n",
       "      <td>1.552</td>\n",
       "      <td>0.244868</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.113384</td>\n",
       "      <td>0.126584</td>\n",
       "      <td>0.102884</td>\n",
       "      <td>0.121390</td>\n",
       "      <td>-0.065384</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022683</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>-0.000348</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>-0.005380</td>\n",
       "      <td>-0.005380</td>\n",
       "      <td>-0.003780</td>\n",
       "      <td>-0.014276</td>\n",
       "      <td>0.019109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>0.970</td>\n",
       "      <td>1.610</td>\n",
       "      <td>0.338458</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.125698</td>\n",
       "      <td>0.138498</td>\n",
       "      <td>0.112998</td>\n",
       "      <td>0.163522</td>\n",
       "      <td>-0.078998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053953</td>\n",
       "      <td>0.017375</td>\n",
       "      <td>0.092115</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>0.011594</td>\n",
       "      <td>0.011694</td>\n",
       "      <td>0.010994</td>\n",
       "      <td>0.034818</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>0.973</td>\n",
       "      <td>1.450</td>\n",
       "      <td>0.319119</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.151332</td>\n",
       "      <td>0.162432</td>\n",
       "      <td>0.138632</td>\n",
       "      <td>0.145496</td>\n",
       "      <td>-0.105132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037758</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>-0.095509</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>-0.055116</td>\n",
       "      <td>-0.014560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.290</td>\n",
       "      <td>0.403375</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.145550</td>\n",
       "      <td>0.157650</td>\n",
       "      <td>0.132650</td>\n",
       "      <td>0.131001</td>\n",
       "      <td>-0.099850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>-0.001820</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.024301</td>\n",
       "      <td>0.025101</td>\n",
       "      <td>0.022601</td>\n",
       "      <td>0.060579</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>0.978</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.445583</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.140596</td>\n",
       "      <td>0.152896</td>\n",
       "      <td>0.128796</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>-0.096396</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005301</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>-0.008366</td>\n",
       "      <td>-0.0022</td>\n",
       "      <td>0.024640</td>\n",
       "      <td>0.026140</td>\n",
       "      <td>0.025140</td>\n",
       "      <td>-0.022683</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.554745</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.151636</td>\n",
       "      <td>0.173536</td>\n",
       "      <td>0.139436</td>\n",
       "      <td>0.113886</td>\n",
       "      <td>-0.106436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011479</td>\n",
       "      <td>-0.035129</td>\n",
       "      <td>0.015494</td>\n",
       "      <td>-0.0155</td>\n",
       "      <td>0.012314</td>\n",
       "      <td>0.011914</td>\n",
       "      <td>0.010114</td>\n",
       "      <td>0.053953</td>\n",
       "      <td>0.017375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.529125</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.146857</td>\n",
       "      <td>0.167357</td>\n",
       "      <td>0.136157</td>\n",
       "      <td>0.073969</td>\n",
       "      <td>-0.102957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.041738</td>\n",
       "      <td>-0.015925</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>0.025634</td>\n",
       "      <td>0.023934</td>\n",
       "      <td>0.025634</td>\n",
       "      <td>-0.037758</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.607271</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.154640</td>\n",
       "      <td>0.184640</td>\n",
       "      <td>0.142940</td>\n",
       "      <td>0.063746</td>\n",
       "      <td>-0.111040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016975</td>\n",
       "      <td>-0.011399</td>\n",
       "      <td>0.024675</td>\n",
       "      <td>-0.0106</td>\n",
       "      <td>-0.005782</td>\n",
       "      <td>-0.004782</td>\n",
       "      <td>-0.005982</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>-0.001820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.944002</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.152261</td>\n",
       "      <td>0.187461</td>\n",
       "      <td>0.142061</td>\n",
       "      <td>0.030352</td>\n",
       "      <td>-0.106761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054655</td>\n",
       "      <td>-0.020497</td>\n",
       "      <td>-0.001913</td>\n",
       "      <td>-0.0012</td>\n",
       "      <td>-0.004954</td>\n",
       "      <td>-0.004754</td>\n",
       "      <td>-0.003854</td>\n",
       "      <td>-0.005301</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.610</td>\n",
       "      <td>1.170732</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.157033</td>\n",
       "      <td>0.208033</td>\n",
       "      <td>0.144533</td>\n",
       "      <td>-0.012944</td>\n",
       "      <td>-0.103833</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020700</td>\n",
       "      <td>-0.086552</td>\n",
       "      <td>0.040092</td>\n",
       "      <td>-0.0029</td>\n",
       "      <td>0.011040</td>\n",
       "      <td>0.020640</td>\n",
       "      <td>0.010640</td>\n",
       "      <td>-0.011479</td>\n",
       "      <td>-0.035129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.560</td>\n",
       "      <td>1.185862</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.117125</td>\n",
       "      <td>0.155625</td>\n",
       "      <td>0.104325</td>\n",
       "      <td>-0.008416</td>\n",
       "      <td>-0.067325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094680</td>\n",
       "      <td>0.087251</td>\n",
       "      <td>-0.017631</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.004780</td>\n",
       "      <td>-0.006180</td>\n",
       "      <td>-0.003280</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.041738</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.510</td>\n",
       "      <td>2.028478</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.067349</td>\n",
       "      <td>0.123249</td>\n",
       "      <td>0.047949</td>\n",
       "      <td>-0.040689</td>\n",
       "      <td>-0.013249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>-0.013678</td>\n",
       "      <td>0.019837</td>\n",
       "      <td>-0.0083</td>\n",
       "      <td>0.007783</td>\n",
       "      <td>0.017283</td>\n",
       "      <td>0.006783</td>\n",
       "      <td>0.016975</td>\n",
       "      <td>-0.011399</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>22</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.460</td>\n",
       "      <td>1.214366</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.037861</td>\n",
       "      <td>0.066961</td>\n",
       "      <td>0.022761</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.009139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007163</td>\n",
       "      <td>0.087617</td>\n",
       "      <td>0.026091</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.002379</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>-0.000879</td>\n",
       "      <td>-0.054655</td>\n",
       "      <td>-0.020497</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>23</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.410</td>\n",
       "      <td>1.442084</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>-0.008982</td>\n",
       "      <td>0.029318</td>\n",
       "      <td>-0.023382</td>\n",
       "      <td>-0.005032</td>\n",
       "      <td>0.054882</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039776</td>\n",
       "      <td>-0.050776</td>\n",
       "      <td>-0.034244</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>0.020572</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>-0.020700</td>\n",
       "      <td>-0.086552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>24</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.418</td>\n",
       "      <td>1.476534</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>-0.007665</td>\n",
       "      <td>0.034635</td>\n",
       "      <td>-0.022365</td>\n",
       "      <td>-0.021883</td>\n",
       "      <td>0.054465</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016406</td>\n",
       "      <td>-0.024622</td>\n",
       "      <td>-0.010147</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>-0.039908</td>\n",
       "      <td>-0.052408</td>\n",
       "      <td>-0.040208</td>\n",
       "      <td>0.094680</td>\n",
       "      <td>0.087251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.833503</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.014580</td>\n",
       "      <td>0.040680</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.030020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033930</td>\n",
       "      <td>0.064430</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>-0.0191</td>\n",
       "      <td>-0.049776</td>\n",
       "      <td>-0.032376</td>\n",
       "      <td>-0.056376</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>-0.013678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>26</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.867997</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.028286</td>\n",
       "      <td>0.057386</td>\n",
       "      <td>0.015486</td>\n",
       "      <td>-0.000458</td>\n",
       "      <td>0.015314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027139</td>\n",
       "      <td>-0.044554</td>\n",
       "      <td>-0.001937</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>-0.029488</td>\n",
       "      <td>-0.056288</td>\n",
       "      <td>-0.025188</td>\n",
       "      <td>0.007163</td>\n",
       "      <td>0.087617</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>27</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.829026</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>0.047374</td>\n",
       "      <td>0.003474</td>\n",
       "      <td>0.006196</td>\n",
       "      <td>0.030126</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039893</td>\n",
       "      <td>-0.019850</td>\n",
       "      <td>-0.016149</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.046844</td>\n",
       "      <td>-0.037644</td>\n",
       "      <td>-0.046144</td>\n",
       "      <td>-0.039776</td>\n",
       "      <td>-0.050776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>28</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.802512</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.018692</td>\n",
       "      <td>0.039992</td>\n",
       "      <td>0.008092</td>\n",
       "      <td>0.009606</td>\n",
       "      <td>0.022608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089273</td>\n",
       "      <td>0.055872</td>\n",
       "      <td>-0.024269</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.005318</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>-0.016406</td>\n",
       "      <td>-0.024622</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>29</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.840731</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.031618</td>\n",
       "      <td>0.052918</td>\n",
       "      <td>0.021218</td>\n",
       "      <td>0.005983</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021640</td>\n",
       "      <td>-0.024534</td>\n",
       "      <td>-0.002422</td>\n",
       "      <td>-0.0127</td>\n",
       "      <td>0.022245</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>0.022945</td>\n",
       "      <td>0.033930</td>\n",
       "      <td>0.064430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>30</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.870364</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.009913</td>\n",
       "      <td>0.036013</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.019441</td>\n",
       "      <td>0.029687</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055071</td>\n",
       "      <td>-0.032337</td>\n",
       "      <td>0.006707</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>0.013706</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>0.014906</td>\n",
       "      <td>-0.027139</td>\n",
       "      <td>-0.044554</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>31</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.773741</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.038047</td>\n",
       "      <td>0.062247</td>\n",
       "      <td>0.029247</td>\n",
       "      <td>0.020815</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055746</td>\n",
       "      <td>0.030302</td>\n",
       "      <td>-0.013871</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>-0.013412</td>\n",
       "      <td>-0.010012</td>\n",
       "      <td>-0.012012</td>\n",
       "      <td>-0.039893</td>\n",
       "      <td>-0.019850</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>32</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.800754</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.029348</td>\n",
       "      <td>0.054648</td>\n",
       "      <td>0.020048</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.007352</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001633</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>-0.007381</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>0.089273</td>\n",
       "      <td>0.055872</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>325</td>\n",
       "      <td>28.712</td>\n",
       "      <td>51.370</td>\n",
       "      <td>0.274491</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.068793</td>\n",
       "      <td>0.082693</td>\n",
       "      <td>0.057993</td>\n",
       "      <td>-0.048432</td>\n",
       "      <td>-0.011993</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057695</td>\n",
       "      <td>-0.013557</td>\n",
       "      <td>-0.006426</td>\n",
       "      <td>-0.0033</td>\n",
       "      <td>0.005709</td>\n",
       "      <td>0.005109</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>-0.028789</td>\n",
       "      <td>-0.018601</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>326</td>\n",
       "      <td>28.854</td>\n",
       "      <td>45.950</td>\n",
       "      <td>0.287124</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.029483</td>\n",
       "      <td>0.046083</td>\n",
       "      <td>0.017283</td>\n",
       "      <td>-0.050348</td>\n",
       "      <td>0.027017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056794</td>\n",
       "      <td>-0.060487</td>\n",
       "      <td>0.023719</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>0.046018</td>\n",
       "      <td>0.045418</td>\n",
       "      <td>0.043318</td>\n",
       "      <td>0.070788</td>\n",
       "      <td>0.044402</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>327</td>\n",
       "      <td>28.387</td>\n",
       "      <td>14.880</td>\n",
       "      <td>0.354984</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.027496</td>\n",
       "      <td>0.061296</td>\n",
       "      <td>0.007296</td>\n",
       "      <td>-0.024793</td>\n",
       "      <td>0.023004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173807</td>\n",
       "      <td>0.318911</td>\n",
       "      <td>0.082747</td>\n",
       "      <td>-0.0089</td>\n",
       "      <td>0.002324</td>\n",
       "      <td>0.005424</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.009819</td>\n",
       "      <td>-0.005996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>328</td>\n",
       "      <td>27.255</td>\n",
       "      <td>6.860</td>\n",
       "      <td>0.446141</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.044489</td>\n",
       "      <td>0.073689</td>\n",
       "      <td>0.024989</td>\n",
       "      <td>-0.037790</td>\n",
       "      <td>0.010511</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267683</td>\n",
       "      <td>-0.357943</td>\n",
       "      <td>-0.072139</td>\n",
       "      <td>-0.0174</td>\n",
       "      <td>0.010932</td>\n",
       "      <td>0.013132</td>\n",
       "      <td>0.008932</td>\n",
       "      <td>-0.025959</td>\n",
       "      <td>-0.030897</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>329</td>\n",
       "      <td>25.594</td>\n",
       "      <td>7.510</td>\n",
       "      <td>0.401876</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.044694</td>\n",
       "      <td>0.063594</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>-0.022546</td>\n",
       "      <td>0.011406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019728</td>\n",
       "      <td>0.206711</td>\n",
       "      <td>-0.024892</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>-0.057695</td>\n",
       "      <td>-0.013557</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>330</td>\n",
       "      <td>23.901</td>\n",
       "      <td>12.540</td>\n",
       "      <td>0.349521</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.036414</td>\n",
       "      <td>0.048214</td>\n",
       "      <td>0.025414</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>0.014886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123606</td>\n",
       "      <td>0.028493</td>\n",
       "      <td>-0.009543</td>\n",
       "      <td>-0.0073</td>\n",
       "      <td>-0.039310</td>\n",
       "      <td>-0.036610</td>\n",
       "      <td>-0.040710</td>\n",
       "      <td>0.056794</td>\n",
       "      <td>-0.060487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>331</td>\n",
       "      <td>22.405</td>\n",
       "      <td>50.970</td>\n",
       "      <td>0.325531</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.025939</td>\n",
       "      <td>0.037039</td>\n",
       "      <td>0.019139</td>\n",
       "      <td>0.010541</td>\n",
       "      <td>0.026661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098336</td>\n",
       "      <td>-0.132506</td>\n",
       "      <td>-0.001029</td>\n",
       "      <td>-0.0110</td>\n",
       "      <td>-0.001987</td>\n",
       "      <td>0.015213</td>\n",
       "      <td>-0.009987</td>\n",
       "      <td>0.173807</td>\n",
       "      <td>0.318911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>332</td>\n",
       "      <td>21.904</td>\n",
       "      <td>60.930</td>\n",
       "      <td>0.381878</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.017483</td>\n",
       "      <td>0.027483</td>\n",
       "      <td>0.010583</td>\n",
       "      <td>0.013861</td>\n",
       "      <td>0.035217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066447</td>\n",
       "      <td>0.039697</td>\n",
       "      <td>-0.001834</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.016994</td>\n",
       "      <td>0.012394</td>\n",
       "      <td>0.017694</td>\n",
       "      <td>-0.267683</td>\n",
       "      <td>-0.357943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>333</td>\n",
       "      <td>22.037</td>\n",
       "      <td>67.100</td>\n",
       "      <td>0.424177</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.010587</td>\n",
       "      <td>0.024087</td>\n",
       "      <td>-0.000613</td>\n",
       "      <td>0.018713</td>\n",
       "      <td>0.038213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112136</td>\n",
       "      <td>0.065943</td>\n",
       "      <td>0.010495</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>-0.010095</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>-0.019728</td>\n",
       "      <td>0.206711</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>334</td>\n",
       "      <td>22.353</td>\n",
       "      <td>71.860</td>\n",
       "      <td>0.384306</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.015839</td>\n",
       "      <td>0.027139</td>\n",
       "      <td>0.004639</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.029461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066957</td>\n",
       "      <td>-0.034212</td>\n",
       "      <td>-0.007103</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.008280</td>\n",
       "      <td>-0.015380</td>\n",
       "      <td>-0.006080</td>\n",
       "      <td>0.123606</td>\n",
       "      <td>0.028493</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>335</td>\n",
       "      <td>22.729</td>\n",
       "      <td>77.350</td>\n",
       "      <td>0.358100</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.023819</td>\n",
       "      <td>0.034619</td>\n",
       "      <td>0.015019</td>\n",
       "      <td>0.012555</td>\n",
       "      <td>0.026381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138471</td>\n",
       "      <td>-0.079181</td>\n",
       "      <td>-0.004741</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.010475</td>\n",
       "      <td>-0.011175</td>\n",
       "      <td>-0.006275</td>\n",
       "      <td>-0.098336</td>\n",
       "      <td>-0.132506</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>336</td>\n",
       "      <td>23.431</td>\n",
       "      <td>81.310</td>\n",
       "      <td>0.354270</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.034635</td>\n",
       "      <td>0.043635</td>\n",
       "      <td>0.026235</td>\n",
       "      <td>0.012445</td>\n",
       "      <td>0.016665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072980</td>\n",
       "      <td>0.017832</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>-0.008456</td>\n",
       "      <td>-0.009556</td>\n",
       "      <td>-0.008556</td>\n",
       "      <td>0.066447</td>\n",
       "      <td>0.039697</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>337</td>\n",
       "      <td>24.341</td>\n",
       "      <td>83.870</td>\n",
       "      <td>0.351570</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.029999</td>\n",
       "      <td>0.037599</td>\n",
       "      <td>0.020499</td>\n",
       "      <td>0.011718</td>\n",
       "      <td>0.019901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046319</td>\n",
       "      <td>0.039739</td>\n",
       "      <td>-0.000411</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>-0.006895</td>\n",
       "      <td>-0.003395</td>\n",
       "      <td>-0.011195</td>\n",
       "      <td>0.112136</td>\n",
       "      <td>0.065943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>338</td>\n",
       "      <td>25.181</td>\n",
       "      <td>86.980</td>\n",
       "      <td>0.399923</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.023439</td>\n",
       "      <td>0.035239</td>\n",
       "      <td>0.009039</td>\n",
       "      <td>0.013201</td>\n",
       "      <td>0.017461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174531</td>\n",
       "      <td>0.105940</td>\n",
       "      <td>0.024103</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.005252</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>0.005252</td>\n",
       "      <td>-0.066957</td>\n",
       "      <td>-0.034212</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>339</td>\n",
       "      <td>26.425</td>\n",
       "      <td>86.950</td>\n",
       "      <td>0.357233</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.024508</td>\n",
       "      <td>0.037708</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>-0.006505</td>\n",
       "      <td>0.014792</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191170</td>\n",
       "      <td>-0.110793</td>\n",
       "      <td>-0.010124</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>-0.138471</td>\n",
       "      <td>-0.079181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>340</td>\n",
       "      <td>27.353</td>\n",
       "      <td>88.540</td>\n",
       "      <td>0.346182</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.022343</td>\n",
       "      <td>0.034743</td>\n",
       "      <td>0.011443</td>\n",
       "      <td>-0.013792</td>\n",
       "      <td>0.017557</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069581</td>\n",
       "      <td>-0.029160</td>\n",
       "      <td>-0.015417</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>0.010816</td>\n",
       "      <td>0.009016</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.072980</td>\n",
       "      <td>0.017832</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>341</td>\n",
       "      <td>28.320</td>\n",
       "      <td>87.920</td>\n",
       "      <td>0.355104</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.024486</td>\n",
       "      <td>0.038286</td>\n",
       "      <td>0.010586</td>\n",
       "      <td>-0.019832</td>\n",
       "      <td>0.011914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141781</td>\n",
       "      <td>0.073003</td>\n",
       "      <td>0.004134</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.004636</td>\n",
       "      <td>-0.006036</td>\n",
       "      <td>-0.005736</td>\n",
       "      <td>0.046319</td>\n",
       "      <td>0.039739</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>342</td>\n",
       "      <td>29.590</td>\n",
       "      <td>86.500</td>\n",
       "      <td>0.340383</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.019880</td>\n",
       "      <td>0.033380</td>\n",
       "      <td>0.007580</td>\n",
       "      <td>-0.018256</td>\n",
       "      <td>0.015020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089890</td>\n",
       "      <td>-0.029066</td>\n",
       "      <td>-0.003236</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>-0.006559</td>\n",
       "      <td>-0.002359</td>\n",
       "      <td>-0.011459</td>\n",
       "      <td>0.174531</td>\n",
       "      <td>0.105940</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>343</td>\n",
       "      <td>31.247</td>\n",
       "      <td>86.510</td>\n",
       "      <td>0.349032</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.021375</td>\n",
       "      <td>0.031175</td>\n",
       "      <td>0.008975</td>\n",
       "      <td>-0.012266</td>\n",
       "      <td>0.015125</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010356</td>\n",
       "      <td>-0.033143</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>-0.191170</td>\n",
       "      <td>-0.110793</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>344</td>\n",
       "      <td>32.112</td>\n",
       "      <td>87.700</td>\n",
       "      <td>0.346130</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.018640</td>\n",
       "      <td>0.027840</td>\n",
       "      <td>0.007740</td>\n",
       "      <td>-0.009548</td>\n",
       "      <td>0.020660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020759</td>\n",
       "      <td>-0.028992</td>\n",
       "      <td>-0.001230</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>-0.002165</td>\n",
       "      <td>-0.002965</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>-0.069581</td>\n",
       "      <td>-0.029160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>345</td>\n",
       "      <td>33.266</td>\n",
       "      <td>90.950</td>\n",
       "      <td>0.338444</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.026111</td>\n",
       "      <td>0.035311</td>\n",
       "      <td>0.016311</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026954</td>\n",
       "      <td>-0.032857</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>-0.000856</td>\n",
       "      <td>0.141781</td>\n",
       "      <td>0.073003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>346</td>\n",
       "      <td>34.404</td>\n",
       "      <td>94.370</td>\n",
       "      <td>0.333521</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.038637</td>\n",
       "      <td>0.046937</td>\n",
       "      <td>0.026437</td>\n",
       "      <td>0.010142</td>\n",
       "      <td>0.007763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036078</td>\n",
       "      <td>0.053978</td>\n",
       "      <td>-0.002871</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.004607</td>\n",
       "      <td>-0.004907</td>\n",
       "      <td>-0.003007</td>\n",
       "      <td>-0.089890</td>\n",
       "      <td>-0.029066</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>347</td>\n",
       "      <td>34.992</td>\n",
       "      <td>100.200</td>\n",
       "      <td>0.304408</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.046791</td>\n",
       "      <td>0.054391</td>\n",
       "      <td>0.037291</td>\n",
       "      <td>0.012184</td>\n",
       "      <td>-0.000591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012456</td>\n",
       "      <td>0.015450</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>-0.002205</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>-0.010356</td>\n",
       "      <td>-0.033143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>348</td>\n",
       "      <td>36.228</td>\n",
       "      <td>100.850</td>\n",
       "      <td>0.350616</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.042398</td>\n",
       "      <td>0.049198</td>\n",
       "      <td>0.031698</td>\n",
       "      <td>0.016464</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100872</td>\n",
       "      <td>0.044448</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.002735</td>\n",
       "      <td>-0.003335</td>\n",
       "      <td>-0.001235</td>\n",
       "      <td>-0.020759</td>\n",
       "      <td>-0.028992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>349</td>\n",
       "      <td>37.381</td>\n",
       "      <td>103.120</td>\n",
       "      <td>0.342928</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.042693</td>\n",
       "      <td>0.048193</td>\n",
       "      <td>0.030893</td>\n",
       "      <td>0.013931</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025411</td>\n",
       "      <td>-0.019798</td>\n",
       "      <td>-0.001226</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>0.008571</td>\n",
       "      <td>-0.026954</td>\n",
       "      <td>-0.032857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>350</td>\n",
       "      <td>38.495</td>\n",
       "      <td>105.960</td>\n",
       "      <td>0.338576</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.033737</td>\n",
       "      <td>0.040637</td>\n",
       "      <td>0.022137</td>\n",
       "      <td>0.008025</td>\n",
       "      <td>0.007363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018806</td>\n",
       "      <td>-0.027218</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>0.012526</td>\n",
       "      <td>0.011626</td>\n",
       "      <td>0.010126</td>\n",
       "      <td>0.036078</td>\n",
       "      <td>0.053978</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>351</td>\n",
       "      <td>39.443</td>\n",
       "      <td>102.310</td>\n",
       "      <td>0.323756</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.029309</td>\n",
       "      <td>0.038809</td>\n",
       "      <td>0.015409</td>\n",
       "      <td>0.005642</td>\n",
       "      <td>0.008591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065100</td>\n",
       "      <td>0.049271</td>\n",
       "      <td>0.002871</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.008154</td>\n",
       "      <td>0.007454</td>\n",
       "      <td>0.010854</td>\n",
       "      <td>-0.012456</td>\n",
       "      <td>0.015450</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>352</td>\n",
       "      <td>40.807</td>\n",
       "      <td>99.250</td>\n",
       "      <td>0.307454</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.026358</td>\n",
       "      <td>0.035358</td>\n",
       "      <td>0.012258</td>\n",
       "      <td>-0.006900</td>\n",
       "      <td>0.010042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061386</td>\n",
       "      <td>-0.027291</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>-0.004393</td>\n",
       "      <td>-0.005193</td>\n",
       "      <td>-0.005593</td>\n",
       "      <td>0.100872</td>\n",
       "      <td>0.044448</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>353</td>\n",
       "      <td>41.742</td>\n",
       "      <td>94.910</td>\n",
       "      <td>0.310187</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.027253</td>\n",
       "      <td>0.036653</td>\n",
       "      <td>0.011453</td>\n",
       "      <td>-0.008102</td>\n",
       "      <td>0.014647</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097891</td>\n",
       "      <td>-0.104827</td>\n",
       "      <td>-0.002247</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>-0.001005</td>\n",
       "      <td>-0.000805</td>\n",
       "      <td>-0.025411</td>\n",
       "      <td>-0.019798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>354</td>\n",
       "      <td>42.510</td>\n",
       "      <td>90.660</td>\n",
       "      <td>0.335612</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.020069</td>\n",
       "      <td>0.032769</td>\n",
       "      <td>0.003369</td>\n",
       "      <td>-0.012910</td>\n",
       "      <td>0.020631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121229</td>\n",
       "      <td>0.103460</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>-0.008956</td>\n",
       "      <td>-0.007556</td>\n",
       "      <td>-0.008756</td>\n",
       "      <td>-0.018806</td>\n",
       "      <td>-0.027218</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0     D12      E12       b.m     tbl       AAA       BAA  \\\n",
       "0             3   0.770    1.110  0.374689  0.0317  0.050313  0.058913   \n",
       "1             4   0.790    1.177  0.363255  0.0327  0.044430  0.053030   \n",
       "2             5   0.810    1.245  0.370300  0.0392  0.045530  0.055330   \n",
       "3             6   0.830    1.312  0.328166  0.0457  0.040150  0.049950   \n",
       "4             7   0.850    1.380  0.259667  0.0426  0.051744  0.061644   \n",
       "5             8   0.880    1.438  0.272300  0.0460  0.064443  0.075343   \n",
       "6             9   0.910    1.495  0.253581  0.0480  0.088744  0.100444   \n",
       "7            10   0.940    1.552  0.244868  0.0458  0.113384  0.126584   \n",
       "8            11   0.970    1.610  0.338458  0.0303  0.125698  0.138498   \n",
       "9            12   0.973    1.450  0.319119  0.0295  0.151332  0.162432   \n",
       "10           13   0.975    1.290  0.403375  0.0189  0.145550  0.157650   \n",
       "11           14   0.978    1.130  0.445583  0.0177  0.140596  0.152896   \n",
       "12           15   0.980    0.970  0.554745  0.0148  0.151636  0.173536   \n",
       "13           16   0.940    0.880  0.529125  0.0138  0.146857  0.167357   \n",
       "14           17   0.900    0.790  0.607271  0.0055  0.154640  0.184640   \n",
       "15           18   0.860    0.700  0.944002  0.0045  0.152261  0.187461   \n",
       "16           19   0.820    0.610  1.170732  0.0241  0.157033  0.208033   \n",
       "17           20   0.740    0.560  1.185862  0.0225  0.117125  0.155625   \n",
       "18           21   0.660    0.510  2.028478  0.0034  0.067349  0.123249   \n",
       "19           22   0.580    0.460  1.214366  0.0003  0.037861  0.066961   \n",
       "20           23   0.500    0.410  1.442084  0.0004 -0.008982  0.029318   \n",
       "21           24   0.485    0.418  1.476534  0.0134 -0.007665  0.034635   \n",
       "22           25   0.470    0.425  0.833503  0.0007  0.014580  0.040680   \n",
       "23           26   0.455    0.433  0.867997  0.0004  0.028286  0.057386   \n",
       "24           27   0.440    0.440  0.829026  0.0029  0.014874  0.047374   \n",
       "25           28   0.443    0.453  0.802512  0.0024  0.018692  0.039992   \n",
       "26           29   0.445    0.465  0.840731  0.0015  0.031618  0.052918   \n",
       "27           30   0.448    0.478  0.870364  0.0021  0.009913  0.036013   \n",
       "28           31   0.450    0.490  0.773741  0.0023  0.038047  0.062247   \n",
       "29           32   0.450    0.730  0.800754  0.0015  0.029348  0.054648   \n",
       "..          ...     ...      ...       ...     ...       ...       ...   \n",
       "322         325  28.712   51.370  0.274491  0.0186  0.068793  0.082693   \n",
       "323         326  28.854   45.950  0.287124  0.0113  0.029483  0.046083   \n",
       "324         327  28.387   14.880  0.354984  0.0003  0.027496  0.061296   \n",
       "325         328  27.255    6.860  0.446141  0.0021  0.044489  0.073689   \n",
       "326         329  25.594    7.510  0.401876  0.0018  0.044694  0.063594   \n",
       "327         330  23.901   12.540  0.349521  0.0012  0.036414  0.048214   \n",
       "328         331  22.405   50.970  0.325531  0.0005  0.025939  0.037039   \n",
       "329         332  21.904   60.930  0.381878  0.0015  0.017483  0.027483   \n",
       "330         333  22.037   67.100  0.424177  0.0012  0.010587  0.024087   \n",
       "331         334  22.353   71.860  0.384306  0.0015  0.015839  0.027139   \n",
       "332         335  22.729   77.350  0.358100  0.0014  0.023819  0.034619   \n",
       "333         336  23.431   81.310  0.354270  0.0010  0.034635  0.043635   \n",
       "334         337  24.341   83.870  0.351570  0.0004  0.029999  0.037599   \n",
       "335         338  25.181   86.980  0.399923  0.0001  0.023439  0.035239   \n",
       "336         339  26.425   86.950  0.357233  0.0001  0.024508  0.037708   \n",
       "337         340  27.353   88.540  0.346182  0.0008  0.022343  0.034743   \n",
       "338         341  28.320   87.920  0.355104  0.0009  0.024486  0.038286   \n",
       "339         342  29.590   86.500  0.340383  0.0011  0.019880  0.033380   \n",
       "340         343  31.247   86.510  0.349032  0.0007  0.021375  0.031175   \n",
       "341         344  32.112   87.700  0.346130  0.0009  0.018640  0.027840   \n",
       "342         345  33.266   90.950  0.338444  0.0005  0.026111  0.035311   \n",
       "343         346  34.404   94.370  0.333521  0.0002  0.038637  0.046937   \n",
       "344         347  34.992  100.200  0.304408  0.0007  0.046791  0.054391   \n",
       "345         348  36.228  100.850  0.350616  0.0005  0.042398  0.049198   \n",
       "346         349  37.381  103.120  0.342928  0.0004  0.042693  0.048193   \n",
       "347         350  38.495  105.960  0.338576  0.0002  0.033737  0.040637   \n",
       "348         351  39.443  102.310  0.323756  0.0003  0.029309  0.038809   \n",
       "349         352  40.807   99.250  0.307454  0.0003  0.026358  0.035358   \n",
       "350         353  41.742   94.910  0.310187  0.0002  0.027253  0.036653   \n",
       "351         354  42.510   90.660  0.335612  0.0002  0.020069  0.032769   \n",
       "\n",
       "          lty      ntis      infl       ...        ltr.diff  corpr.diff  \\\n",
       "0    0.037313  0.076475 -0.005713       ...        0.012577   -0.004409   \n",
       "1    0.031530  0.054364  0.000170       ...       -0.020049   -0.005564   \n",
       "2    0.032430  0.050299  0.000170       ...       -0.011019   -0.022453   \n",
       "3    0.028650  0.027980  0.005950       ...       -0.014276    0.019109   \n",
       "4    0.039644  0.063069 -0.005644       ...        0.034818    0.002796   \n",
       "5    0.055143  0.079805 -0.017443       ...       -0.055116   -0.014560   \n",
       "6    0.077744  0.116197 -0.041044       ...        0.060579    0.003230   \n",
       "7    0.102884  0.121390 -0.065384       ...       -0.022683    0.005639   \n",
       "8    0.112998  0.163522 -0.078998       ...        0.053953    0.017375   \n",
       "9    0.138632  0.145496 -0.105132       ...       -0.037758    0.002332   \n",
       "10   0.132650  0.131001 -0.099850       ...        0.001955   -0.001820   \n",
       "11   0.128796  0.127200 -0.096396       ...       -0.005301    0.004981   \n",
       "12   0.139436  0.113886 -0.106436       ...       -0.011479   -0.035129   \n",
       "13   0.136157  0.073969 -0.102957       ...        0.005998    0.041738   \n",
       "14   0.142940  0.063746 -0.111040       ...        0.016975   -0.011399   \n",
       "15   0.142061  0.030352 -0.106761       ...       -0.054655   -0.020497   \n",
       "16   0.144533 -0.012944 -0.103833       ...       -0.020700   -0.086552   \n",
       "17   0.104325 -0.008416 -0.067325       ...        0.094680    0.087251   \n",
       "18   0.047949 -0.040689 -0.013249       ...        0.004268   -0.013678   \n",
       "19   0.022761  0.000705  0.009139       ...        0.007163    0.087617   \n",
       "20  -0.023382 -0.005032  0.054882       ...       -0.039776   -0.050776   \n",
       "21  -0.022365 -0.021883  0.054465       ...       -0.016406   -0.024622   \n",
       "22   0.000580  0.000762  0.030020       ...        0.033930    0.064430   \n",
       "23   0.015486 -0.000458  0.015314       ...       -0.027139   -0.044554   \n",
       "24   0.003474  0.006196  0.030126       ...       -0.039893   -0.019850   \n",
       "25   0.008092  0.009606  0.022608       ...        0.089273    0.055872   \n",
       "26   0.021218  0.005983  0.007682       ...       -0.021640   -0.024534   \n",
       "27   0.001313  0.019441  0.029687       ...       -0.055071   -0.032337   \n",
       "28   0.029247  0.020815  0.000053       ...        0.055746    0.030302   \n",
       "29   0.020048  0.012048  0.007352       ...       -0.001633    0.006384   \n",
       "..        ...       ...       ...       ...             ...         ...   \n",
       "322  0.057993 -0.048432 -0.011993       ...       -0.057695   -0.013557   \n",
       "323  0.017283 -0.050348  0.027017       ...        0.056794   -0.060487   \n",
       "324  0.007296 -0.024793  0.023004       ...        0.173807    0.318911   \n",
       "325  0.024989 -0.037790  0.010511       ...       -0.267683   -0.357943   \n",
       "326  0.031494 -0.022546  0.011406       ...       -0.019728    0.206711   \n",
       "327  0.025414 -0.000070  0.014886       ...        0.123606    0.028493   \n",
       "328  0.019139  0.010541  0.026661       ...       -0.098336   -0.132506   \n",
       "329  0.010583  0.013861  0.035217       ...        0.066447    0.039697   \n",
       "330 -0.000613  0.018713  0.038213       ...        0.112136    0.065943   \n",
       "331  0.004639  0.003227  0.029461       ...       -0.066957   -0.034212   \n",
       "332  0.015019  0.012555  0.026381       ...       -0.138471   -0.079181   \n",
       "333  0.026235  0.012445  0.016665       ...        0.072980    0.017832   \n",
       "334  0.020499  0.011718  0.019901       ...        0.046319    0.039739   \n",
       "335  0.009039  0.013201  0.017461       ...        0.174531    0.105940   \n",
       "336  0.010008 -0.006505  0.014792       ...       -0.191170   -0.110793   \n",
       "337  0.011443 -0.013792  0.017557       ...       -0.069581   -0.029160   \n",
       "338  0.010586 -0.019832  0.011914       ...        0.141781    0.073003   \n",
       "339  0.007580 -0.018256  0.015020       ...       -0.089890   -0.029066   \n",
       "340  0.008975 -0.012266  0.015125       ...       -0.010356   -0.033143   \n",
       "341  0.007740 -0.009548  0.020660       ...       -0.020759   -0.028992   \n",
       "342  0.016311  0.000141  0.016589       ...       -0.026954   -0.032857   \n",
       "343  0.026437  0.010142  0.007763       ...        0.036078    0.053978   \n",
       "344  0.037291  0.012184 -0.000591       ...       -0.012456    0.015450   \n",
       "345  0.031698  0.016464  0.001402       ...        0.100872    0.044448   \n",
       "346  0.030893  0.013931 -0.000193       ...       -0.025411   -0.019798   \n",
       "347  0.022137  0.008025  0.007363       ...       -0.018806   -0.027218   \n",
       "348  0.015409  0.005642  0.008591       ...        0.065100    0.049271   \n",
       "349  0.012258 -0.006900  0.010042       ...       -0.061386   -0.027291   \n",
       "350  0.011453 -0.008102  0.014647       ...       -0.097891   -0.104827   \n",
       "351  0.003369 -0.012910  0.020631       ...        0.121229    0.103460   \n",
       "\n",
       "     svar.diff  tbl.lagdiff  AAA.lagdiff  BAA.lagdiff  lty.lagdiff  \\\n",
       "0     0.000268      -0.0035    -0.011438    -0.012138    -0.012838   \n",
       "1    -0.000161       0.0013     0.016741     0.015941     0.015041   \n",
       "2     0.004100      -0.0013    -0.029141    -0.028641    -0.028441   \n",
       "3    -0.003478      -0.0039     0.011161     0.010261     0.011161   \n",
       "4     0.002725       0.0049    -0.006513    -0.006713    -0.007113   \n",
       "5     0.001703       0.0010    -0.005882    -0.005882    -0.005782   \n",
       "6     0.000422       0.0065     0.001100     0.002300     0.000900   \n",
       "7    -0.000348       0.0065    -0.005380    -0.005380    -0.003780   \n",
       "8     0.092115      -0.0031     0.011594     0.011694     0.010994   \n",
       "9    -0.095509       0.0034     0.012700     0.013700     0.015500   \n",
       "10    0.016413       0.0020     0.024301     0.025101     0.022601   \n",
       "11   -0.008366      -0.0022     0.024640     0.026140     0.025140   \n",
       "12    0.015494      -0.0155     0.012314     0.011914     0.010114   \n",
       "13   -0.015925      -0.0008     0.025634     0.023934     0.025634   \n",
       "14    0.024675      -0.0106    -0.005782    -0.004782    -0.005982   \n",
       "15   -0.001913      -0.0012    -0.004954    -0.004754    -0.003854   \n",
       "16    0.040092      -0.0029     0.011040     0.020640     0.010640   \n",
       "17   -0.017631      -0.0010    -0.004780    -0.006180    -0.003280   \n",
       "18    0.019837      -0.0083     0.007783     0.017283     0.006783   \n",
       "19    0.026091      -0.0010    -0.002379     0.002821    -0.000879   \n",
       "20   -0.034244       0.0196     0.004772     0.020572     0.002472   \n",
       "21   -0.010147      -0.0016    -0.039908    -0.052408    -0.040208   \n",
       "22    0.001897      -0.0191    -0.049776    -0.032376    -0.056376   \n",
       "23   -0.001937      -0.0031    -0.029488    -0.056288    -0.025188   \n",
       "24   -0.016149       0.0001    -0.046844    -0.037644    -0.046144   \n",
       "25   -0.024269       0.0130     0.001318     0.005318     0.001018   \n",
       "26   -0.002422      -0.0127     0.022245     0.006045     0.022945   \n",
       "27    0.006707      -0.0003     0.013706     0.016706     0.014906   \n",
       "28   -0.013871       0.0025    -0.013412    -0.010012    -0.012012   \n",
       "29    0.000474      -0.0005     0.003819    -0.007381     0.004619   \n",
       "..         ...          ...          ...          ...          ...   \n",
       "322  -0.006426      -0.0033     0.005709     0.005109     0.003609   \n",
       "323   0.023719      -0.0072     0.046018     0.045418     0.043318   \n",
       "324   0.082747      -0.0089     0.002324     0.005424     0.000924   \n",
       "325  -0.072139      -0.0174     0.010932     0.013132     0.008932   \n",
       "326  -0.024892       0.0060     0.000274     0.000374     0.001374   \n",
       "327  -0.009543      -0.0073    -0.039310    -0.036610    -0.040710   \n",
       "328  -0.001029      -0.0110    -0.001987     0.015213    -0.009987   \n",
       "329  -0.001834       0.0018     0.016994     0.012394     0.017694   \n",
       "330   0.010495      -0.0003     0.000205    -0.010095     0.006505   \n",
       "331  -0.007103      -0.0006    -0.008280    -0.015380    -0.006080   \n",
       "332  -0.004741      -0.0007    -0.010475    -0.011175    -0.006275   \n",
       "333   0.000626       0.0010    -0.008456    -0.009556    -0.008556   \n",
       "334  -0.000411      -0.0003    -0.006895    -0.003395    -0.011195   \n",
       "335   0.024103       0.0003     0.005252     0.003052     0.005252   \n",
       "336  -0.010124      -0.0001     0.007980     0.007480     0.010380   \n",
       "337  -0.015417      -0.0004     0.010816     0.009016     0.011216   \n",
       "338   0.004134      -0.0006    -0.004636    -0.006036    -0.005736   \n",
       "339  -0.003236      -0.0003    -0.006559    -0.002359    -0.011459   \n",
       "340   0.000539       0.0000     0.001069     0.002469     0.000969   \n",
       "341  -0.001230       0.0007    -0.002165    -0.002965     0.001435   \n",
       "342   0.002312       0.0001     0.002144     0.003544    -0.000856   \n",
       "343  -0.002871       0.0002    -0.004607    -0.004907    -0.003007   \n",
       "344   0.000754      -0.0004     0.001495    -0.002205     0.001395   \n",
       "345   0.000598       0.0002    -0.002735    -0.003335    -0.001235   \n",
       "346  -0.001226      -0.0004     0.007471     0.007471     0.008571   \n",
       "347   0.000016      -0.0003     0.012526     0.011626     0.010126   \n",
       "348   0.002871       0.0005     0.008154     0.007454     0.010854   \n",
       "349  -0.000327      -0.0002    -0.004393    -0.005193    -0.005593   \n",
       "350  -0.002247      -0.0001     0.000295    -0.001005    -0.000805   \n",
       "351   0.008547      -0.0002    -0.008956    -0.007556    -0.008756   \n",
       "\n",
       "     ltr.lagdiff  corpr.lagdiff  EqPremResponse  \n",
       "0       0.030167       0.005259               1  \n",
       "1       0.007711      -0.000199               0  \n",
       "2      -0.038657      -0.012230               1  \n",
       "3       0.011038       0.014918               1  \n",
       "4       0.012577      -0.004409               1  \n",
       "5      -0.020049      -0.005564               1  \n",
       "6      -0.011019      -0.022453               1  \n",
       "7      -0.014276       0.019109               0  \n",
       "8       0.034818       0.002796               1  \n",
       "9      -0.055116      -0.014560               0  \n",
       "10      0.060579       0.003230               0  \n",
       "11     -0.022683       0.005639               0  \n",
       "12      0.053953       0.017375               1  \n",
       "13     -0.037758       0.002332               0  \n",
       "14      0.001955      -0.001820               0  \n",
       "15     -0.005301       0.004981               0  \n",
       "16     -0.011479      -0.035129               0  \n",
       "17      0.005998       0.041738               0  \n",
       "18      0.016975      -0.011399               1  \n",
       "19     -0.054655      -0.020497               0  \n",
       "20     -0.020700      -0.086552               0  \n",
       "21      0.094680       0.087251               1  \n",
       "22      0.004268      -0.013678               0  \n",
       "23      0.007163       0.087617               1  \n",
       "24     -0.039776      -0.050776               1  \n",
       "25     -0.016406      -0.024622               0  \n",
       "26      0.033930       0.064430               0  \n",
       "27     -0.027139      -0.044554               1  \n",
       "28     -0.039893      -0.019850               0  \n",
       "29      0.089273       0.055872               1  \n",
       "..           ...            ...             ...  \n",
       "322    -0.028789      -0.018601               0  \n",
       "323     0.070788       0.044402               0  \n",
       "324     0.009819      -0.005996               0  \n",
       "325    -0.025959      -0.030897               1  \n",
       "326    -0.057695      -0.013557               1  \n",
       "327     0.056794      -0.060487               1  \n",
       "328     0.173807       0.318911               1  \n",
       "329    -0.267683      -0.357943               0  \n",
       "330    -0.019728       0.206711               1  \n",
       "331     0.123606       0.028493               1  \n",
       "332    -0.098336      -0.132506               1  \n",
       "333     0.066447       0.039697               0  \n",
       "334     0.112136       0.065943               0  \n",
       "335    -0.066957      -0.034212               1  \n",
       "336    -0.138471      -0.079181               1  \n",
       "337     0.072980       0.017832               0  \n",
       "338     0.046319       0.039739               1  \n",
       "339     0.174531       0.105940               0  \n",
       "340    -0.191170      -0.110793               1  \n",
       "341    -0.069581      -0.029160               1  \n",
       "342     0.141781       0.073003               1  \n",
       "343    -0.089890      -0.029066               1  \n",
       "344    -0.010356      -0.033143               0  \n",
       "345    -0.020759      -0.028992               1  \n",
       "346    -0.026954      -0.032857               0  \n",
       "347     0.036078       0.053978               1  \n",
       "348    -0.012456       0.015450               0  \n",
       "349     0.100872       0.044448               0  \n",
       "350    -0.025411      -0.019798               0  \n",
       "351    -0.018806      -0.027218               1  \n",
       "\n",
       "[352 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################################\n",
    "# 1. load data\n",
    "############################################################\n",
    "\n",
    "# load dataset\n",
    "print(\"Loading data...\")\n",
    "dataframe = pd.read_csv(\"EqPremClass.csv\")\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas dataframe -> numpy ndarray\n",
    "dataset = dataframe.values\n",
    "del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 352\n",
      "Features: 31\n",
      "Histogram: check all 0s and 1s, no -1s etc.\n",
      "(array([176,   0,   0,   0,   0,   0,   0,   0,   0, 176]),\n",
      " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.767621</td>\n",
       "      <td>-0.689589</td>\n",
       "      <td>-0.774685</td>\n",
       "      <td>-0.099709</td>\n",
       "      <td>0.519319</td>\n",
       "      <td>0.428291</td>\n",
       "      <td>0.379438</td>\n",
       "      <td>2.381861</td>\n",
       "      <td>-0.910326</td>\n",
       "      <td>0.266007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421662</td>\n",
       "      <td>0.185423</td>\n",
       "      <td>-0.071273</td>\n",
       "      <td>0.018805</td>\n",
       "      <td>-0.398291</td>\n",
       "      <td>-0.782983</td>\n",
       "      <td>-0.821576</td>\n",
       "      <td>-0.870520</td>\n",
       "      <td>0.451925</td>\n",
       "      <td>0.085675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.765478</td>\n",
       "      <td>-0.686961</td>\n",
       "      <td>-0.817516</td>\n",
       "      <td>-0.067674</td>\n",
       "      <td>0.380785</td>\n",
       "      <td>0.297435</td>\n",
       "      <td>0.240222</td>\n",
       "      <td>1.486580</td>\n",
       "      <td>-0.762275</td>\n",
       "      <td>-0.164262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434384</td>\n",
       "      <td>-0.299704</td>\n",
       "      <td>-0.089852</td>\n",
       "      <td>-0.014161</td>\n",
       "      <td>0.163439</td>\n",
       "      <td>1.163271</td>\n",
       "      <td>1.096738</td>\n",
       "      <td>1.035651</td>\n",
       "      <td>0.114856</td>\n",
       "      <td>-0.002960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.763335</td>\n",
       "      <td>-0.684293</td>\n",
       "      <td>-0.791125</td>\n",
       "      <td>0.140554</td>\n",
       "      <td>0.406691</td>\n",
       "      <td>0.348599</td>\n",
       "      <td>0.261890</td>\n",
       "      <td>1.321980</td>\n",
       "      <td>-0.762275</td>\n",
       "      <td>-0.400750</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006831</td>\n",
       "      <td>-0.165443</td>\n",
       "      <td>-0.361633</td>\n",
       "      <td>0.313814</td>\n",
       "      <td>-0.140831</td>\n",
       "      <td>-2.005689</td>\n",
       "      <td>-1.949039</td>\n",
       "      <td>-1.937349</td>\n",
       "      <td>-0.581140</td>\n",
       "      <td>-0.198327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.761192</td>\n",
       "      <td>-0.681664</td>\n",
       "      <td>-0.948962</td>\n",
       "      <td>0.348782</td>\n",
       "      <td>0.279979</td>\n",
       "      <td>0.228911</td>\n",
       "      <td>0.170875</td>\n",
       "      <td>0.418281</td>\n",
       "      <td>-0.616790</td>\n",
       "      <td>-0.707137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426733</td>\n",
       "      <td>-0.213873</td>\n",
       "      <td>0.307189</td>\n",
       "      <td>-0.269523</td>\n",
       "      <td>-0.445101</td>\n",
       "      <td>0.777854</td>\n",
       "      <td>0.708668</td>\n",
       "      <td>0.770342</td>\n",
       "      <td>0.164790</td>\n",
       "      <td>0.242523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.759049</td>\n",
       "      <td>-0.678996</td>\n",
       "      <td>-1.205563</td>\n",
       "      <td>0.249473</td>\n",
       "      <td>0.553026</td>\n",
       "      <td>0.489048</td>\n",
       "      <td>0.435564</td>\n",
       "      <td>1.839043</td>\n",
       "      <td>-0.908595</td>\n",
       "      <td>0.040092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.876449</td>\n",
       "      <td>0.516128</td>\n",
       "      <td>0.044673</td>\n",
       "      <td>0.207974</td>\n",
       "      <td>0.584736</td>\n",
       "      <td>-0.442798</td>\n",
       "      <td>-0.450921</td>\n",
       "      <td>-0.479058</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-0.071330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.755834</td>\n",
       "      <td>-0.676721</td>\n",
       "      <td>-1.158236</td>\n",
       "      <td>0.358393</td>\n",
       "      <td>0.852110</td>\n",
       "      <td>0.793800</td>\n",
       "      <td>0.808729</td>\n",
       "      <td>2.516687</td>\n",
       "      <td>-1.205575</td>\n",
       "      <td>-1.142770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.891871</td>\n",
       "      <td>-0.821140</td>\n",
       "      <td>-0.234620</td>\n",
       "      <td>0.129286</td>\n",
       "      <td>0.128331</td>\n",
       "      <td>-0.399259</td>\n",
       "      <td>-0.394191</td>\n",
       "      <td>-0.388096</td>\n",
       "      <td>-0.301823</td>\n",
       "      <td>-0.090078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.752620</td>\n",
       "      <td>-0.674484</td>\n",
       "      <td>-1.228362</td>\n",
       "      <td>0.422463</td>\n",
       "      <td>1.424413</td>\n",
       "      <td>1.352177</td>\n",
       "      <td>1.352862</td>\n",
       "      <td>3.990253</td>\n",
       "      <td>-1.799575</td>\n",
       "      <td>0.157329</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.777035</td>\n",
       "      <td>0.899185</td>\n",
       "      <td>0.051657</td>\n",
       "      <td>0.030703</td>\n",
       "      <td>0.771979</td>\n",
       "      <td>0.082991</td>\n",
       "      <td>0.164810</td>\n",
       "      <td>0.068793</td>\n",
       "      <td>-0.166292</td>\n",
       "      <td>-0.364338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.749405</td>\n",
       "      <td>-0.672248</td>\n",
       "      <td>-1.260999</td>\n",
       "      <td>0.351986</td>\n",
       "      <td>2.004710</td>\n",
       "      <td>1.933677</td>\n",
       "      <td>1.958135</td>\n",
       "      <td>4.200511</td>\n",
       "      <td>-2.412188</td>\n",
       "      <td>-0.329478</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.832500</td>\n",
       "      <td>-0.338877</td>\n",
       "      <td>0.090419</td>\n",
       "      <td>-0.028619</td>\n",
       "      <td>0.771979</td>\n",
       "      <td>-0.364587</td>\n",
       "      <td>-0.359895</td>\n",
       "      <td>-0.251214</td>\n",
       "      <td>-0.215180</td>\n",
       "      <td>0.310585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.746191</td>\n",
       "      <td>-0.669973</td>\n",
       "      <td>-0.910407</td>\n",
       "      <td>-0.144558</td>\n",
       "      <td>2.294717</td>\n",
       "      <td>2.198710</td>\n",
       "      <td>2.201640</td>\n",
       "      <td>5.906464</td>\n",
       "      <td>-2.754837</td>\n",
       "      <td>0.828413</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.027973</td>\n",
       "      <td>0.800656</td>\n",
       "      <td>0.279277</td>\n",
       "      <td>7.089744</td>\n",
       "      <td>-0.351480</td>\n",
       "      <td>0.807774</td>\n",
       "      <td>0.806583</td>\n",
       "      <td>0.758939</td>\n",
       "      <td>0.521726</td>\n",
       "      <td>0.045674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.745869</td>\n",
       "      <td>-0.676250</td>\n",
       "      <td>-0.982851</td>\n",
       "      <td>-0.170187</td>\n",
       "      <td>2.898424</td>\n",
       "      <td>2.731137</td>\n",
       "      <td>2.818806</td>\n",
       "      <td>5.176581</td>\n",
       "      <td>-3.412602</td>\n",
       "      <td>0.018082</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.967060</td>\n",
       "      <td>-0.563031</td>\n",
       "      <td>0.037204</td>\n",
       "      <td>-7.354659</td>\n",
       "      <td>0.409195</td>\n",
       "      <td>0.884135</td>\n",
       "      <td>0.943601</td>\n",
       "      <td>1.066998</td>\n",
       "      <td>-0.828192</td>\n",
       "      <td>-0.236167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.745655</td>\n",
       "      <td>-0.682527</td>\n",
       "      <td>-0.667222</td>\n",
       "      <td>-0.509759</td>\n",
       "      <td>2.762247</td>\n",
       "      <td>2.624753</td>\n",
       "      <td>2.674777</td>\n",
       "      <td>4.589664</td>\n",
       "      <td>-3.279654</td>\n",
       "      <td>0.060034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389372</td>\n",
       "      <td>0.027475</td>\n",
       "      <td>-0.029610</td>\n",
       "      <td>1.261790</td>\n",
       "      <td>0.245358</td>\n",
       "      <td>1.685392</td>\n",
       "      <td>1.722505</td>\n",
       "      <td>1.552522</td>\n",
       "      <td>0.908407</td>\n",
       "      <td>0.052722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.745334</td>\n",
       "      <td>-0.688805</td>\n",
       "      <td>-0.509109</td>\n",
       "      <td>-0.548201</td>\n",
       "      <td>2.645580</td>\n",
       "      <td>2.519002</td>\n",
       "      <td>2.581993</td>\n",
       "      <td>4.435772</td>\n",
       "      <td>-3.192725</td>\n",
       "      <td>-0.053735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252228</td>\n",
       "      <td>-0.080417</td>\n",
       "      <td>0.079835</td>\n",
       "      <td>-0.645896</td>\n",
       "      <td>-0.246156</td>\n",
       "      <td>1.708838</td>\n",
       "      <td>1.793519</td>\n",
       "      <td>1.726152</td>\n",
       "      <td>-0.341366</td>\n",
       "      <td>0.091837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.745119</td>\n",
       "      <td>-0.695082</td>\n",
       "      <td>-0.100182</td>\n",
       "      <td>-0.641103</td>\n",
       "      <td>2.905582</td>\n",
       "      <td>2.978149</td>\n",
       "      <td>2.838161</td>\n",
       "      <td>3.896661</td>\n",
       "      <td>-3.445420</td>\n",
       "      <td>-0.300079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.759896</td>\n",
       "      <td>-0.172271</td>\n",
       "      <td>-0.565611</td>\n",
       "      <td>1.191043</td>\n",
       "      <td>-1.802614</td>\n",
       "      <td>0.857512</td>\n",
       "      <td>0.821622</td>\n",
       "      <td>0.698780</td>\n",
       "      <td>0.808945</td>\n",
       "      <td>0.282419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.749405</td>\n",
       "      <td>-0.698613</td>\n",
       "      <td>-0.196157</td>\n",
       "      <td>-0.673138</td>\n",
       "      <td>2.793012</td>\n",
       "      <td>2.840675</td>\n",
       "      <td>2.759195</td>\n",
       "      <td>2.280408</td>\n",
       "      <td>-3.357836</td>\n",
       "      <td>-0.171356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254181</td>\n",
       "      <td>0.087595</td>\n",
       "      <td>0.671341</td>\n",
       "      <td>-1.227832</td>\n",
       "      <td>-0.082318</td>\n",
       "      <td>1.777490</td>\n",
       "      <td>1.642809</td>\n",
       "      <td>1.759927</td>\n",
       "      <td>-0.567640</td>\n",
       "      <td>0.038137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.753691</td>\n",
       "      <td>-0.702144</td>\n",
       "      <td>0.096583</td>\n",
       "      <td>-0.939029</td>\n",
       "      <td>2.976317</td>\n",
       "      <td>3.225152</td>\n",
       "      <td>2.922511</td>\n",
       "      <td>1.866470</td>\n",
       "      <td>-3.561285</td>\n",
       "      <td>0.192956</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613136</td>\n",
       "      <td>0.250823</td>\n",
       "      <td>-0.183756</td>\n",
       "      <td>1.897846</td>\n",
       "      <td>-1.229182</td>\n",
       "      <td>-0.392345</td>\n",
       "      <td>-0.319034</td>\n",
       "      <td>-0.401763</td>\n",
       "      <td>0.028451</td>\n",
       "      <td>-0.029287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.757977</td>\n",
       "      <td>-0.705675</td>\n",
       "      <td>1.357994</td>\n",
       "      <td>-0.971065</td>\n",
       "      <td>2.920299</td>\n",
       "      <td>3.287915</td>\n",
       "      <td>2.901358</td>\n",
       "      <td>0.514341</td>\n",
       "      <td>-3.453597</td>\n",
       "      <td>-0.979999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314093</td>\n",
       "      <td>-0.814276</td>\n",
       "      <td>-0.330166</td>\n",
       "      <td>-0.149046</td>\n",
       "      <td>-0.129129</td>\n",
       "      <td>-0.335127</td>\n",
       "      <td>-0.317092</td>\n",
       "      <td>-0.256237</td>\n",
       "      <td>-0.080461</td>\n",
       "      <td>0.081157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.762263</td>\n",
       "      <td>-0.709206</td>\n",
       "      <td>2.207338</td>\n",
       "      <td>-0.343176</td>\n",
       "      <td>3.032685</td>\n",
       "      <td>3.745551</td>\n",
       "      <td>2.960875</td>\n",
       "      <td>-1.238768</td>\n",
       "      <td>-3.379904</td>\n",
       "      <td>-1.424245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212785</td>\n",
       "      <td>-0.309388</td>\n",
       "      <td>-1.393125</td>\n",
       "      <td>3.084688</td>\n",
       "      <td>-0.328074</td>\n",
       "      <td>0.769517</td>\n",
       "      <td>1.417760</td>\n",
       "      <td>0.734741</td>\n",
       "      <td>-0.173185</td>\n",
       "      <td>-0.570177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.770835</td>\n",
       "      <td>-0.711167</td>\n",
       "      <td>2.264018</td>\n",
       "      <td>-0.394432</td>\n",
       "      <td>2.092817</td>\n",
       "      <td>2.579710</td>\n",
       "      <td>1.992831</td>\n",
       "      <td>-1.055398</td>\n",
       "      <td>-2.461043</td>\n",
       "      <td>0.607710</td>\n",
       "      <td>...</td>\n",
       "      <td>2.731502</td>\n",
       "      <td>1.406253</td>\n",
       "      <td>1.403731</td>\n",
       "      <td>-1.359133</td>\n",
       "      <td>-0.105723</td>\n",
       "      <td>-0.323112</td>\n",
       "      <td>-0.414516</td>\n",
       "      <td>-0.216994</td>\n",
       "      <td>0.089139</td>\n",
       "      <td>0.678059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.779407</td>\n",
       "      <td>-0.713129</td>\n",
       "      <td>5.420504</td>\n",
       "      <td>-1.006303</td>\n",
       "      <td>0.920548</td>\n",
       "      <td>1.859491</td>\n",
       "      <td>0.635526</td>\n",
       "      <td>-2.362157</td>\n",
       "      <td>-1.100015</td>\n",
       "      <td>0.699302</td>\n",
       "      <td>...</td>\n",
       "      <td>4.049219</td>\n",
       "      <td>0.061868</td>\n",
       "      <td>-0.220421</td>\n",
       "      <td>1.525357</td>\n",
       "      <td>-0.960020</td>\n",
       "      <td>0.544592</td>\n",
       "      <td>1.188443</td>\n",
       "      <td>0.471054</td>\n",
       "      <td>0.253912</td>\n",
       "      <td>-0.184839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.787979</td>\n",
       "      <td>-0.715091</td>\n",
       "      <td>2.370793</td>\n",
       "      <td>-1.105612</td>\n",
       "      <td>0.226078</td>\n",
       "      <td>0.607336</td>\n",
       "      <td>0.029099</td>\n",
       "      <td>-0.686104</td>\n",
       "      <td>-0.536535</td>\n",
       "      <td>0.853027</td>\n",
       "      <td>...</td>\n",
       "      <td>1.672420</td>\n",
       "      <td>0.104917</td>\n",
       "      <td>1.409625</td>\n",
       "      <td>2.006815</td>\n",
       "      <td>-0.105723</td>\n",
       "      <td>-0.157266</td>\n",
       "      <td>0.200431</td>\n",
       "      <td>-0.052815</td>\n",
       "      <td>-0.821262</td>\n",
       "      <td>-0.332584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.796551</td>\n",
       "      <td>-0.717052</td>\n",
       "      <td>3.223840</td>\n",
       "      <td>-1.102409</td>\n",
       "      <td>-0.877133</td>\n",
       "      <td>-0.230066</td>\n",
       "      <td>-1.081854</td>\n",
       "      <td>-0.918381</td>\n",
       "      <td>0.614780</td>\n",
       "      <td>-0.000621</td>\n",
       "      <td>...</td>\n",
       "      <td>3.424244</td>\n",
       "      <td>-0.593043</td>\n",
       "      <td>-0.817408</td>\n",
       "      <td>-2.638109</td>\n",
       "      <td>2.305032</td>\n",
       "      <td>0.336609</td>\n",
       "      <td>1.413119</td>\n",
       "      <td>0.176278</td>\n",
       "      <td>-0.311599</td>\n",
       "      <td>-1.405241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.798158</td>\n",
       "      <td>-0.716738</td>\n",
       "      <td>3.352892</td>\n",
       "      <td>-0.685952</td>\n",
       "      <td>-0.846100</td>\n",
       "      <td>-0.111771</td>\n",
       "      <td>-1.057352</td>\n",
       "      <td>-1.600695</td>\n",
       "      <td>0.604266</td>\n",
       "      <td>-0.352722</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038162</td>\n",
       "      <td>-0.245546</td>\n",
       "      <td>-0.396539</td>\n",
       "      <td>-0.782964</td>\n",
       "      <td>-0.175939</td>\n",
       "      <td>-2.749304</td>\n",
       "      <td>-3.572722</td>\n",
       "      <td>-2.741859</td>\n",
       "      <td>1.420272</td>\n",
       "      <td>1.417131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.799766</td>\n",
       "      <td>-0.716464</td>\n",
       "      <td>0.944061</td>\n",
       "      <td>-1.092798</td>\n",
       "      <td>-0.322217</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>-0.504936</td>\n",
       "      <td>-0.683813</td>\n",
       "      <td>-0.010978</td>\n",
       "      <td>0.375464</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.840343</td>\n",
       "      <td>0.502934</td>\n",
       "      <td>1.036507</td>\n",
       "      <td>0.144224</td>\n",
       "      <td>-2.223911</td>\n",
       "      <td>-3.430859</td>\n",
       "      <td>-2.204179</td>\n",
       "      <td>-3.847307</td>\n",
       "      <td>0.063170</td>\n",
       "      <td>-0.221838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.801373</td>\n",
       "      <td>-0.716150</td>\n",
       "      <td>1.073275</td>\n",
       "      <td>-1.102409</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.394327</td>\n",
       "      <td>-0.146063</td>\n",
       "      <td>-0.733209</td>\n",
       "      <td>-0.381107</td>\n",
       "      <td>-0.206980</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.109868</td>\n",
       "      <td>-0.405139</td>\n",
       "      <td>-0.717279</td>\n",
       "      <td>-0.150958</td>\n",
       "      <td>-0.351480</td>\n",
       "      <td>-2.029634</td>\n",
       "      <td>-3.837804</td>\n",
       "      <td>-1.714912</td>\n",
       "      <td>0.106625</td>\n",
       "      <td>1.423078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.802980</td>\n",
       "      <td>-0.715875</td>\n",
       "      <td>0.927289</td>\n",
       "      <td>-1.022321</td>\n",
       "      <td>-0.315303</td>\n",
       "      <td>0.171598</td>\n",
       "      <td>-0.435271</td>\n",
       "      <td>-0.463776</td>\n",
       "      <td>-0.008299</td>\n",
       "      <td>-1.063143</td>\n",
       "      <td>...</td>\n",
       "      <td>1.104188</td>\n",
       "      <td>-0.594786</td>\n",
       "      <td>-0.319750</td>\n",
       "      <td>-1.245052</td>\n",
       "      <td>0.023006</td>\n",
       "      <td>-3.228336</td>\n",
       "      <td>-2.564060</td>\n",
       "      <td>-3.147701</td>\n",
       "      <td>-0.597937</td>\n",
       "      <td>-0.824272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.802659</td>\n",
       "      <td>-0.715365</td>\n",
       "      <td>0.827967</td>\n",
       "      <td>-1.038338</td>\n",
       "      <td>-0.225367</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>-0.324069</td>\n",
       "      <td>-0.325683</td>\n",
       "      <td>-0.197538</td>\n",
       "      <td>0.852764</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.570790</td>\n",
       "      <td>1.325849</td>\n",
       "      <td>0.898785</td>\n",
       "      <td>-1.870190</td>\n",
       "      <td>1.532654</td>\n",
       "      <td>0.098028</td>\n",
       "      <td>0.370974</td>\n",
       "      <td>0.076841</td>\n",
       "      <td>-0.247152</td>\n",
       "      <td>-0.399563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.802444</td>\n",
       "      <td>-0.714895</td>\n",
       "      <td>0.971137</td>\n",
       "      <td>-1.067170</td>\n",
       "      <td>0.079037</td>\n",
       "      <td>0.294931</td>\n",
       "      <td>-0.008063</td>\n",
       "      <td>-0.472403</td>\n",
       "      <td>-0.573192</td>\n",
       "      <td>0.388347</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.126331</td>\n",
       "      <td>-0.323363</td>\n",
       "      <td>-0.395126</td>\n",
       "      <td>-0.188237</td>\n",
       "      <td>-1.474939</td>\n",
       "      <td>1.543393</td>\n",
       "      <td>0.420641</td>\n",
       "      <td>1.576045</td>\n",
       "      <td>0.508407</td>\n",
       "      <td>1.046556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.802123</td>\n",
       "      <td>-0.714384</td>\n",
       "      <td>1.082145</td>\n",
       "      <td>-1.047949</td>\n",
       "      <td>-0.432139</td>\n",
       "      <td>-0.081133</td>\n",
       "      <td>-0.487298</td>\n",
       "      <td>0.072535</td>\n",
       "      <td>-0.019349</td>\n",
       "      <td>-0.793544</td>\n",
       "      <td>...</td>\n",
       "      <td>1.643699</td>\n",
       "      <td>-0.820468</td>\n",
       "      <td>-0.520688</td>\n",
       "      <td>0.514511</td>\n",
       "      <td>-0.023804</td>\n",
       "      <td>0.953642</td>\n",
       "      <td>1.148990</td>\n",
       "      <td>1.026410</td>\n",
       "      <td>-0.408255</td>\n",
       "      <td>-0.723228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.801908</td>\n",
       "      <td>-0.713914</td>\n",
       "      <td>0.720188</td>\n",
       "      <td>-1.041542</td>\n",
       "      <td>0.230453</td>\n",
       "      <td>0.502465</td>\n",
       "      <td>0.185250</td>\n",
       "      <td>0.128152</td>\n",
       "      <td>-0.765212</td>\n",
       "      <td>0.402834</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.229607</td>\n",
       "      <td>0.827322</td>\n",
       "      <td>0.487300</td>\n",
       "      <td>-1.069688</td>\n",
       "      <td>0.303871</td>\n",
       "      <td>-0.919330</td>\n",
       "      <td>-0.676340</td>\n",
       "      <td>-0.814053</td>\n",
       "      <td>-0.599696</td>\n",
       "      <td>-0.322074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.801908</td>\n",
       "      <td>-0.704498</td>\n",
       "      <td>0.821381</td>\n",
       "      <td>-1.067170</td>\n",
       "      <td>0.025577</td>\n",
       "      <td>0.333416</td>\n",
       "      <td>-0.036231</td>\n",
       "      <td>-0.226810</td>\n",
       "      <td>-0.581498</td>\n",
       "      <td>0.367790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540662</td>\n",
       "      <td>-0.025872</td>\n",
       "      <td>0.102414</td>\n",
       "      <td>0.034693</td>\n",
       "      <td>-0.047210</td>\n",
       "      <td>0.270771</td>\n",
       "      <td>-0.496589</td>\n",
       "      <td>0.323057</td>\n",
       "      <td>1.339106</td>\n",
       "      <td>0.907578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>2.226342</td>\n",
       "      <td>1.282266</td>\n",
       "      <td>-1.150029</td>\n",
       "      <td>-0.519369</td>\n",
       "      <td>0.954547</td>\n",
       "      <td>0.957295</td>\n",
       "      <td>0.877336</td>\n",
       "      <td>-2.675697</td>\n",
       "      <td>-1.068394</td>\n",
       "      <td>-0.823140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100116</td>\n",
       "      <td>-0.859490</td>\n",
       "      <td>-0.218477</td>\n",
       "      <td>-0.496480</td>\n",
       "      <td>-0.374885</td>\n",
       "      <td>0.401293</td>\n",
       "      <td>0.356687</td>\n",
       "      <td>0.253986</td>\n",
       "      <td>-0.433020</td>\n",
       "      <td>-0.301789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>2.241557</td>\n",
       "      <td>1.069623</td>\n",
       "      <td>-1.102708</td>\n",
       "      <td>-0.753226</td>\n",
       "      <td>0.028760</td>\n",
       "      <td>0.142886</td>\n",
       "      <td>-0.102798</td>\n",
       "      <td>-2.753261</td>\n",
       "      <td>-0.086557</td>\n",
       "      <td>0.395719</td>\n",
       "      <td>...</td>\n",
       "      <td>2.919178</td>\n",
       "      <td>0.842898</td>\n",
       "      <td>-0.973682</td>\n",
       "      <td>1.824228</td>\n",
       "      <td>-0.831290</td>\n",
       "      <td>3.185336</td>\n",
       "      <td>3.110535</td>\n",
       "      <td>2.969010</td>\n",
       "      <td>1.061640</td>\n",
       "      <td>0.721314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>2.191518</td>\n",
       "      <td>-0.149349</td>\n",
       "      <td>-0.848498</td>\n",
       "      <td>-1.105612</td>\n",
       "      <td>-0.018045</td>\n",
       "      <td>0.481298</td>\n",
       "      <td>-0.343254</td>\n",
       "      <td>-1.718515</td>\n",
       "      <td>-0.187550</td>\n",
       "      <td>4.125837</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.307802</td>\n",
       "      <td>2.582829</td>\n",
       "      <td>5.131648</td>\n",
       "      <td>6.368563</td>\n",
       "      <td>-1.030236</td>\n",
       "      <td>0.167547</td>\n",
       "      <td>0.378253</td>\n",
       "      <td>0.070451</td>\n",
       "      <td>0.146491</td>\n",
       "      <td>-0.097099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>2.070226</td>\n",
       "      <td>-0.463999</td>\n",
       "      <td>-0.507021</td>\n",
       "      <td>-1.047949</td>\n",
       "      <td>0.382175</td>\n",
       "      <td>0.757005</td>\n",
       "      <td>0.082742</td>\n",
       "      <td>-2.244807</td>\n",
       "      <td>-0.502005</td>\n",
       "      <td>-1.618973</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.943951</td>\n",
       "      <td>-3.981896</td>\n",
       "      <td>-5.760390</td>\n",
       "      <td>-5.555482</td>\n",
       "      <td>-2.024965</td>\n",
       "      <td>0.762035</td>\n",
       "      <td>0.904807</td>\n",
       "      <td>0.617938</td>\n",
       "      <td>-0.390538</td>\n",
       "      <td>-0.501455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>1.892251</td>\n",
       "      <td>-0.438497</td>\n",
       "      <td>-0.672838</td>\n",
       "      <td>-1.057559</td>\n",
       "      <td>0.387001</td>\n",
       "      <td>0.532436</td>\n",
       "      <td>0.239355</td>\n",
       "      <td>-1.627570</td>\n",
       "      <td>-0.479477</td>\n",
       "      <td>-2.042352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060305</td>\n",
       "      <td>-0.294931</td>\n",
       "      <td>3.326101</td>\n",
       "      <td>-1.918137</td>\n",
       "      <td>0.713465</td>\n",
       "      <td>0.025953</td>\n",
       "      <td>0.033241</td>\n",
       "      <td>0.101213</td>\n",
       "      <td>-0.866904</td>\n",
       "      <td>-0.219877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>1.710847</td>\n",
       "      <td>-0.241155</td>\n",
       "      <td>-0.868962</td>\n",
       "      <td>-1.076780</td>\n",
       "      <td>0.191993</td>\n",
       "      <td>0.190294</td>\n",
       "      <td>0.092966</td>\n",
       "      <td>-0.717481</td>\n",
       "      <td>-0.391883</td>\n",
       "      <td>0.610392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254213</td>\n",
       "      <td>1.836368</td>\n",
       "      <td>0.458202</td>\n",
       "      <td>-0.736489</td>\n",
       "      <td>-0.842993</td>\n",
       "      <td>-2.708011</td>\n",
       "      <td>-2.493444</td>\n",
       "      <td>-2.776191</td>\n",
       "      <td>0.851588</td>\n",
       "      <td>-0.981971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>1.550552</td>\n",
       "      <td>1.266573</td>\n",
       "      <td>-0.958833</td>\n",
       "      <td>-1.099205</td>\n",
       "      <td>-0.054703</td>\n",
       "      <td>-0.058300</td>\n",
       "      <td>-0.058112</td>\n",
       "      <td>-0.287832</td>\n",
       "      <td>-0.095519</td>\n",
       "      <td>-1.500013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.876374</td>\n",
       "      <td>-1.463790</td>\n",
       "      <td>-2.132624</td>\n",
       "      <td>-0.081000</td>\n",
       "      <td>-1.275993</td>\n",
       "      <td>-0.130246</td>\n",
       "      <td>1.046972</td>\n",
       "      <td>-0.675606</td>\n",
       "      <td>2.607977</td>\n",
       "      <td>5.179055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>1.496870</td>\n",
       "      <td>1.657335</td>\n",
       "      <td>-0.747752</td>\n",
       "      <td>-1.067170</td>\n",
       "      <td>-0.253856</td>\n",
       "      <td>-0.270884</td>\n",
       "      <td>-0.264112</td>\n",
       "      <td>-0.153425</td>\n",
       "      <td>0.119832</td>\n",
       "      <td>-0.073969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634945</td>\n",
       "      <td>0.986446</td>\n",
       "      <td>0.638493</td>\n",
       "      <td>-0.143025</td>\n",
       "      <td>0.221952</td>\n",
       "      <td>1.180732</td>\n",
       "      <td>0.854401</td>\n",
       "      <td>1.217030</td>\n",
       "      <td>-4.018846</td>\n",
       "      <td>-5.812346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1.511121</td>\n",
       "      <td>1.899403</td>\n",
       "      <td>-0.589300</td>\n",
       "      <td>-1.076780</td>\n",
       "      <td>-0.416249</td>\n",
       "      <td>-0.346417</td>\n",
       "      <td>-0.533652</td>\n",
       "      <td>0.043060</td>\n",
       "      <td>0.195223</td>\n",
       "      <td>2.332615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217845</td>\n",
       "      <td>1.665816</td>\n",
       "      <td>1.060850</td>\n",
       "      <td>0.806166</td>\n",
       "      <td>-0.023804</td>\n",
       "      <td>0.021172</td>\n",
       "      <td>-0.681995</td>\n",
       "      <td>0.452017</td>\n",
       "      <td>-0.297004</td>\n",
       "      <td>3.357037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>1.544980</td>\n",
       "      <td>2.086153</td>\n",
       "      <td>-0.738658</td>\n",
       "      <td>-1.067170</td>\n",
       "      <td>-0.292571</td>\n",
       "      <td>-0.278534</td>\n",
       "      <td>-0.407217</td>\n",
       "      <td>-0.583980</td>\n",
       "      <td>-0.025042</td>\n",
       "      <td>0.895628</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.663253</td>\n",
       "      <td>-0.997211</td>\n",
       "      <td>-0.550861</td>\n",
       "      <td>-0.548614</td>\n",
       "      <td>-0.058912</td>\n",
       "      <td>-0.564876</td>\n",
       "      <td>-1.043069</td>\n",
       "      <td>-0.408466</td>\n",
       "      <td>1.854455</td>\n",
       "      <td>0.462975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>1.585268</td>\n",
       "      <td>2.301542</td>\n",
       "      <td>-0.836825</td>\n",
       "      <td>-1.070373</td>\n",
       "      <td>-0.104639</td>\n",
       "      <td>-0.112142</td>\n",
       "      <td>-0.157313</td>\n",
       "      <td>-0.206271</td>\n",
       "      <td>-0.102558</td>\n",
       "      <td>-2.076139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.237837</td>\n",
       "      <td>-2.060587</td>\n",
       "      <td>-1.274514</td>\n",
       "      <td>-0.366757</td>\n",
       "      <td>-0.070615</td>\n",
       "      <td>-0.716462</td>\n",
       "      <td>-0.755776</td>\n",
       "      <td>-0.421782</td>\n",
       "      <td>-1.476920</td>\n",
       "      <td>-2.151486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>1.660487</td>\n",
       "      <td>2.456905</td>\n",
       "      <td>-0.851174</td>\n",
       "      <td>-1.083187</td>\n",
       "      <td>0.150089</td>\n",
       "      <td>0.088425</td>\n",
       "      <td>0.112725</td>\n",
       "      <td>-0.210724</td>\n",
       "      <td>-0.347100</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.735600</td>\n",
       "      <td>1.083576</td>\n",
       "      <td>0.286639</td>\n",
       "      <td>0.046391</td>\n",
       "      <td>0.128331</td>\n",
       "      <td>-0.577031</td>\n",
       "      <td>-0.645185</td>\n",
       "      <td>-0.577755</td>\n",
       "      <td>0.996493</td>\n",
       "      <td>0.644911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>1.757993</td>\n",
       "      <td>2.557342</td>\n",
       "      <td>-0.861288</td>\n",
       "      <td>-1.102409</td>\n",
       "      <td>0.040908</td>\n",
       "      <td>-0.045848</td>\n",
       "      <td>-0.025374</td>\n",
       "      <td>-0.240175</td>\n",
       "      <td>-0.265654</td>\n",
       "      <td>0.484146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235887</td>\n",
       "      <td>0.687140</td>\n",
       "      <td>0.639166</td>\n",
       "      <td>-0.033410</td>\n",
       "      <td>-0.023804</td>\n",
       "      <td>-0.469228</td>\n",
       "      <td>-0.224289</td>\n",
       "      <td>-0.758200</td>\n",
       "      <td>1.682289</td>\n",
       "      <td>1.071121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>1.847998</td>\n",
       "      <td>2.679357</td>\n",
       "      <td>-0.680156</td>\n",
       "      <td>-1.112019</td>\n",
       "      <td>-0.113572</td>\n",
       "      <td>-0.098335</td>\n",
       "      <td>-0.301270</td>\n",
       "      <td>-0.180138</td>\n",
       "      <td>-0.327081</td>\n",
       "      <td>4.229796</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189890</td>\n",
       "      <td>2.593591</td>\n",
       "      <td>1.704492</td>\n",
       "      <td>1.853809</td>\n",
       "      <td>0.046412</td>\n",
       "      <td>0.369725</td>\n",
       "      <td>0.216153</td>\n",
       "      <td>0.366318</td>\n",
       "      <td>-1.005928</td>\n",
       "      <td>-0.555292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>1.981292</td>\n",
       "      <td>2.678180</td>\n",
       "      <td>-0.840076</td>\n",
       "      <td>-1.112019</td>\n",
       "      <td>-0.088400</td>\n",
       "      <td>-0.043414</td>\n",
       "      <td>-0.277945</td>\n",
       "      <td>-0.978027</td>\n",
       "      <td>-0.394252</td>\n",
       "      <td>0.127043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.207010</td>\n",
       "      <td>-2.844193</td>\n",
       "      <td>-1.783220</td>\n",
       "      <td>-0.781223</td>\n",
       "      <td>-0.000399</td>\n",
       "      <td>0.558160</td>\n",
       "      <td>0.518685</td>\n",
       "      <td>0.716952</td>\n",
       "      <td>-2.079362</td>\n",
       "      <td>-1.285547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>2.080726</td>\n",
       "      <td>2.740561</td>\n",
       "      <td>-0.881472</td>\n",
       "      <td>-1.089594</td>\n",
       "      <td>-0.139400</td>\n",
       "      <td>-0.109383</td>\n",
       "      <td>-0.243408</td>\n",
       "      <td>-1.273100</td>\n",
       "      <td>-0.324648</td>\n",
       "      <td>-1.366243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200599</td>\n",
       "      <td>-1.036218</td>\n",
       "      <td>-0.469565</td>\n",
       "      <td>-1.188721</td>\n",
       "      <td>-0.035507</td>\n",
       "      <td>0.754052</td>\n",
       "      <td>0.623638</td>\n",
       "      <td>0.774129</td>\n",
       "      <td>1.094542</td>\n",
       "      <td>0.289848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>2.184339</td>\n",
       "      <td>2.716236</td>\n",
       "      <td>-0.848050</td>\n",
       "      <td>-1.086391</td>\n",
       "      <td>-0.088912</td>\n",
       "      <td>-0.030550</td>\n",
       "      <td>-0.264022</td>\n",
       "      <td>-1.517643</td>\n",
       "      <td>-0.466695</td>\n",
       "      <td>1.676559</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.430150</td>\n",
       "      <td>2.106621</td>\n",
       "      <td>1.174458</td>\n",
       "      <td>0.316439</td>\n",
       "      <td>-0.058912</td>\n",
       "      <td>-0.313174</td>\n",
       "      <td>-0.404685</td>\n",
       "      <td>-0.384924</td>\n",
       "      <td>0.694356</td>\n",
       "      <td>0.645590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>2.320419</td>\n",
       "      <td>2.660525</td>\n",
       "      <td>-0.903196</td>\n",
       "      <td>-1.079984</td>\n",
       "      <td>-0.197401</td>\n",
       "      <td>-0.139699</td>\n",
       "      <td>-0.336408</td>\n",
       "      <td>-1.453840</td>\n",
       "      <td>-0.388506</td>\n",
       "      <td>-0.252597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226182</td>\n",
       "      <td>-1.338211</td>\n",
       "      <td>-0.468046</td>\n",
       "      <td>-0.250944</td>\n",
       "      <td>-0.023804</td>\n",
       "      <td>-0.446022</td>\n",
       "      <td>-0.153512</td>\n",
       "      <td>-0.776251</td>\n",
       "      <td>2.618840</td>\n",
       "      <td>1.720635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>2.497965</td>\n",
       "      <td>2.660918</td>\n",
       "      <td>-0.870794</td>\n",
       "      <td>-1.092798</td>\n",
       "      <td>-0.162182</td>\n",
       "      <td>-0.188740</td>\n",
       "      <td>-0.302811</td>\n",
       "      <td>-1.211290</td>\n",
       "      <td>-0.385875</td>\n",
       "      <td>-0.474839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>-0.155572</td>\n",
       "      <td>-0.533660</td>\n",
       "      <td>0.039711</td>\n",
       "      <td>0.011304</td>\n",
       "      <td>0.080839</td>\n",
       "      <td>0.176344</td>\n",
       "      <td>0.073499</td>\n",
       "      <td>-2.870381</td>\n",
       "      <td>-1.798894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>2.590649</td>\n",
       "      <td>2.707605</td>\n",
       "      <td>-0.881667</td>\n",
       "      <td>-1.086391</td>\n",
       "      <td>-0.226600</td>\n",
       "      <td>-0.262935</td>\n",
       "      <td>-0.332552</td>\n",
       "      <td>-1.101262</td>\n",
       "      <td>-0.246559</td>\n",
       "      <td>-0.920344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408351</td>\n",
       "      <td>-0.310261</td>\n",
       "      <td>-0.466859</td>\n",
       "      <td>-0.096483</td>\n",
       "      <td>0.093223</td>\n",
       "      <td>-0.142546</td>\n",
       "      <td>-0.194917</td>\n",
       "      <td>0.105339</td>\n",
       "      <td>-1.045304</td>\n",
       "      <td>-0.473255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>2.714299</td>\n",
       "      <td>2.835113</td>\n",
       "      <td>-0.910458</td>\n",
       "      <td>-1.099205</td>\n",
       "      <td>-0.050644</td>\n",
       "      <td>-0.096732</td>\n",
       "      <td>-0.126189</td>\n",
       "      <td>-0.708929</td>\n",
       "      <td>-0.349029</td>\n",
       "      <td>-1.498801</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312205</td>\n",
       "      <td>-0.402376</td>\n",
       "      <td>-0.529048</td>\n",
       "      <td>0.176183</td>\n",
       "      <td>0.023006</td>\n",
       "      <td>0.155082</td>\n",
       "      <td>0.249782</td>\n",
       "      <td>-0.051285</td>\n",
       "      <td>2.127264</td>\n",
       "      <td>1.185766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>2.836235</td>\n",
       "      <td>2.969290</td>\n",
       "      <td>-0.928899</td>\n",
       "      <td>-1.108816</td>\n",
       "      <td>0.244345</td>\n",
       "      <td>0.161884</td>\n",
       "      <td>0.117594</td>\n",
       "      <td>-0.303996</td>\n",
       "      <td>-0.571158</td>\n",
       "      <td>-0.724516</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.668807</td>\n",
       "      <td>0.534874</td>\n",
       "      <td>0.868312</td>\n",
       "      <td>-0.222842</td>\n",
       "      <td>0.034709</td>\n",
       "      <td>-0.311144</td>\n",
       "      <td>-0.327527</td>\n",
       "      <td>-0.198309</td>\n",
       "      <td>-1.350154</td>\n",
       "      <td>-0.471722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>2.899239</td>\n",
       "      <td>3.198019</td>\n",
       "      <td>-1.037959</td>\n",
       "      <td>-1.092798</td>\n",
       "      <td>0.436371</td>\n",
       "      <td>0.327695</td>\n",
       "      <td>0.378906</td>\n",
       "      <td>-0.221332</td>\n",
       "      <td>-0.781410</td>\n",
       "      <td>-0.991838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.633410</td>\n",
       "      <td>-0.186806</td>\n",
       "      <td>0.248306</td>\n",
       "      <td>0.056213</td>\n",
       "      <td>-0.035507</td>\n",
       "      <td>0.110304</td>\n",
       "      <td>-0.142931</td>\n",
       "      <td>0.102668</td>\n",
       "      <td>-0.156328</td>\n",
       "      <td>-0.537935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>3.031675</td>\n",
       "      <td>3.223520</td>\n",
       "      <td>-0.864862</td>\n",
       "      <td>-1.099205</td>\n",
       "      <td>0.332920</td>\n",
       "      <td>0.212181</td>\n",
       "      <td>0.244257</td>\n",
       "      <td>-0.048003</td>\n",
       "      <td>-0.731256</td>\n",
       "      <td>1.173004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142633</td>\n",
       "      <td>1.498324</td>\n",
       "      <td>0.714945</td>\n",
       "      <td>0.044227</td>\n",
       "      <td>0.034709</td>\n",
       "      <td>-0.181899</td>\n",
       "      <td>-0.220180</td>\n",
       "      <td>-0.077201</td>\n",
       "      <td>-0.312480</td>\n",
       "      <td>-0.470525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>3.155218</td>\n",
       "      <td>3.312579</td>\n",
       "      <td>-0.893660</td>\n",
       "      <td>-1.102409</td>\n",
       "      <td>0.339866</td>\n",
       "      <td>0.189823</td>\n",
       "      <td>0.224874</td>\n",
       "      <td>-0.150557</td>\n",
       "      <td>-0.771399</td>\n",
       "      <td>0.627648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126462</td>\n",
       "      <td>-0.379442</td>\n",
       "      <td>-0.318908</td>\n",
       "      <td>-0.096212</td>\n",
       "      <td>-0.035507</td>\n",
       "      <td>0.523038</td>\n",
       "      <td>0.518103</td>\n",
       "      <td>0.593299</td>\n",
       "      <td>-0.405466</td>\n",
       "      <td>-0.533281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>3.274582</td>\n",
       "      <td>3.424001</td>\n",
       "      <td>-0.909964</td>\n",
       "      <td>-1.108816</td>\n",
       "      <td>0.128951</td>\n",
       "      <td>0.021743</td>\n",
       "      <td>0.014073</td>\n",
       "      <td>-0.389726</td>\n",
       "      <td>-0.581232</td>\n",
       "      <td>0.224044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559895</td>\n",
       "      <td>-0.281229</td>\n",
       "      <td>-0.438308</td>\n",
       "      <td>-0.000533</td>\n",
       "      <td>-0.023804</td>\n",
       "      <td>0.872123</td>\n",
       "      <td>0.801915</td>\n",
       "      <td>0.699569</td>\n",
       "      <td>0.540649</td>\n",
       "      <td>0.876827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>3.376160</td>\n",
       "      <td>3.280801</td>\n",
       "      <td>-0.965482</td>\n",
       "      <td>-1.105612</td>\n",
       "      <td>0.024667</td>\n",
       "      <td>-0.018923</td>\n",
       "      <td>-0.147911</td>\n",
       "      <td>-0.486183</td>\n",
       "      <td>-0.550323</td>\n",
       "      <td>1.621165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085281</td>\n",
       "      <td>0.966407</td>\n",
       "      <td>0.792565</td>\n",
       "      <td>0.219206</td>\n",
       "      <td>0.069817</td>\n",
       "      <td>0.570167</td>\n",
       "      <td>0.516898</td>\n",
       "      <td>0.749350</td>\n",
       "      <td>-0.187857</td>\n",
       "      <td>0.251165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>3.522311</td>\n",
       "      <td>3.160747</td>\n",
       "      <td>-1.026549</td>\n",
       "      <td>-1.105612</td>\n",
       "      <td>-0.044831</td>\n",
       "      <td>-0.095691</td>\n",
       "      <td>-0.223774</td>\n",
       "      <td>-0.994028</td>\n",
       "      <td>-0.513804</td>\n",
       "      <td>0.303738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102002</td>\n",
       "      <td>-0.914373</td>\n",
       "      <td>-0.439485</td>\n",
       "      <td>-0.026939</td>\n",
       "      <td>-0.012102</td>\n",
       "      <td>-0.296372</td>\n",
       "      <td>-0.347074</td>\n",
       "      <td>-0.375127</td>\n",
       "      <td>1.513213</td>\n",
       "      <td>0.722060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>3.622496</td>\n",
       "      <td>2.990476</td>\n",
       "      <td>-1.016312</td>\n",
       "      <td>-1.108816</td>\n",
       "      <td>-0.023756</td>\n",
       "      <td>-0.066886</td>\n",
       "      <td>-0.243158</td>\n",
       "      <td>-1.042688</td>\n",
       "      <td>-0.397899</td>\n",
       "      <td>-1.797134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.338581</td>\n",
       "      <td>-1.457185</td>\n",
       "      <td>-1.687210</td>\n",
       "      <td>-0.174794</td>\n",
       "      <td>-0.000399</td>\n",
       "      <td>0.027389</td>\n",
       "      <td>-0.060984</td>\n",
       "      <td>-0.047786</td>\n",
       "      <td>-0.382316</td>\n",
       "      <td>-0.321223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>3.704786</td>\n",
       "      <td>2.823735</td>\n",
       "      <td>-0.921068</td>\n",
       "      <td>-1.108816</td>\n",
       "      <td>-0.192953</td>\n",
       "      <td>-0.153295</td>\n",
       "      <td>-0.437796</td>\n",
       "      <td>-1.237392</td>\n",
       "      <td>-0.247280</td>\n",
       "      <td>0.804591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442033</td>\n",
       "      <td>1.801020</td>\n",
       "      <td>1.664571</td>\n",
       "      <td>0.656181</td>\n",
       "      <td>-0.012102</td>\n",
       "      <td>-0.611525</td>\n",
       "      <td>-0.508509</td>\n",
       "      <td>-0.591390</td>\n",
       "      <td>-0.283173</td>\n",
       "      <td>-0.441713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -0.767621 -0.689589 -0.774685 -0.099709  0.519319  0.428291  0.379438   \n",
       "1   -0.765478 -0.686961 -0.817516 -0.067674  0.380785  0.297435  0.240222   \n",
       "2   -0.763335 -0.684293 -0.791125  0.140554  0.406691  0.348599  0.261890   \n",
       "3   -0.761192 -0.681664 -0.948962  0.348782  0.279979  0.228911  0.170875   \n",
       "4   -0.759049 -0.678996 -1.205563  0.249473  0.553026  0.489048  0.435564   \n",
       "5   -0.755834 -0.676721 -1.158236  0.358393  0.852110  0.793800  0.808729   \n",
       "6   -0.752620 -0.674484 -1.228362  0.422463  1.424413  1.352177  1.352862   \n",
       "7   -0.749405 -0.672248 -1.260999  0.351986  2.004710  1.933677  1.958135   \n",
       "8   -0.746191 -0.669973 -0.910407 -0.144558  2.294717  2.198710  2.201640   \n",
       "9   -0.745869 -0.676250 -0.982851 -0.170187  2.898424  2.731137  2.818806   \n",
       "10  -0.745655 -0.682527 -0.667222 -0.509759  2.762247  2.624753  2.674777   \n",
       "11  -0.745334 -0.688805 -0.509109 -0.548201  2.645580  2.519002  2.581993   \n",
       "12  -0.745119 -0.695082 -0.100182 -0.641103  2.905582  2.978149  2.838161   \n",
       "13  -0.749405 -0.698613 -0.196157 -0.673138  2.793012  2.840675  2.759195   \n",
       "14  -0.753691 -0.702144  0.096583 -0.939029  2.976317  3.225152  2.922511   \n",
       "15  -0.757977 -0.705675  1.357994 -0.971065  2.920299  3.287915  2.901358   \n",
       "16  -0.762263 -0.709206  2.207338 -0.343176  3.032685  3.745551  2.960875   \n",
       "17  -0.770835 -0.711167  2.264018 -0.394432  2.092817  2.579710  1.992831   \n",
       "18  -0.779407 -0.713129  5.420504 -1.006303  0.920548  1.859491  0.635526   \n",
       "19  -0.787979 -0.715091  2.370793 -1.105612  0.226078  0.607336  0.029099   \n",
       "20  -0.796551 -0.717052  3.223840 -1.102409 -0.877133 -0.230066 -1.081854   \n",
       "21  -0.798158 -0.716738  3.352892 -0.685952 -0.846100 -0.111771 -1.057352   \n",
       "22  -0.799766 -0.716464  0.944061 -1.092798 -0.322217  0.022696 -0.504936   \n",
       "23  -0.801373 -0.716150  1.073275 -1.102409  0.000569  0.394327 -0.146063   \n",
       "24  -0.802980 -0.715875  0.927289 -1.022321 -0.315303  0.171598 -0.435271   \n",
       "25  -0.802659 -0.715365  0.827967 -1.038338 -0.225367  0.007400 -0.324069   \n",
       "26  -0.802444 -0.714895  0.971137 -1.067170  0.079037  0.294931 -0.008063   \n",
       "27  -0.802123 -0.714384  1.082145 -1.047949 -0.432139 -0.081133 -0.487298   \n",
       "28  -0.801908 -0.713914  0.720188 -1.041542  0.230453  0.502465  0.185250   \n",
       "29  -0.801908 -0.704498  0.821381 -1.067170  0.025577  0.333416 -0.036231   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "322  2.226342  1.282266 -1.150029 -0.519369  0.954547  0.957295  0.877336   \n",
       "323  2.241557  1.069623 -1.102708 -0.753226  0.028760  0.142886 -0.102798   \n",
       "324  2.191518 -0.149349 -0.848498 -1.105612 -0.018045  0.481298 -0.343254   \n",
       "325  2.070226 -0.463999 -0.507021 -1.047949  0.382175  0.757005  0.082742   \n",
       "326  1.892251 -0.438497 -0.672838 -1.057559  0.387001  0.532436  0.239355   \n",
       "327  1.710847 -0.241155 -0.868962 -1.076780  0.191993  0.190294  0.092966   \n",
       "328  1.550552  1.266573 -0.958833 -1.099205 -0.054703 -0.058300 -0.058112   \n",
       "329  1.496870  1.657335 -0.747752 -1.067170 -0.253856 -0.270884 -0.264112   \n",
       "330  1.511121  1.899403 -0.589300 -1.076780 -0.416249 -0.346417 -0.533652   \n",
       "331  1.544980  2.086153 -0.738658 -1.067170 -0.292571 -0.278534 -0.407217   \n",
       "332  1.585268  2.301542 -0.836825 -1.070373 -0.104639 -0.112142 -0.157313   \n",
       "333  1.660487  2.456905 -0.851174 -1.083187  0.150089  0.088425  0.112725   \n",
       "334  1.757993  2.557342 -0.861288 -1.102409  0.040908 -0.045848 -0.025374   \n",
       "335  1.847998  2.679357 -0.680156 -1.112019 -0.113572 -0.098335 -0.301270   \n",
       "336  1.981292  2.678180 -0.840076 -1.112019 -0.088400 -0.043414 -0.277945   \n",
       "337  2.080726  2.740561 -0.881472 -1.089594 -0.139400 -0.109383 -0.243408   \n",
       "338  2.184339  2.716236 -0.848050 -1.086391 -0.088912 -0.030550 -0.264022   \n",
       "339  2.320419  2.660525 -0.903196 -1.079984 -0.197401 -0.139699 -0.336408   \n",
       "340  2.497965  2.660918 -0.870794 -1.092798 -0.162182 -0.188740 -0.302811   \n",
       "341  2.590649  2.707605 -0.881667 -1.086391 -0.226600 -0.262935 -0.332552   \n",
       "342  2.714299  2.835113 -0.910458 -1.099205 -0.050644 -0.096732 -0.126189   \n",
       "343  2.836235  2.969290 -0.928899 -1.108816  0.244345  0.161884  0.117594   \n",
       "344  2.899239  3.198019 -1.037959 -1.092798  0.436371  0.327695  0.378906   \n",
       "345  3.031675  3.223520 -0.864862 -1.099205  0.332920  0.212181  0.244257   \n",
       "346  3.155218  3.312579 -0.893660 -1.102409  0.339866  0.189823  0.224874   \n",
       "347  3.274582  3.424001 -0.909964 -1.108816  0.128951  0.021743  0.014073   \n",
       "348  3.376160  3.280801 -0.965482 -1.105612  0.024667 -0.018923 -0.147911   \n",
       "349  3.522311  3.160747 -1.026549 -1.105612 -0.044831 -0.095691 -0.223774   \n",
       "350  3.622496  2.990476 -1.016312 -1.108816 -0.023756 -0.066886 -0.243158   \n",
       "351  3.704786  2.823735 -0.921068 -1.108816 -0.192953 -0.153295 -0.437796   \n",
       "\n",
       "           7         8         9     ...           20        21        22  \\\n",
       "0    2.381861 -0.910326  0.266007    ...     0.421662  0.185423 -0.071273   \n",
       "1    1.486580 -0.762275 -0.164262    ...     0.434384 -0.299704 -0.089852   \n",
       "2    1.321980 -0.762275 -0.400750    ...    -0.006831 -0.165443 -0.361633   \n",
       "3    0.418281 -0.616790 -0.707137    ...     0.426733 -0.213873  0.307189   \n",
       "4    1.839043 -0.908595  0.040092    ...    -0.876449  0.516128  0.044673   \n",
       "5    2.516687 -1.205575 -1.142770    ...    -0.891871 -0.821140 -0.234620   \n",
       "6    3.990253 -1.799575  0.157329    ...    -1.777035  0.899185  0.051657   \n",
       "7    4.200511 -2.412188 -0.329478    ...    -1.832500 -0.338877  0.090419   \n",
       "8    5.906464 -2.754837  0.828413    ...    -1.027973  0.800656  0.279277   \n",
       "9    5.176581 -3.412602  0.018082    ...    -1.967060 -0.563031  0.037204   \n",
       "10   4.589664 -3.279654  0.060034    ...     0.389372  0.027475 -0.029610   \n",
       "11   4.435772 -3.192725 -0.053735    ...     0.252228 -0.080417  0.079835   \n",
       "12   3.896661 -3.445420 -0.300079    ...    -0.759896 -0.172271 -0.565611   \n",
       "13   2.280408 -3.357836 -0.171356    ...     0.254181  0.087595  0.671341   \n",
       "14   1.866470 -3.561285  0.192956    ...    -0.613136  0.250823 -0.183756   \n",
       "15   0.514341 -3.453597 -0.979999    ...     0.314093 -0.814276 -0.330166   \n",
       "16  -1.238768 -3.379904 -1.424245    ...     0.212785 -0.309388 -1.393125   \n",
       "17  -1.055398 -2.461043  0.607710    ...     2.731502  1.406253  1.403731   \n",
       "18  -2.362157 -1.100015  0.699302    ...     4.049219  0.061868 -0.220421   \n",
       "19  -0.686104 -0.536535  0.853027    ...     1.672420  0.104917  1.409625   \n",
       "20  -0.918381  0.614780 -0.000621    ...     3.424244 -0.593043 -0.817408   \n",
       "21  -1.600695  0.604266 -0.352722    ...    -0.038162 -0.245546 -0.396539   \n",
       "22  -0.683813 -0.010978  0.375464    ...    -1.840343  0.502934  1.036507   \n",
       "23  -0.733209 -0.381107 -0.206980    ...    -1.109868 -0.405139 -0.717279   \n",
       "24  -0.463776 -0.008299 -1.063143    ...     1.104188 -0.594786 -0.319750   \n",
       "25  -0.325683 -0.197538  0.852764    ...    -0.570790  1.325849  0.898785   \n",
       "26  -0.472403 -0.573192  0.388347    ...    -1.126331 -0.323363 -0.395126   \n",
       "27   0.072535 -0.019349 -0.793544    ...     1.643699 -0.820468 -0.520688   \n",
       "28   0.128152 -0.765212  0.402834    ...    -2.229607  0.827322  0.487300   \n",
       "29  -0.226810 -0.581498  0.367790    ...     0.540662 -0.025872  0.102414   \n",
       "..        ...       ...       ...    ...          ...       ...       ...   \n",
       "322 -2.675697 -1.068394 -0.823140    ...     0.100116 -0.859490 -0.218477   \n",
       "323 -2.753261 -0.086557  0.395719    ...     2.919178  0.842898 -0.973682   \n",
       "324 -1.718515 -0.187550  4.125837    ...    -0.307802  2.582829  5.131648   \n",
       "325 -2.244807 -0.502005 -1.618973    ...    -0.943951 -3.981896 -5.760390   \n",
       "326 -1.627570 -0.479477 -2.042352    ...     0.060305 -0.294931  3.326101   \n",
       "327 -0.717481 -0.391883  0.610392    ...     0.254213  1.836368  0.458202   \n",
       "328 -0.287832 -0.095519 -1.500013    ...     0.876374 -1.463790 -2.132624   \n",
       "329 -0.153425  0.119832 -0.073969    ...     0.634945  0.986446  0.638493   \n",
       "330  0.043060  0.195223  2.332615    ...     0.217845  1.665816  1.060850   \n",
       "331 -0.583980 -0.025042  0.895628    ...    -0.663253 -0.997211 -0.550861   \n",
       "332 -0.206271 -0.102558 -2.076139    ...    -0.237837 -2.060587 -1.274514   \n",
       "333 -0.210724 -0.347100 -0.509907    ...    -0.735600  1.083576  0.286639   \n",
       "334 -0.240175 -0.265654  0.484146    ...     0.235887  0.687140  0.639166   \n",
       "335 -0.180138 -0.327081  4.229796    ...    -0.189890  2.593591  1.704492   \n",
       "336 -0.978027 -0.394252  0.127043    ...    -0.207010 -2.844193 -1.783220   \n",
       "337 -1.273100 -0.324648 -1.366243    ...     0.200599 -1.036218 -0.469565   \n",
       "338 -1.517643 -0.466695  1.676559    ...    -0.430150  2.106621  1.174458   \n",
       "339 -1.453840 -0.388506 -0.252597    ...     0.226182 -1.338211 -0.468046   \n",
       "340 -1.211290 -0.385875 -0.474839    ...     0.001011 -0.155572 -0.533660   \n",
       "341 -1.101262 -0.246559 -0.920344    ...     0.408351 -0.310261 -0.466859   \n",
       "342 -0.708929 -0.349029 -1.498801    ...    -0.312205 -0.402376 -0.529048   \n",
       "343 -0.303996 -0.571158 -0.724516    ...    -0.668807  0.534874  0.868312   \n",
       "344 -0.221332 -0.781410 -0.991838    ...    -0.633410 -0.186806  0.248306   \n",
       "345 -0.048003 -0.731256  1.173004    ...     0.142633  1.498324  0.714945   \n",
       "346 -0.150557 -0.771399  0.627648    ...    -0.126462 -0.379442 -0.318908   \n",
       "347 -0.389726 -0.581232  0.224044    ...     0.559895 -0.281229 -0.438308   \n",
       "348 -0.486183 -0.550323  1.621165    ...     0.085281  0.966407  0.792565   \n",
       "349 -0.994028 -0.513804  0.303738    ...     0.102002 -0.914373 -0.439485   \n",
       "350 -1.042688 -0.397899 -1.797134    ...     0.338581 -1.457185 -1.687210   \n",
       "351 -1.237392 -0.247280  0.804591    ...     0.442033  1.801020  1.664571   \n",
       "\n",
       "           23        24        25        26        27        28        29  \n",
       "0    0.018805 -0.398291 -0.782983 -0.821576 -0.870520  0.451925  0.085675  \n",
       "1   -0.014161  0.163439  1.163271  1.096738  1.035651  0.114856 -0.002960  \n",
       "2    0.313814 -0.140831 -2.005689 -1.949039 -1.937349 -0.581140 -0.198327  \n",
       "3   -0.269523 -0.445101  0.777854  0.708668  0.770342  0.164790  0.242523  \n",
       "4    0.207974  0.584736 -0.442798 -0.450921 -0.479058  0.187893 -0.071330  \n",
       "5    0.129286  0.128331 -0.399259 -0.394191 -0.388096 -0.301823 -0.090078  \n",
       "6    0.030703  0.771979  0.082991  0.164810  0.068793 -0.166292 -0.364338  \n",
       "7   -0.028619  0.771979 -0.364587 -0.359895 -0.251214 -0.215180  0.310585  \n",
       "8    7.089744 -0.351480  0.807774  0.806583  0.758939  0.521726  0.045674  \n",
       "9   -7.354659  0.409195  0.884135  0.943601  1.066998 -0.828192 -0.236167  \n",
       "10   1.261790  0.245358  1.685392  1.722505  1.552522  0.908407  0.052722  \n",
       "11  -0.645896 -0.246156  1.708838  1.793519  1.726152 -0.341366  0.091837  \n",
       "12   1.191043 -1.802614  0.857512  0.821622  0.698780  0.808945  0.282419  \n",
       "13  -1.227832 -0.082318  1.777490  1.642809  1.759927 -0.567640  0.038137  \n",
       "14   1.897846 -1.229182 -0.392345 -0.319034 -0.401763  0.028451 -0.029287  \n",
       "15  -0.149046 -0.129129 -0.335127 -0.317092 -0.256237 -0.080461  0.081157  \n",
       "16   3.084688 -0.328074  0.769517  1.417760  0.734741 -0.173185 -0.570177  \n",
       "17  -1.359133 -0.105723 -0.323112 -0.414516 -0.216994  0.089139  0.678059  \n",
       "18   1.525357 -0.960020  0.544592  1.188443  0.471054  0.253912 -0.184839  \n",
       "19   2.006815 -0.105723 -0.157266  0.200431 -0.052815 -0.821262 -0.332584  \n",
       "20  -2.638109  2.305032  0.336609  1.413119  0.176278 -0.311599 -1.405241  \n",
       "21  -0.782964 -0.175939 -2.749304 -3.572722 -2.741859  1.420272  1.417131  \n",
       "22   0.144224 -2.223911 -3.430859 -2.204179 -3.847307  0.063170 -0.221838  \n",
       "23  -0.150958 -0.351480 -2.029634 -3.837804 -1.714912  0.106625  1.423078  \n",
       "24  -1.245052  0.023006 -3.228336 -2.564060 -3.147701 -0.597937 -0.824272  \n",
       "25  -1.870190  1.532654  0.098028  0.370974  0.076841 -0.247152 -0.399563  \n",
       "26  -0.188237 -1.474939  1.543393  0.420641  1.576045  0.508407  1.046556  \n",
       "27   0.514511 -0.023804  0.953642  1.148990  1.026410 -0.408255 -0.723228  \n",
       "28  -1.069688  0.303871 -0.919330 -0.676340 -0.814053 -0.599696 -0.322074  \n",
       "29   0.034693 -0.047210  0.270771 -0.496589  0.323057  1.339106  0.907578  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "322 -0.496480 -0.374885  0.401293  0.356687  0.253986 -0.433020 -0.301789  \n",
       "323  1.824228 -0.831290  3.185336  3.110535  2.969010  1.061640  0.721314  \n",
       "324  6.368563 -1.030236  0.167547  0.378253  0.070451  0.146491 -0.097099  \n",
       "325 -5.555482 -2.024965  0.762035  0.904807  0.617938 -0.390538 -0.501455  \n",
       "326 -1.918137  0.713465  0.025953  0.033241  0.101213 -0.866904 -0.219877  \n",
       "327 -0.736489 -0.842993 -2.708011 -2.493444 -2.776191  0.851588 -0.981971  \n",
       "328 -0.081000 -1.275993 -0.130246  1.046972 -0.675606  2.607977  5.179055  \n",
       "329 -0.143025  0.221952  1.180732  0.854401  1.217030 -4.018846 -5.812346  \n",
       "330  0.806166 -0.023804  0.021172 -0.681995  0.452017 -0.297004  3.357037  \n",
       "331 -0.548614 -0.058912 -0.564876 -1.043069 -0.408466  1.854455  0.462975  \n",
       "332 -0.366757 -0.070615 -0.716462 -0.755776 -0.421782 -1.476920 -2.151486  \n",
       "333  0.046391  0.128331 -0.577031 -0.645185 -0.577755  0.996493  0.644911  \n",
       "334 -0.033410 -0.023804 -0.469228 -0.224289 -0.758200  1.682289  1.071121  \n",
       "335  1.853809  0.046412  0.369725  0.216153  0.366318 -1.005928 -0.555292  \n",
       "336 -0.781223 -0.000399  0.558160  0.518685  0.716952 -2.079362 -1.285547  \n",
       "337 -1.188721 -0.035507  0.754052  0.623638  0.774129  1.094542  0.289848  \n",
       "338  0.316439 -0.058912 -0.313174 -0.404685 -0.384924  0.694356  0.645590  \n",
       "339 -0.250944 -0.023804 -0.446022 -0.153512 -0.776251  2.618840  1.720635  \n",
       "340  0.039711  0.011304  0.080839  0.176344  0.073499 -2.870381 -1.798894  \n",
       "341 -0.096483  0.093223 -0.142546 -0.194917  0.105339 -1.045304 -0.473255  \n",
       "342  0.176183  0.023006  0.155082  0.249782 -0.051285  2.127264  1.185766  \n",
       "343 -0.222842  0.034709 -0.311144 -0.327527 -0.198309 -1.350154 -0.471722  \n",
       "344  0.056213 -0.035507  0.110304 -0.142931  0.102668 -0.156328 -0.537935  \n",
       "345  0.044227  0.034709 -0.181899 -0.220180 -0.077201 -0.312480 -0.470525  \n",
       "346 -0.096212 -0.035507  0.523038  0.518103  0.593299 -0.405466 -0.533281  \n",
       "347 -0.000533 -0.023804  0.872123  0.801915  0.699569  0.540649  0.876827  \n",
       "348  0.219206  0.069817  0.570167  0.516898  0.749350 -0.187857  0.251165  \n",
       "349 -0.026939 -0.012102 -0.296372 -0.347074 -0.375127  1.513213  0.722060  \n",
       "350 -0.174794 -0.000399  0.027389 -0.060984 -0.047786 -0.382316 -0.321223  \n",
       "351  0.656181 -0.012102 -0.611525 -0.508509 -0.591390 -0.283173 -0.441713  \n",
       "\n",
       "[352 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_obs, num_features = dataset.shape\n",
    "num_features -=1\n",
    "num_labels=1\n",
    "print(\"Observations: %d\\nFeatures: %d\" % (num_obs, num_features))\n",
    "\n",
    "# last column is target \n",
    "y=dataset[:,num_features].astype(float)\n",
    "print(\"Histogram: check all 0s and 1s, no -1s etc.\")\n",
    "pprint(np.histogram(y))\n",
    "\n",
    "# omit 1st id column\n",
    "X_raw = dataset[:,1:num_features].astype(float)\n",
    "num_features -=1\n",
    "del dataset\n",
    "\n",
    "# normalize\n",
    "# not necessary for NN but may speed convergence, lets pca work\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# X = min_max_scaler.fit_transform(X_raw)\n",
    "X = preprocessing.scale(X_raw)\n",
    "\n",
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into training, xval, test\n",
      "Split into train, xval\n",
      "Training set\n",
      "(210, 30)\n",
      "(array([103,   0,   0,   0,   0,   0,   0,   0,   0, 107]),\n",
      " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]))\n",
      "Xval set\n",
      "(71, 30)\n",
      "(array([36,  0,  0,  0,  0,  0,  0,  0,  0, 35]),\n",
      " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]))\n",
      "Test set\n",
      "(71, 30)\n",
      "(array([37,  0,  0,  0,  0,  0,  0,  0,  0, 34]),\n",
      " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]))\n"
     ]
    }
   ],
   "source": [
    "print(\"Split into training, xval, test\")\n",
    "# split into training, xval, test, 60/20/20\n",
    "X_bigtrain, X_test, y_bigtrain, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(\"Split into train, xval\")\n",
    "X_train, X_xval, y_train, y_xval = train_test_split(X_bigtrain, y_bigtrain, test_size=0.25)\n",
    "\n",
    "print \"Training set\"\n",
    "print X_train.shape\n",
    "pprint(np.histogram(y_train))\n",
    "\n",
    "print \"Xval set\"\n",
    "print X_xval.shape\n",
    "pprint(np.histogram(y_xval))\n",
    "\n",
    "print \"Test set\"\n",
    "print X_test.shape\n",
    "pprint(np.histogram(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 18.29  34.6   45.39  55.11  63.01  68.51  73.64  77.94  82.02  85.13\n",
      "  88.11  90.65  92.88  94.82  96.29  97.47  98.44  98.86  99.22  99.54\n",
      "  99.65  99.76  99.83  99.89  99.93  99.95  99.97  99.98  99.99  99.99]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f37dc088310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# principal component analysis\n",
    "# scree chart to see how much variation is explained by how many predictors\n",
    "\n",
    "# we can predict using PCA components for dimensionality reduction when we have too many/collinear columns\n",
    "# can speed things up or sometimes get a better result\n",
    "# but won't do that here\n",
    "# merely exploratory to understand the data, see that it's scaled\n",
    "\n",
    "pca = PCA(n_components=num_features)\n",
    "pca.fit(X_train)\n",
    "\n",
    "#The amount of variance that each PC explains\n",
    "var= pca.explained_variance_ratio_\n",
    "\n",
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "\n",
    "#print(var1)\n",
    "%matplotlib inline\n",
    "plt.plot(var1)\n",
    "print(var1)\n",
    "# looks like ~10 orthogonal PCA components explain > 80% of the variation\n",
    "\n",
    "num_pca_components=15\n",
    "pca = PCA(n_components=num_pca_components)\n",
    "pca.fit(X_train)\n",
    "\n",
    "pca_train=pca.transform(preprocessing.normalize(X_train))\n",
    "pca_bigtrain=pca.transform(preprocessing.normalize(X_bigtrain))\n",
    "pca_xval=pca.transform(preprocessing.normalize(X_xval))\n",
    "pca_test=pca.transform(preprocessing.normalize(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to generate Keras feed-forward neural network model\n",
    "\n",
    "def declare_model(num_components=num_features, \n",
    "                  hidden_layer_size=30, \n",
    "                  dropout=(1.0/3.0), \n",
    "                  reg_penalty=0.0001, \n",
    "                  activation='relu'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # 1 hidden layer of specified size hidden_layer_size, specified L1 regularization, specified activation\n",
    "    model.add(Dense(hidden_layer_size, \n",
    "                    input_dim=num_components, \n",
    "                    kernel_initializer='TruncatedNormal', \n",
    "                    kernel_regularizer=regularizers.l1(reg_penalty),\n",
    "                    activation=activation\n",
    "                   ))\n",
    "    # 1 dropout layer\n",
    "    model.add(Dropout(dropout))\n",
    "    # send outputs to sigmoid layer for binary classification\n",
    "    model.add(Dense(1, \n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='TruncatedNormal', \n",
    "                    kernel_regularizer=regularizers.l1(reg_penalty)\n",
    "                   ))\n",
    "        # Compile model\n",
    "    return model\n",
    "\n",
    "def create_model(num_components=num_features, \n",
    "                 hidden_layer_size=30, \n",
    "                 dropout=(1.0/3.0), \n",
    "                 reg_penalty=0.0001, \n",
    "                 activation='relu'):\n",
    "    \n",
    "    model = declare_model(num_components=num_components, \n",
    "                 hidden_layer_size=hidden_layer_size, \n",
    "                 dropout=dropout, \n",
    "                 reg_penalty=reg_penalty, \n",
    "                 activation=activation)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:00:32 Training...\n",
      "03:00:39 Train Accuracy 0.976, Train F1 0.977\n",
      "03:00:39 Confusion matrix (train):\n",
      "[[101   3]\n",
      " [  2 104]]\n",
      "03:00:39 Xval Accuracy 0.535, Xval F1 0.267\n",
      "03:00:39 Confusion matrix (xval):\n",
      "[[32 29]\n",
      " [ 4  6]]\n",
      "03:00:39 Test Accuracy 0.563, Test F1 0.311\n",
      "03:00:39 Confusion matrix (xval):\n",
      "[[33 27]\n",
      " [ 4  7]]\n"
     ]
    }
   ],
   "source": [
    "# run this a couple of different ways\n",
    "# 1st, run a simple model\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "ac='sigmoid'    # sigmoid activation\n",
    "hl = 8          # 8 hidden units\n",
    "dr = 0          # no dropout\n",
    "rp = 0          # no regularization\n",
    "\n",
    "# create model\n",
    "model = declare_model(num_components=num_features,\n",
    "                     hidden_layer_size=hl, \n",
    "                     dropout=dr, \n",
    "                     reg_penalty=rp,\n",
    "                     activation=ac)\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=optimizers.SGD(lr=1, clipnorm=1.),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# fit model on training data\n",
    "print('%s Training...' % time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "fit = model.fit(X_train, y_train, validation_data=(X_xval, y_xval), \n",
    "                                    epochs=1000, \n",
    "                                    batch_size=X_train.shape[0], # small data, full batch, no sgd \n",
    "                                    verbose=False)\n",
    "# See how it does on training data\n",
    "# predict probabilities\n",
    "y_train_prob = model.predict(X_train)\n",
    "                \n",
    "# select threshold that maximizes accuracy\n",
    "thresh, score = selectThresholdAcc(y_train_prob, y_train)\n",
    "\n",
    "# predict true if predicted prob > threshold\n",
    "y_train_pred = y_train_prob >= thresh\n",
    "         \n",
    "# show training accuracy and F1\n",
    "# (see https://en.wikipedia.org/wiki/F1_score , https://en.wikipedia.org/wiki/Receiver_operating_characteristic )\n",
    "print(\"%s Train Accuracy %.3f, Train F1 %.3f\" % \n",
    "      (time.strftime(\"%H:%M:%S\"),\n",
    "       sklearn.metrics.accuracy_score(y_train_pred, y_train), \n",
    "       sklearn.metrics.f1_score(y_train_pred, y_train)))\n",
    "                \n",
    "# show training set confusion matrix \n",
    "# True negative      False negative\n",
    "# False postiive     True positive\n",
    "\n",
    "print \"%s Confusion matrix (train):\" % time.strftime(\"%H:%M:%S\")\n",
    "print(sklearn.metrics.confusion_matrix(y_train_pred, y_train))\n",
    "\n",
    "# same in cross-validation set\n",
    "y_xval_prob = model.predict(X_xval)\n",
    "thresh, score = selectThresholdAcc(y_xval_prob, y_xval)\n",
    "y_xval_pred = y_xval_prob >= thresh\n",
    "\n",
    "print(\"%s Xval Accuracy %.3f, Xval F1 %.3f\" % \n",
    "      (time.strftime(\"%H:%M:%S\"),\n",
    "       sklearn.metrics.accuracy_score(y_xval_pred, y_xval), \n",
    "       sklearn.metrics.f1_score(y_xval_pred, y_xval)))\n",
    "\n",
    "print \"%s Confusion matrix (xval):\" % time.strftime(\"%H:%M:%S\")\n",
    "print(sklearn.metrics.confusion_matrix(y_xval_pred, y_xval))\n",
    "\n",
    "# same in test\n",
    "# note we use threshold selected using xval\n",
    "y_test_prob = model.predict(X_test)\n",
    "y_test_pred = y_test_prob >= thresh\n",
    "print(\"%s Test Accuracy %.3f, Test F1 %.3f\" % \n",
    "      (time.strftime(\"%H:%M:%S\"),\n",
    "       sklearn.metrics.accuracy_score(y_test_pred, y_test), \n",
    "       sklearn.metrics.f1_score(y_test_pred, y_test)))\n",
    "\n",
    "print \"%s Confusion matrix (xval):\" % time.strftime(\"%H:%M:%S\")\n",
    "print(sklearn.metrics.confusion_matrix(y_test_pred, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3767c68990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3767b88e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3767a89fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# xval loss, proxy for total error \n",
    "total_loss = np.array(fit.history['val_loss'][:200])\n",
    "# training loss, proxy for bias error\n",
    "bias = np.array(fit.history['loss'][:200])\n",
    "# difference, proxy for variance error\n",
    "variance = total_loss - bias\n",
    "\n",
    "plt.plot(total_loss)\n",
    "plt.ylabel('Total loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(bias)\n",
    "plt.ylabel('Bias loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(variance)\n",
    "plt.ylabel('Variance loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n",
    "\n",
    "# note that training loss (bias) declines continuously\n",
    "# total loss declines, reaches a minimum, then climbs as overfitting and variance increases\n",
    "# http://scott.fortmann-roe.com/docs/BiasVariance.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1f8DP/NHw43kMbIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3767a24fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROC curve\n",
    " \n",
    "(fpr, tpr, thresholds) = sklearn.metrics.roc_curve(y_xval, y_xval_prob)\n",
    "roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (AUC = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# AUC of a coinflip is 0.5 so area under curve (AUC) of 0.494 is terrible\n",
    "# even though some thresholds have predictive value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.02777778,  0.02777778,  0.08333333,  0.08333333,\n",
       "        0.11111111,  0.11111111,  0.16666667,  0.16666667,  0.25      ,\n",
       "        0.25      ,  0.27777778,  0.27777778,  0.33333333,  0.33333333,\n",
       "        0.36111111,  0.36111111,  0.38888889,  0.38888889,  0.5       ,\n",
       "        0.5       ,  0.61111111,  0.61111111,  0.66666667,  0.66666667,\n",
       "        0.72222222,  0.72222222,  0.75      ,  0.75      ,  0.77777778,\n",
       "        0.77777778,  0.88888889,  0.88888889,  0.97222222,  0.97222222,  1.        ])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:22:18 Starting\n",
      "00:30:20 Finishing\n",
      "Best Xval: 0.540925 using {'activation': 'relu', 'hidden_layer_size': 8, 'dropout': 0.333, 'reg_penalty': 0}\n",
      "0.508897 (0.067146) with: {'activation': 'relu', 'hidden_layer_size': 4, 'dropout': 0.333, 'reg_penalty': 0}\n",
      "0.483986 (0.029291) with: {'activation': 'relu', 'hidden_layer_size': 4, 'dropout': 0.333, 'reg_penalty': 0.001}\n",
      "0.483986 (0.023241) with: {'activation': 'relu', 'hidden_layer_size': 4, 'dropout': 0.333, 'reg_penalty': 0.003}\n",
      "0.505338 (0.004347) with: {'activation': 'relu', 'hidden_layer_size': 4, 'dropout': 0.333, 'reg_penalty': 0.01}\n",
      "0.505338 (0.004347) with: {'activation': 'relu', 'hidden_layer_size': 4, 'dropout': 0.333, 'reg_penalty': 0.03}\n",
      "0.505338 (0.004347) with: {'activation': 'relu', 'hidden_layer_size': 4, 'dropout': 0.333, 'reg_penalty': 0.1}\n",
      "0.540925 (0.016503) with: {'activation': 'relu', 'hidden_layer_size': 8, 'dropout': 0.333, 'reg_penalty': 0}\n",
      "0.480427 (0.050019) with: {'activation': 'relu', 'hidden_layer_size': 8, 'dropout': 0.333, 'reg_penalty': 0.001}\n",
      "0.494662 (0.014398) with: {'activation': 'relu', 'hidden_layer_size': 8, 'dropout': 0.333, 'reg_penalty': 0.003}\n",
      "0.505338 (0.004347) with: {'activation': 'relu', 'hidden_layer_size': 8, 'dropout': 0.333, 'reg_penalty': 0.01}\n",
      "0.505338 (0.004347) with: {'activation': 'relu', 'hidden_layer_size': 8, 'dropout': 0.333, 'reg_penalty': 0.03}\n",
      "0.505338 (0.004347) with: {'activation': 'relu', 'hidden_layer_size': 8, 'dropout': 0.333, 'reg_penalty': 0.1}\n",
      "0.519573 (0.036291) with: {'activation': 'relu', 'hidden_layer_size': 16, 'dropout': 0.333, 'reg_penalty': 0}\n",
      "0.480427 (0.066484) with: {'activation': 'relu', 'hidden_layer_size': 16, 'dropout': 0.333, 'reg_penalty': 0.001}\n",
      "0.501779 (0.040416) with: {'activation': 'relu', 'hidden_layer_size': 16, 'dropout': 0.333, 'reg_penalty': 0.003}\n",
      "0.505338 (0.004347) with: {'activation': 'relu', 'hidden_layer_size': 16, 'dropout': 0.333, 'reg_penalty': 0.01}\n",
      "0.505338 (0.004347) with: {'activation': 'relu', 'hidden_layer_size': 16, 'dropout': 0.333, 'reg_penalty': 0.03}\n",
      "0.505338 (0.004347) with: {'activation': 'relu', 'hidden_layer_size': 16, 'dropout': 0.333, 'reg_penalty': 0.1}\n",
      "0.508897 (0.028429) with: {'activation': 'relu', 'hidden_layer_size': 32, 'dropout': 0.333, 'reg_penalty': 0}\n",
      "0.523132 (0.049334) with: {'activation': 'relu', 'hidden_layer_size': 32, 'dropout': 0.333, 'reg_penalty': 0.001}\n",
      "0.498221 (0.050833) with: {'activation': 'relu', 'hidden_layer_size': 32, 'dropout': 0.333, 'reg_penalty': 0.003}\n",
      "0.505338 (0.004347) with: {'activation': 'relu', 'hidden_layer_size': 32, 'dropout': 0.333, 'reg_penalty': 0.01}\n",
      "0.505338 (0.004347) with: {'activation': 'relu', 'hidden_layer_size': 32, 'dropout': 0.333, 'reg_penalty': 0.03}\n",
      "0.505338 (0.004347) with: {'activation': 'relu', 'hidden_layer_size': 32, 'dropout': 0.333, 'reg_penalty': 0.1}\n",
      "0.491103 (0.023243) with: {'activation': 'sigmoid', 'hidden_layer_size': 4, 'dropout': 0.333, 'reg_penalty': 0}\n",
      "0.494662 (0.019837) with: {'activation': 'sigmoid', 'hidden_layer_size': 4, 'dropout': 0.333, 'reg_penalty': 0.001}\n",
      "0.505338 (0.004347) with: {'activation': 'sigmoid', 'hidden_layer_size': 4, 'dropout': 0.333, 'reg_penalty': 0.003}\n",
      "0.505338 (0.004347) with: {'activation': 'sigmoid', 'hidden_layer_size': 4, 'dropout': 0.333, 'reg_penalty': 0.01}\n",
      "0.505338 (0.004347) with: {'activation': 'sigmoid', 'hidden_layer_size': 4, 'dropout': 0.333, 'reg_penalty': 0.03}\n",
      "0.505338 (0.004347) with: {'activation': 'sigmoid', 'hidden_layer_size': 4, 'dropout': 0.333, 'reg_penalty': 0.1}\n",
      "0.498221 (0.031478) with: {'activation': 'sigmoid', 'hidden_layer_size': 8, 'dropout': 0.333, 'reg_penalty': 0}\n",
      "0.491103 (0.023049) with: {'activation': 'sigmoid', 'hidden_layer_size': 8, 'dropout': 0.333, 'reg_penalty': 0.001}\n",
      "0.505338 (0.004347) with: {'activation': 'sigmoid', 'hidden_layer_size': 8, 'dropout': 0.333, 'reg_penalty': 0.003}\n",
      "0.505338 (0.004347) with: {'activation': 'sigmoid', 'hidden_layer_size': 8, 'dropout': 0.333, 'reg_penalty': 0.01}\n",
      "0.505338 (0.004347) with: {'activation': 'sigmoid', 'hidden_layer_size': 8, 'dropout': 0.333, 'reg_penalty': 0.03}\n",
      "0.505338 (0.004347) with: {'activation': 'sigmoid', 'hidden_layer_size': 8, 'dropout': 0.333, 'reg_penalty': 0.1}\n",
      "0.501779 (0.051480) with: {'activation': 'sigmoid', 'hidden_layer_size': 16, 'dropout': 0.333, 'reg_penalty': 0}\n",
      "0.501779 (0.006650) with: {'activation': 'sigmoid', 'hidden_layer_size': 16, 'dropout': 0.333, 'reg_penalty': 0.001}\n",
      "0.505338 (0.004347) with: {'activation': 'sigmoid', 'hidden_layer_size': 16, 'dropout': 0.333, 'reg_penalty': 0.003}\n",
      "0.501779 (0.006650) with: {'activation': 'sigmoid', 'hidden_layer_size': 16, 'dropout': 0.333, 'reg_penalty': 0.01}\n",
      "0.505338 (0.004347) with: {'activation': 'sigmoid', 'hidden_layer_size': 16, 'dropout': 0.333, 'reg_penalty': 0.03}\n",
      "0.505338 (0.004347) with: {'activation': 'sigmoid', 'hidden_layer_size': 16, 'dropout': 0.333, 'reg_penalty': 0.1}\n",
      "0.501779 (0.057580) with: {'activation': 'sigmoid', 'hidden_layer_size': 32, 'dropout': 0.333, 'reg_penalty': 0}\n",
      "0.498221 (0.015253) with: {'activation': 'sigmoid', 'hidden_layer_size': 32, 'dropout': 0.333, 'reg_penalty': 0.001}\n",
      "0.501779 (0.006650) with: {'activation': 'sigmoid', 'hidden_layer_size': 32, 'dropout': 0.333, 'reg_penalty': 0.003}\n",
      "0.505338 (0.004347) with: {'activation': 'sigmoid', 'hidden_layer_size': 32, 'dropout': 0.333, 'reg_penalty': 0.01}\n",
      "0.505338 (0.004347) with: {'activation': 'sigmoid', 'hidden_layer_size': 32, 'dropout': 0.333, 'reg_penalty': 0.03}\n",
      "0.505338 (0.004347) with: {'activation': 'sigmoid', 'hidden_layer_size': 32, 'dropout': 0.333, 'reg_penalty': 0.1}\n",
      "Evaluate performance in test set\n",
      "[[16  8]\n",
      " [21 26]]\n",
      "Test Accuracy 0.592\n",
      "Test F1 0.642\n"
     ]
    }
   ],
   "source": [
    "# 2nd, use Keras native k-fold cross-validation and grid search\n",
    "# this will take ~ 15 minutes on AWS p2.xlarge\n",
    "\n",
    "print('%s Starting' % time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "estimator = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=False)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "# hyperparameter options to try\n",
    "hidden_layer_hp = [4, 8, 16, 32]\n",
    "dropout_hp = [0.333]\n",
    "reg_penalty_hp = [0, 0.001, 0.003, 0.01, 0.03, 0.1]\n",
    "activation_hp = ['relu', 'sigmoid']\n",
    "\n",
    "param_grid = dict(hidden_layer_size=hidden_layer_hp, \n",
    "                  dropout=dropout_hp, \n",
    "                  reg_penalty=reg_penalty_hp,\n",
    "                  activation=activation_hp,\n",
    "                  )\n",
    "\n",
    "grid = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=kfold, n_jobs=-1)\n",
    "classifier = grid.fit(X_bigtrain, y_bigtrain)\n",
    "\n",
    "print('%s Finishing' % time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# summarize xval results\n",
    "print(\"Best Xval: %f using %s\" % (classifier.best_score_, classifier.best_params_))\n",
    "means = classifier.cv_results_['mean_test_score']\n",
    "stds = classifier.cv_results_['std_test_score']\n",
    "params = classifier.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "        \n",
    "# evaluate with test set\n",
    "print(\"Evaluate performance in test set\")\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_test_pred, y_test)\n",
    "print(confusion_matrix)\n",
    "\n",
    "print(\"Test Accuracy %.3f\" % sklearn.metrics.accuracy_score(y_test_pred, y_test))\n",
    "print(\"Test F1 %.3f\" % sklearn.metrics.f1_score(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally, roll our own grid search\n",
    "# more fine-grained control such as custom metric and threshold\n",
    "\n",
    "# define some custom metrics\n",
    "import keras.backend as K\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # return keras tensor for recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # return keras tensor for precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "def f_score(y_true, y_pred):\n",
    "    beta = 1 #  can adjust to penalize false positives/negatives\n",
    "    return fbeta_score(y_true, y_pred, beta=beta)\n",
    "\n",
    "def selectThresholdF1 (logits, labels, beta=1):\n",
    "    # return threshold, f-score that yields best F-score\n",
    "    # predict using true if >= threshold\n",
    "\n",
    "    precision, recall, thresholds = sklearn.metrics.precision_recall_curve(labels, logits)\n",
    "    bb = beta**2\n",
    "    f1_scores = (1 + bb) * precision * recall / (bb * precision + recall)\n",
    "    f1_scores = np.nan_to_num(f1_scores)\n",
    "    \n",
    "    best_index = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_index]\n",
    "    best_score = f1_scores[best_index]\n",
    "    return (best_threshold, best_score)\n",
    "\n",
    "def selectThresholdAcc (logits, labels, beta=1):\n",
    "    # return threshold that yields best accuracy\n",
    "    # predict using true if >= threshold\n",
    "\n",
    "    precision, recall, thresholds = sklearn.metrics.precision_recall_curve(labels, logits)\n",
    "    accuracies = [sklearn.metrics.accuracy_score(logits >= thresh, labels) for thresh in thresholds]\n",
    "    \n",
    "    best_index = np.argmax(accuracies)\n",
    "    best_threshold = thresholds[best_index]\n",
    "    best_score = accuracies[best_index]\n",
    "    return (best_threshold, best_score)\n",
    "\n",
    "def selectThresholdTest (logits, labels, beta=1):\n",
    "    # show all thresholds, resulting F1 and accuracy\n",
    "\n",
    "    precision, recall, thresholds = sklearn.metrics.precision_recall_curve(labels, logits)\n",
    "    bb = beta**2\n",
    "    f1_scores = (1 + bb) * precision * recall / (bb * precision + recall)\n",
    "    f1_scores = np.nan_to_num(f1_scores)\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        labels_pred = logits >= thresh\n",
    "        f_test = sklearn.metrics.f1_score(labels_pred, labels)\n",
    "        acc_test = sklearn.metrics.accuracy_score(labels_pred, labels)\n",
    "        \n",
    "        print (\"Threshold %f, f1 %f, accuracy %f\") % (thresh, f_test, acc_test)\n",
    "        print(sklearn.metrics.confusion_matrix(labels_pred, labels))\n",
    "    \n",
    "    best_index = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_index]\n",
    "    best_score = f1_scores[best_index]\n",
    "    return (best_threshold, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same as above, compile with custom metric\n",
    "def create_model(num_components=num_features, \n",
    "                 hidden_layer_size=30, \n",
    "                 dropout=(1.0/3.0), \n",
    "                 reg_penalty=0.0001, \n",
    "                 activation='relu'):\n",
    "    \n",
    "    model = declare_model(num_components=num_components, \n",
    "                 hidden_layer_size=hidden_layer_size, \n",
    "                 dropout=dropout, \n",
    "                 reg_penalty=reg_penalty, \n",
    "                 activation=activation)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f_score])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'model*.json': No such file or directory\n",
      "rm: cannot remove 'model*.h5': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# delete old saved model files\n",
    "!rm model*.json\n",
    "!rm model*.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:35:20 Starting\n",
      "00:35:21 epoch 0 of 100 Train loss: 0.6932 Train f_score 0.2256 Xval loss: 0.6928 Xval f_score 0.5075\n",
      "00:35:21 epoch 10 of 100 Train loss: 0.6924 Train f_score 0.6642 Xval loss: 0.6929 Xval f_score 0.6374\n",
      "00:35:21 epoch 20 of 100 Train loss: 0.6915 Train f_score 0.6540 Xval loss: 0.6930 Xval f_score 0.5843\n",
      "00:35:21 epoch 30 of 100 Train loss: 0.6903 Train f_score 0.6486 Xval loss: 0.6931 Xval f_score 0.5843\n",
      "00:35:21 epoch 40 of 100 Train loss: 0.6884 Train f_score 0.6423 Xval loss: 0.6931 Xval f_score 0.5977\n",
      "00:35:21 epoch 50 of 100 Train loss: 0.6860 Train f_score 0.6307 Xval loss: 0.6930 Xval f_score 0.6118\n",
      "00:35:21 epoch 60 of 100 Train loss: 0.6829 Train f_score 0.6160 Xval loss: 0.6928 Xval f_score 0.5854\n",
      "00:35:21 epoch 70 of 100 Train loss: 0.6794 Train f_score 0.6298 Xval loss: 0.6929 Xval f_score 0.5926\n",
      "00:35:21 epoch 80 of 100 Train loss: 0.6754 Train f_score 0.6234 Xval loss: 0.6934 Xval f_score 0.6000\n",
      "00:35:21 epoch 90 of 100 Train loss: 0.6713 Train f_score 0.6320 Xval loss: 0.6943 Xval f_score 0.6000\n",
      "Best Xval loss epoch 63, value 0.692775\n",
      "NN units 4\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.633, Train F1 0.642\n",
      "[[64 38]\n",
      " [39 69]]\n",
      "Final Xval Accuracy 0.592, Xval F1 0.580\n",
      "[[22 15]\n",
      " [14 20]]\n",
      "00:35:21 Starting\n",
      "00:35:22 epoch 0 of 100 Train loss: 0.6932 Train f_score 0.0000 Xval loss: 0.6935 Xval f_score 0.1026\n",
      "00:35:22 epoch 10 of 100 Train loss: 0.6924 Train f_score 0.6025 Xval loss: 0.6938 Xval f_score 0.5333\n",
      "00:35:22 epoch 20 of 100 Train loss: 0.6915 Train f_score 0.6270 Xval loss: 0.6939 Xval f_score 0.5926\n",
      "00:35:22 epoch 30 of 100 Train loss: 0.6901 Train f_score 0.6561 Xval loss: 0.6940 Xval f_score 0.5570\n",
      "00:35:22 epoch 40 of 100 Train loss: 0.6883 Train f_score 0.6502 Xval loss: 0.6941 Xval f_score 0.5385\n",
      "00:35:22 epoch 50 of 100 Train loss: 0.6859 Train f_score 0.6407 Xval loss: 0.6941 Xval f_score 0.5333\n",
      "00:35:22 epoch 60 of 100 Train loss: 0.6831 Train f_score 0.6457 Xval loss: 0.6948 Xval f_score 0.5070\n",
      "00:35:22 epoch 70 of 100 Train loss: 0.6801 Train f_score 0.6038 Xval loss: 0.6955 Xval f_score 0.4848\n",
      "00:35:22 epoch 80 of 100 Train loss: 0.6769 Train f_score 0.6019 Xval loss: 0.6967 Xval f_score 0.4615\n",
      "00:35:22 epoch 90 of 100 Train loss: 0.6737 Train f_score 0.6049 Xval loss: 0.6989 Xval f_score 0.4375\n",
      "Best Xval loss epoch 0, value 0.693508\n",
      "NN units 4\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.638, Train F1 0.712\n",
      "[[40 13]\n",
      " [63 94]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.467\n",
      "[[25 21]\n",
      " [11 14]]\n",
      "00:35:22 Starting\n",
      "00:35:23 epoch 0 of 100 Train loss: 0.6939 Train f_score 0.5286 Xval loss: 0.6947 Xval f_score 0.6279\n",
      "00:35:23 epoch 10 of 100 Train loss: 0.6930 Train f_score 0.6865 Xval loss: 0.6948 Xval f_score 0.6392\n",
      "00:35:23 epoch 20 of 100 Train loss: 0.6920 Train f_score 0.6796 Xval loss: 0.6946 Xval f_score 0.6535\n",
      "00:35:23 epoch 30 of 100 Train loss: 0.6907 Train f_score 0.6928 Xval loss: 0.6946 Xval f_score 0.6400\n",
      "00:35:23 epoch 40 of 100 Train loss: 0.6893 Train f_score 0.6915 Xval loss: 0.6947 Xval f_score 0.6458\n",
      "00:35:23 epoch 50 of 100 Train loss: 0.6875 Train f_score 0.7018 Xval loss: 0.6950 Xval f_score 0.6237\n",
      "00:35:23 epoch 60 of 100 Train loss: 0.6853 Train f_score 0.7007 Xval loss: 0.6953 Xval f_score 0.6000\n",
      "00:35:23 epoch 70 of 100 Train loss: 0.6827 Train f_score 0.6798 Xval loss: 0.6957 Xval f_score 0.5977\n",
      "00:35:23 epoch 80 of 100 Train loss: 0.6796 Train f_score 0.6694 Xval loss: 0.6963 Xval f_score 0.5854\n",
      "00:35:23 epoch 90 of 100 Train loss: 0.6761 Train f_score 0.6667 Xval loss: 0.6972 Xval f_score 0.5750\n",
      "Best Xval loss epoch 27, value 0.694560\n",
      "NN units 4\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.652, Train F1 0.697\n",
      "[[53 23]\n",
      " [50 84]]\n",
      "Final Xval Accuracy 0.606, Xval F1 0.576\n",
      "[[24 16]\n",
      " [12 19]]\n",
      "00:35:24 Starting\n",
      "00:35:24 epoch 0 of 100 Train loss: 0.6986 Train f_score 0.4909 Xval loss: 0.6969 Xval f_score 0.6190\n",
      "00:35:24 epoch 10 of 100 Train loss: 0.6971 Train f_score 0.6578 Xval loss: 0.6962 Xval f_score 0.6600\n",
      "00:35:24 epoch 20 of 100 Train loss: 0.6958 Train f_score 0.6796 Xval loss: 0.6957 Xval f_score 0.6538\n",
      "00:35:24 epoch 30 of 100 Train loss: 0.6946 Train f_score 0.6795 Xval loss: 0.6952 Xval f_score 0.6408\n",
      "00:35:24 epoch 40 of 100 Train loss: 0.6935 Train f_score 0.6815 Xval loss: 0.6950 Xval f_score 0.6346\n",
      "00:35:24 epoch 50 of 100 Train loss: 0.6925 Train f_score 0.6837 Xval loss: 0.6948 Xval f_score 0.6476\n",
      "00:35:25 epoch 60 of 100 Train loss: 0.6913 Train f_score 0.6818 Xval loss: 0.6949 Xval f_score 0.6408\n",
      "00:35:25 epoch 70 of 100 Train loss: 0.6899 Train f_score 0.6865 Xval loss: 0.6953 Xval f_score 0.6275\n",
      "00:35:25 epoch 80 of 100 Train loss: 0.6885 Train f_score 0.6849 Xval loss: 0.6958 Xval f_score 0.6465\n",
      "00:35:25 epoch 90 of 100 Train loss: 0.6869 Train f_score 0.6784 Xval loss: 0.6963 Xval f_score 0.6327\n",
      "Best Xval loss epoch 50, value 0.694799\n",
      "NN units 4\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.595, Train F1 0.693\n",
      "[[29 11]\n",
      " [74 96]]\n",
      "Final Xval Accuracy 0.634, Xval F1 0.618\n",
      "[[24 14]\n",
      " [12 21]]\n",
      "00:35:25 Starting\n",
      "00:35:26 epoch 0 of 100 Train loss: 0.7063 Train f_score 0.3086 Xval loss: 0.7061 Xval f_score 0.2807\n",
      "00:35:26 epoch 10 of 100 Train loss: 0.7030 Train f_score 0.6067 Xval loss: 0.7031 Xval f_score 0.5934\n",
      "00:35:26 epoch 20 of 100 Train loss: 0.7003 Train f_score 0.6861 Xval loss: 0.7007 Xval f_score 0.6337\n",
      "00:35:26 epoch 30 of 100 Train loss: 0.6981 Train f_score 0.6773 Xval loss: 0.6988 Xval f_score 0.6476\n",
      "00:35:26 epoch 40 of 100 Train loss: 0.6965 Train f_score 0.6752 Xval loss: 0.6974 Xval f_score 0.6604\n",
      "00:35:26 epoch 50 of 100 Train loss: 0.6953 Train f_score 0.6751 Xval loss: 0.6963 Xval f_score 0.6604\n",
      "00:35:26 epoch 60 of 100 Train loss: 0.6945 Train f_score 0.6751 Xval loss: 0.6956 Xval f_score 0.6604\n",
      "00:35:26 epoch 70 of 100 Train loss: 0.6939 Train f_score 0.6751 Xval loss: 0.6950 Xval f_score 0.6604\n",
      "00:35:26 epoch 80 of 100 Train loss: 0.6936 Train f_score 0.6751 Xval loss: 0.6946 Xval f_score 0.6604\n",
      "00:35:26 epoch 90 of 100 Train loss: 0.6935 Train f_score 0.6751 Xval loss: 0.6945 Xval f_score 0.6604\n",
      "Best Xval loss epoch 95, value 0.694487\n",
      "NN units 4\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.562, Train F1 0.699\n",
      "[[ 11   0]\n",
      " [ 92 107]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.464\n",
      "[[28 22]\n",
      " [ 8 13]]\n",
      "00:35:26 Starting\n",
      "00:35:27 epoch 0 of 100 Train loss: 0.6931 Train f_score 0.5992 Xval loss: 0.6939 Xval f_score 0.5934\n",
      "00:35:27 epoch 10 of 100 Train loss: 0.6917 Train f_score 0.6839 Xval loss: 0.6936 Xval f_score 0.6408\n",
      "00:35:27 epoch 20 of 100 Train loss: 0.6898 Train f_score 0.6931 Xval loss: 0.6940 Xval f_score 0.6535\n",
      "00:35:27 epoch 30 of 100 Train loss: 0.6870 Train f_score 0.6966 Xval loss: 0.6945 Xval f_score 0.6526\n",
      "00:35:27 epoch 40 of 100 Train loss: 0.6833 Train f_score 0.6818 Xval loss: 0.6951 Xval f_score 0.6374\n",
      "00:35:27 epoch 50 of 100 Train loss: 0.6784 Train f_score 0.6667 Xval loss: 0.6958 Xval f_score 0.6353\n",
      "00:35:27 epoch 60 of 100 Train loss: 0.6727 Train f_score 0.6639 Xval loss: 0.6972 Xval f_score 0.6173\n",
      "00:35:27 epoch 70 of 100 Train loss: 0.6663 Train f_score 0.6471 Xval loss: 0.6992 Xval f_score 0.5455\n",
      "00:35:27 epoch 80 of 100 Train loss: 0.6596 Train f_score 0.6581 Xval loss: 0.7015 Xval f_score 0.5455\n",
      "00:35:27 epoch 90 of 100 Train loss: 0.6526 Train f_score 0.6466 Xval loss: 0.7048 Xval f_score 0.5385\n",
      "Best Xval loss epoch 8, value 0.693537\n",
      "NN units 8\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.638, Train F1 0.600\n",
      "[[77 50]\n",
      " [26 57]]\n",
      "Final Xval Accuracy 0.592, Xval F1 0.681\n",
      "[[11  4]\n",
      " [25 31]]\n",
      "00:35:28 Starting\n",
      "00:35:28 epoch 0 of 100 Train loss: 0.6947 Train f_score 0.2667 Xval loss: 0.6936 Xval f_score 0.4074\n",
      "00:35:28 epoch 10 of 100 Train loss: 0.6934 Train f_score 0.6340 Xval loss: 0.6935 Xval f_score 0.6742\n",
      "00:35:28 epoch 20 of 100 Train loss: 0.6924 Train f_score 0.6713 Xval loss: 0.6936 Xval f_score 0.6200\n",
      "00:35:29 epoch 30 of 100 Train loss: 0.6908 Train f_score 0.6713 Xval loss: 0.6937 Xval f_score 0.6598\n",
      "00:35:29 epoch 40 of 100 Train loss: 0.6883 Train f_score 0.6884 Xval loss: 0.6940 Xval f_score 0.6452\n",
      "00:35:29 epoch 50 of 100 Train loss: 0.6846 Train f_score 0.6920 Xval loss: 0.6946 Xval f_score 0.5843\n",
      "00:35:29 epoch 60 of 100 Train loss: 0.6795 Train f_score 0.6721 Xval loss: 0.6955 Xval f_score 0.5977\n",
      "00:35:29 epoch 70 of 100 Train loss: 0.6734 Train f_score 0.6610 Xval loss: 0.6964 Xval f_score 0.5714\n",
      "00:35:29 epoch 80 of 100 Train loss: 0.6665 Train f_score 0.6580 Xval loss: 0.6977 Xval f_score 0.5679\n",
      "00:35:29 epoch 90 of 100 Train loss: 0.6590 Train f_score 0.6608 Xval loss: 0.6995 Xval f_score 0.5128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Xval loss epoch 12, value 0.693477\n",
      "NN units 8\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.667, Train F1 0.646\n",
      "[[76 43]\n",
      " [27 64]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.348\n",
      "[[33 27]\n",
      " [ 3  8]]\n",
      "00:35:29 Starting\n",
      "00:35:30 epoch 0 of 100 Train loss: 0.6965 Train f_score 0.3537 Xval loss: 0.6957 Xval f_score 0.4068\n",
      "00:35:30 epoch 10 of 100 Train loss: 0.6945 Train f_score 0.6142 Xval loss: 0.6953 Xval f_score 0.5647\n",
      "00:35:30 epoch 20 of 100 Train loss: 0.6929 Train f_score 0.6595 Xval loss: 0.6952 Xval f_score 0.6105\n",
      "00:35:30 epoch 30 of 100 Train loss: 0.6907 Train f_score 0.6831 Xval loss: 0.6951 Xval f_score 0.6250\n",
      "00:35:30 epoch 40 of 100 Train loss: 0.6878 Train f_score 0.6739 Xval loss: 0.6948 Xval f_score 0.6237\n",
      "00:35:30 epoch 50 of 100 Train loss: 0.6839 Train f_score 0.6794 Xval loss: 0.6943 Xval f_score 0.6374\n",
      "00:35:30 epoch 60 of 100 Train loss: 0.6793 Train f_score 0.6613 Xval loss: 0.6939 Xval f_score 0.6279\n",
      "00:35:30 epoch 70 of 100 Train loss: 0.6741 Train f_score 0.6500 Xval loss: 0.6936 Xval f_score 0.6341\n",
      "00:35:30 epoch 80 of 100 Train loss: 0.6683 Train f_score 0.6471 Xval loss: 0.6938 Xval f_score 0.6154\n",
      "00:35:30 epoch 90 of 100 Train loss: 0.6625 Train f_score 0.6667 Xval loss: 0.6949 Xval f_score 0.5974\n",
      "Best Xval loss epoch 73, value 0.693592\n",
      "NN units 8\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.648, Train F1 0.675\n",
      "[[59 30]\n",
      " [44 77]]\n",
      "Final Xval Accuracy 0.592, Xval F1 0.674\n",
      "[[12  5]\n",
      " [24 30]]\n",
      "00:35:31 Starting\n",
      "00:35:31 epoch 0 of 100 Train loss: 0.7019 Train f_score 0.6308 Xval loss: 0.7037 Xval f_score 0.5610\n",
      "00:35:31 epoch 10 of 100 Train loss: 0.6986 Train f_score 0.6324 Xval loss: 0.7018 Xval f_score 0.5176\n",
      "00:35:32 epoch 20 of 100 Train loss: 0.6955 Train f_score 0.6594 Xval loss: 0.7000 Xval f_score 0.5393\n",
      "00:35:32 epoch 30 of 100 Train loss: 0.6923 Train f_score 0.6692 Xval loss: 0.6979 Xval f_score 0.5977\n",
      "00:35:32 epoch 40 of 100 Train loss: 0.6893 Train f_score 0.6641 Xval loss: 0.6970 Xval f_score 0.5977\n",
      "00:35:32 epoch 50 of 100 Train loss: 0.6860 Train f_score 0.6585 Xval loss: 0.6966 Xval f_score 0.6000\n",
      "00:35:32 epoch 60 of 100 Train loss: 0.6827 Train f_score 0.6611 Xval loss: 0.6962 Xval f_score 0.6053\n",
      "00:35:32 epoch 70 of 100 Train loss: 0.6792 Train f_score 0.6638 Xval loss: 0.6964 Xval f_score 0.5676\n",
      "00:35:32 epoch 80 of 100 Train loss: 0.6754 Train f_score 0.6781 Xval loss: 0.6972 Xval f_score 0.5479\n",
      "00:35:32 epoch 90 of 100 Train loss: 0.6714 Train f_score 0.6638 Xval loss: 0.6986 Xval f_score 0.5143\n",
      "Best Xval loss epoch 59, value 0.696171\n",
      "NN units 8\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.643, Train F1 0.717\n",
      "[[40 12]\n",
      " [63 95]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.375\n",
      "[[32 26]\n",
      " [ 4  9]]\n",
      "00:35:32 Starting\n",
      "00:35:33 epoch 0 of 100 Train loss: 0.7217 Train f_score 0.2029 Xval loss: 0.7227 Xval f_score 0.2128\n",
      "00:35:33 epoch 10 of 100 Train loss: 0.7146 Train f_score 0.5767 Xval loss: 0.7160 Xval f_score 0.5316\n",
      "00:35:33 epoch 20 of 100 Train loss: 0.7088 Train f_score 0.6643 Xval loss: 0.7107 Xval f_score 0.6105\n",
      "00:35:33 epoch 30 of 100 Train loss: 0.7043 Train f_score 0.6865 Xval loss: 0.7063 Xval f_score 0.6600\n",
      "00:35:33 epoch 40 of 100 Train loss: 0.7006 Train f_score 0.6817 Xval loss: 0.7026 Xval f_score 0.6602\n",
      "00:35:33 epoch 50 of 100 Train loss: 0.6979 Train f_score 0.6837 Xval loss: 0.6998 Xval f_score 0.6602\n",
      "00:35:33 epoch 60 of 100 Train loss: 0.6959 Train f_score 0.6753 Xval loss: 0.6979 Xval f_score 0.6600\n",
      "00:35:33 epoch 70 of 100 Train loss: 0.6945 Train f_score 0.6776 Xval loss: 0.6968 Xval f_score 0.6600\n",
      "00:35:33 epoch 80 of 100 Train loss: 0.6934 Train f_score 0.6847 Xval loss: 0.6960 Xval f_score 0.6327\n",
      "00:35:33 epoch 90 of 100 Train loss: 0.6926 Train f_score 0.6829 Xval loss: 0.6958 Xval f_score 0.6316\n",
      "Best Xval loss epoch 99, value 0.695663\n",
      "NN units 8\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.595, Train F1 0.682\n",
      "[[34 16]\n",
      " [69 91]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.652\n",
      "[[ 9  5]\n",
      " [27 30]]\n",
      "00:35:34 Starting\n",
      "00:35:35 epoch 0 of 100 Train loss: 0.6935 Train f_score 0.6434 Xval loss: 0.6915 Xval f_score 0.6400\n",
      "00:35:35 epoch 10 of 100 Train loss: 0.6893 Train f_score 0.6667 Xval loss: 0.6916 Xval f_score 0.6122\n",
      "00:35:35 epoch 20 of 100 Train loss: 0.6848 Train f_score 0.6767 Xval loss: 0.6911 Xval f_score 0.6067\n",
      "00:35:35 epoch 30 of 100 Train loss: 0.6792 Train f_score 0.6639 Xval loss: 0.6904 Xval f_score 0.6024\n",
      "00:35:35 epoch 40 of 100 Train loss: 0.6725 Train f_score 0.6757 Xval loss: 0.6904 Xval f_score 0.5526\n",
      "00:35:35 epoch 50 of 100 Train loss: 0.6648 Train f_score 0.6509 Xval loss: 0.6910 Xval f_score 0.5000\n",
      "00:35:35 epoch 60 of 100 Train loss: 0.6564 Train f_score 0.6377 Xval loss: 0.6924 Xval f_score 0.5000\n",
      "00:35:35 epoch 70 of 100 Train loss: 0.6472 Train f_score 0.6505 Xval loss: 0.6953 Xval f_score 0.5000\n",
      "00:35:35 epoch 80 of 100 Train loss: 0.6377 Train f_score 0.6731 Xval loss: 0.6985 Xval f_score 0.4932\n",
      "00:35:35 epoch 90 of 100 Train loss: 0.6279 Train f_score 0.6857 Xval loss: 0.7020 Xval f_score 0.5000\n",
      "Best Xval loss epoch 35, value 0.690252\n",
      "NN units 16\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.700, Train F1 0.722\n",
      "[[65 25]\n",
      " [38 82]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.673\n",
      "[[ 6  2]\n",
      " [30 33]]\n",
      "00:35:35 Starting\n",
      "00:35:36 epoch 0 of 100 Train loss: 0.6969 Train f_score 0.5530 Xval loss: 0.6957 Xval f_score 0.5500\n",
      "00:35:36 epoch 10 of 100 Train loss: 0.6935 Train f_score 0.6773 Xval loss: 0.6950 Xval f_score 0.6408\n",
      "00:35:36 epoch 20 of 100 Train loss: 0.6906 Train f_score 0.6751 Xval loss: 0.6941 Xval f_score 0.6604\n",
      "00:35:36 epoch 30 of 100 Train loss: 0.6867 Train f_score 0.6772 Xval loss: 0.6927 Xval f_score 0.6408\n",
      "00:35:36 epoch 40 of 100 Train loss: 0.6815 Train f_score 0.6780 Xval loss: 0.6914 Xval f_score 0.6250\n",
      "00:35:36 epoch 50 of 100 Train loss: 0.6756 Train f_score 0.6963 Xval loss: 0.6911 Xval f_score 0.6292\n",
      "00:35:36 epoch 60 of 100 Train loss: 0.6691 Train f_score 0.6875 Xval loss: 0.6916 Xval f_score 0.6279\n",
      "00:35:36 epoch 70 of 100 Train loss: 0.6621 Train f_score 0.6857 Xval loss: 0.6928 Xval f_score 0.6024\n",
      "00:35:36 epoch 80 of 100 Train loss: 0.6545 Train f_score 0.6723 Xval loss: 0.6954 Xval f_score 0.6024\n",
      "00:35:36 epoch 90 of 100 Train loss: 0.6464 Train f_score 0.6953 Xval loss: 0.6984 Xval f_score 0.6024\n",
      "Best Xval loss epoch 50, value 0.691130\n",
      "NN units 16\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.681, Train F1 0.720\n",
      "[[57 21]\n",
      " [46 86]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.652\n",
      "[[11  6]\n",
      " [25 29]]\n",
      "00:35:37 Starting\n",
      "00:35:38 epoch 0 of 100 Train loss: 0.6982 Train f_score 0.3171 Xval loss: 0.6996 Xval f_score 0.3390\n",
      "00:35:38 epoch 10 of 100 Train loss: 0.6947 Train f_score 0.6000 Xval loss: 0.6982 Xval f_score 0.5526\n",
      "00:35:38 epoch 20 of 100 Train loss: 0.6911 Train f_score 0.6293 Xval loss: 0.6976 Xval f_score 0.5385\n",
      "00:35:38 epoch 30 of 100 Train loss: 0.6869 Train f_score 0.6234 Xval loss: 0.6973 Xval f_score 0.5526\n",
      "00:35:38 epoch 40 of 100 Train loss: 0.6816 Train f_score 0.6188 Xval loss: 0.6971 Xval f_score 0.5479\n",
      "00:35:38 epoch 50 of 100 Train loss: 0.6752 Train f_score 0.6306 Xval loss: 0.6974 Xval f_score 0.5352\n",
      "00:35:38 epoch 60 of 100 Train loss: 0.6677 Train f_score 0.6389 Xval loss: 0.6987 Xval f_score 0.5352\n",
      "00:35:38 epoch 70 of 100 Train loss: 0.6595 Train f_score 0.6326 Xval loss: 0.7019 Xval f_score 0.5352\n",
      "00:35:38 epoch 80 of 100 Train loss: 0.6508 Train f_score 0.6449 Xval loss: 0.7062 Xval f_score 0.4638\n",
      "00:35:38 epoch 90 of 100 Train loss: 0.6420 Train f_score 0.6574 Xval loss: 0.7119 Xval f_score 0.4857\n",
      "Best Xval loss epoch 41, value 0.697129\n",
      "NN units 16\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.676, Train F1 0.696\n",
      "[[64 29]\n",
      " [39 78]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.200\n",
      "[[35 31]\n",
      " [ 1  4]]\n",
      "00:35:39 Starting\n",
      "00:35:40 epoch 0 of 100 Train loss: 0.7123 Train f_score 0.5620 Xval loss: 0.7099 Xval f_score 0.6098\n",
      "00:35:40 epoch 10 of 100 Train loss: 0.7062 Train f_score 0.6567 Xval loss: 0.7066 Xval f_score 0.6593\n",
      "00:35:40 epoch 20 of 100 Train loss: 0.7007 Train f_score 0.6667 Xval loss: 0.7038 Xval f_score 0.6813\n",
      "00:35:40 epoch 30 of 100 Train loss: 0.6958 Train f_score 0.6569 Xval loss: 0.7017 Xval f_score 0.6889\n",
      "00:35:40 epoch 40 of 100 Train loss: 0.6914 Train f_score 0.6540 Xval loss: 0.7008 Xval f_score 0.6517\n",
      "00:35:40 epoch 50 of 100 Train loss: 0.6868 Train f_score 0.6457 Xval loss: 0.7007 Xval f_score 0.5854\n",
      "00:35:40 epoch 60 of 100 Train loss: 0.6819 Train f_score 0.6496 Xval loss: 0.7018 Xval f_score 0.5823\n",
      "00:35:40 epoch 70 of 100 Train loss: 0.6770 Train f_score 0.6486 Xval loss: 0.7031 Xval f_score 0.5263\n",
      "00:35:40 epoch 80 of 100 Train loss: 0.6718 Train f_score 0.6359 Xval loss: 0.7051 Xval f_score 0.5455\n",
      "00:35:40 epoch 90 of 100 Train loss: 0.6662 Train f_score 0.6291 Xval loss: 0.7080 Xval f_score 0.5455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Xval loss epoch 45, value 0.700626\n",
      "NN units 16\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.662, Train F1 0.687\n",
      "[[61 29]\n",
      " [42 78]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.610\n",
      "[[14 10]\n",
      " [22 25]]\n",
      "00:35:40 Starting\n",
      "00:35:41 epoch 0 of 100 Train loss: 0.7474 Train f_score 0.6485 Xval loss: 0.7459 Xval f_score 0.6304\n",
      "00:35:41 epoch 10 of 100 Train loss: 0.7338 Train f_score 0.6624 Xval loss: 0.7333 Xval f_score 0.6337\n",
      "00:35:41 epoch 20 of 100 Train loss: 0.7223 Train f_score 0.6751 Xval loss: 0.7228 Xval f_score 0.6604\n",
      "00:35:41 epoch 30 of 100 Train loss: 0.7133 Train f_score 0.6751 Xval loss: 0.7143 Xval f_score 0.6604\n",
      "00:35:41 epoch 40 of 100 Train loss: 0.7066 Train f_score 0.6751 Xval loss: 0.7079 Xval f_score 0.6604\n",
      "00:35:41 epoch 50 of 100 Train loss: 0.7015 Train f_score 0.6751 Xval loss: 0.7029 Xval f_score 0.6604\n",
      "00:35:41 epoch 60 of 100 Train loss: 0.6978 Train f_score 0.6751 Xval loss: 0.6994 Xval f_score 0.6604\n",
      "00:35:41 epoch 70 of 100 Train loss: 0.6949 Train f_score 0.6751 Xval loss: 0.6971 Xval f_score 0.6604\n",
      "00:35:42 epoch 80 of 100 Train loss: 0.6928 Train f_score 0.6751 Xval loss: 0.6954 Xval f_score 0.6604\n",
      "00:35:42 epoch 90 of 100 Train loss: 0.6912 Train f_score 0.6751 Xval loss: 0.6945 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.693945\n",
      "NN units 16\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.586, Train F1 0.620\n",
      "[[52 36]\n",
      " [51 71]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.250\n",
      "[[36 30]\n",
      " [ 0  5]]\n",
      "00:35:42 Starting\n",
      "00:35:43 epoch 0 of 100 Train loss: 0.6931 Train f_score 0.6222 Xval loss: 0.6931 Xval f_score 0.6105\n",
      "00:35:43 epoch 10 of 100 Train loss: 0.6851 Train f_score 0.6787 Xval loss: 0.6920 Xval f_score 0.6522\n",
      "00:35:43 epoch 20 of 100 Train loss: 0.6763 Train f_score 0.6667 Xval loss: 0.6910 Xval f_score 0.6591\n",
      "00:35:43 epoch 30 of 100 Train loss: 0.6661 Train f_score 0.6807 Xval loss: 0.6909 Xval f_score 0.6353\n",
      "00:35:43 epoch 40 of 100 Train loss: 0.6545 Train f_score 0.6725 Xval loss: 0.6924 Xval f_score 0.6024\n",
      "00:35:43 epoch 50 of 100 Train loss: 0.6417 Train f_score 0.6606 Xval loss: 0.6950 Xval f_score 0.5952\n",
      "00:35:43 epoch 60 of 100 Train loss: 0.6279 Train f_score 0.6667 Xval loss: 0.6995 Xval f_score 0.6024\n",
      "00:35:43 epoch 70 of 100 Train loss: 0.6137 Train f_score 0.6759 Xval loss: 0.7059 Xval f_score 0.6024\n",
      "00:35:43 epoch 80 of 100 Train loss: 0.5997 Train f_score 0.6791 Xval loss: 0.7144 Xval f_score 0.6024\n",
      "00:35:43 epoch 90 of 100 Train loss: 0.5856 Train f_score 0.6825 Xval loss: 0.7261 Xval f_score 0.5854\n",
      "Best Xval loss epoch 26, value 0.690750\n",
      "NN units 32\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.719, Train F1 0.740\n",
      "[[67 23]\n",
      " [36 84]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.652\n",
      "[[11  6]\n",
      " [25 29]]\n",
      "00:35:44 Starting\n",
      "00:35:45 epoch 0 of 100 Train loss: 0.6990 Train f_score 0.4222 Xval loss: 0.6991 Xval f_score 0.4308\n",
      "00:35:45 epoch 10 of 100 Train loss: 0.6910 Train f_score 0.6891 Xval loss: 0.6990 Xval f_score 0.5432\n",
      "00:35:45 epoch 20 of 100 Train loss: 0.6835 Train f_score 0.6911 Xval loss: 0.6995 Xval f_score 0.5714\n",
      "00:35:45 epoch 30 of 100 Train loss: 0.6750 Train f_score 0.6778 Xval loss: 0.7005 Xval f_score 0.6000\n",
      "00:35:45 epoch 40 of 100 Train loss: 0.6653 Train f_score 0.6696 Xval loss: 0.7024 Xval f_score 0.5897\n",
      "00:35:45 epoch 50 of 100 Train loss: 0.6544 Train f_score 0.6784 Xval loss: 0.7053 Xval f_score 0.5714\n",
      "00:35:45 epoch 60 of 100 Train loss: 0.6422 Train f_score 0.6818 Xval loss: 0.7105 Xval f_score 0.5714\n",
      "00:35:45 epoch 70 of 100 Train loss: 0.6292 Train f_score 0.6912 Xval loss: 0.7180 Xval f_score 0.5455\n",
      "00:35:45 epoch 80 of 100 Train loss: 0.6157 Train f_score 0.7130 Xval loss: 0.7281 Xval f_score 0.5385\n",
      "00:35:45 epoch 90 of 100 Train loss: 0.6016 Train f_score 0.7222 Xval loss: 0.7401 Xval f_score 0.4872\n",
      "Best Xval loss epoch 7, value 0.698921\n",
      "NN units 32\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.729, Train F1 0.727\n",
      "[[77 31]\n",
      " [26 76]]\n",
      "Final Xval Accuracy 0.535, Xval F1 0.629\n",
      "[[10  7]\n",
      " [26 28]]\n",
      "00:35:46 Starting\n",
      "00:35:47 epoch 0 of 100 Train loss: 0.7055 Train f_score 0.5410 Xval loss: 0.7044 Xval f_score 0.5128\n",
      "00:35:47 epoch 10 of 100 Train loss: 0.6966 Train f_score 0.6926 Xval loss: 0.7023 Xval f_score 0.6593\n",
      "00:35:47 epoch 20 of 100 Train loss: 0.6887 Train f_score 0.6977 Xval loss: 0.7007 Xval f_score 0.6098\n",
      "00:35:47 epoch 30 of 100 Train loss: 0.6807 Train f_score 0.6778 Xval loss: 0.7002 Xval f_score 0.5714\n",
      "00:35:47 epoch 40 of 100 Train loss: 0.6718 Train f_score 0.6608 Xval loss: 0.7010 Xval f_score 0.5600\n",
      "00:35:47 epoch 50 of 100 Train loss: 0.6623 Train f_score 0.6486 Xval loss: 0.7027 Xval f_score 0.5352\n",
      "00:35:47 epoch 60 of 100 Train loss: 0.6518 Train f_score 0.6697 Xval loss: 0.7062 Xval f_score 0.5352\n",
      "00:35:47 epoch 70 of 100 Train loss: 0.6407 Train f_score 0.6791 Xval loss: 0.7130 Xval f_score 0.5714\n",
      "00:35:47 epoch 80 of 100 Train loss: 0.6294 Train f_score 0.6852 Xval loss: 0.7218 Xval f_score 0.5641\n",
      "00:35:47 epoch 90 of 100 Train loss: 0.6177 Train f_score 0.6884 Xval loss: 0.7321 Xval f_score 0.5570\n",
      "Best Xval loss epoch 29, value 0.700177\n",
      "NN units 32\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.719, Train F1 0.735\n",
      "[[69 25]\n",
      " [34 82]]\n",
      "Final Xval Accuracy 0.535, Xval F1 0.660\n",
      "[[ 6  3]\n",
      " [30 32]]\n",
      "00:35:48 Starting\n",
      "00:35:49 epoch 0 of 100 Train loss: 0.7302 Train f_score 0.5285 Xval loss: 0.7295 Xval f_score 0.6173\n",
      "00:35:49 epoch 10 of 100 Train loss: 0.7180 Train f_score 0.6641 Xval loss: 0.7237 Xval f_score 0.6353\n",
      "00:35:49 epoch 20 of 100 Train loss: 0.7082 Train f_score 0.6561 Xval loss: 0.7197 Xval f_score 0.6265\n",
      "00:35:49 epoch 30 of 100 Train loss: 0.6993 Train f_score 0.6749 Xval loss: 0.7165 Xval f_score 0.5926\n",
      "00:35:49 epoch 40 of 100 Train loss: 0.6914 Train f_score 0.6639 Xval loss: 0.7147 Xval f_score 0.5926\n",
      "00:35:49 epoch 50 of 100 Train loss: 0.6840 Train f_score 0.6376 Xval loss: 0.7145 Xval f_score 0.5750\n",
      "00:35:49 epoch 60 of 100 Train loss: 0.6766 Train f_score 0.6460 Xval loss: 0.7159 Xval f_score 0.5500\n",
      "00:35:49 epoch 70 of 100 Train loss: 0.6693 Train f_score 0.6607 Xval loss: 0.7193 Xval f_score 0.5128\n",
      "00:35:49 epoch 80 of 100 Train loss: 0.6622 Train f_score 0.6577 Xval loss: 0.7237 Xval f_score 0.5316\n",
      "00:35:49 epoch 90 of 100 Train loss: 0.6550 Train f_score 0.6637 Xval loss: 0.7297 Xval f_score 0.5316\n",
      "Best Xval loss epoch 44, value 0.714535\n",
      "NN units 32\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.671, Train F1 0.682\n",
      "[[67 33]\n",
      " [36 74]]\n",
      "Final Xval Accuracy 0.535, Xval F1 0.108\n",
      "[[36 33]\n",
      " [ 0  2]]\n",
      "00:35:49 Starting\n",
      "00:35:51 epoch 0 of 100 Train loss: 0.8015 Train f_score 0.5517 Xval loss: 0.8018 Xval f_score 0.4590\n",
      "00:35:51 epoch 10 of 100 Train loss: 0.7750 Train f_score 0.6114 Xval loss: 0.7778 Xval f_score 0.5946\n",
      "00:35:51 epoch 20 of 100 Train loss: 0.7531 Train f_score 0.6316 Xval loss: 0.7580 Xval f_score 0.5679\n",
      "00:35:51 epoch 30 of 100 Train loss: 0.7352 Train f_score 0.6743 Xval loss: 0.7420 Xval f_score 0.5610\n",
      "00:35:51 epoch 40 of 100 Train loss: 0.7213 Train f_score 0.6587 Xval loss: 0.7298 Xval f_score 0.5882\n",
      "00:35:51 epoch 50 of 100 Train loss: 0.7105 Train f_score 0.6420 Xval loss: 0.7202 Xval f_score 0.5952\n",
      "00:35:51 epoch 60 of 100 Train loss: 0.7022 Train f_score 0.6500 Xval loss: 0.7143 Xval f_score 0.6190\n",
      "00:35:51 epoch 70 of 100 Train loss: 0.6963 Train f_score 0.6525 Xval loss: 0.7114 Xval f_score 0.5263\n",
      "00:35:51 epoch 80 of 100 Train loss: 0.6919 Train f_score 0.6404 Xval loss: 0.7107 Xval f_score 0.5205\n",
      "00:35:51 epoch 90 of 100 Train loss: 0.6884 Train f_score 0.6278 Xval loss: 0.7108 Xval f_score 0.5070\n",
      "Best Xval loss epoch 82, value 0.710602\n",
      "NN units 32\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.0000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.629, Train F1 0.667\n",
      "[[54 29]\n",
      " [49 78]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.279\n",
      "[[34 29]\n",
      " [ 2  6]]\n",
      "00:35:52 Starting\n",
      "00:35:53 epoch 0 of 100 Train loss: 0.6932 Train f_score 0.0708 Xval loss: 0.6926 Xval f_score 0.1951\n",
      "00:35:53 epoch 10 of 100 Train loss: 0.6927 Train f_score 0.6751 Xval loss: 0.6927 Xval f_score 0.6604\n",
      "00:35:53 epoch 20 of 100 Train loss: 0.6924 Train f_score 0.6751 Xval loss: 0.6927 Xval f_score 0.6604\n",
      "00:35:53 epoch 30 of 100 Train loss: 0.6920 Train f_score 0.6751 Xval loss: 0.6925 Xval f_score 0.6604\n",
      "00:35:53 epoch 40 of 100 Train loss: 0.6914 Train f_score 0.6773 Xval loss: 0.6920 Xval f_score 0.6667\n",
      "00:35:53 epoch 50 of 100 Train loss: 0.6907 Train f_score 0.6776 Xval loss: 0.6915 Xval f_score 0.6535\n",
      "00:35:53 epoch 60 of 100 Train loss: 0.6898 Train f_score 0.6897 Xval loss: 0.6911 Xval f_score 0.5957\n",
      "00:35:53 epoch 70 of 100 Train loss: 0.6888 Train f_score 0.6741 Xval loss: 0.6908 Xval f_score 0.6000\n",
      "00:35:53 epoch 80 of 100 Train loss: 0.6877 Train f_score 0.6353 Xval loss: 0.6906 Xval f_score 0.5647\n",
      "00:35:53 epoch 90 of 100 Train loss: 0.6864 Train f_score 0.6154 Xval loss: 0.6906 Xval f_score 0.5714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Xval loss epoch 86, value 0.690620\n",
      "NN units 4\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.581, Train F1 0.681\n",
      "[[28 13]\n",
      " [75 94]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.597\n",
      "[[17 12]\n",
      " [19 23]]\n",
      "00:35:54 Starting\n",
      "00:35:55 epoch 0 of 100 Train loss: 0.6948 Train f_score 0.0000 Xval loss: 0.6942 Xval f_score 0.0000\n",
      "00:35:55 epoch 10 of 100 Train loss: 0.6940 Train f_score 0.6601 Xval loss: 0.6941 Xval f_score 0.6337\n",
      "00:35:55 epoch 20 of 100 Train loss: 0.6935 Train f_score 0.6751 Xval loss: 0.6941 Xval f_score 0.6604\n",
      "00:35:55 epoch 30 of 100 Train loss: 0.6931 Train f_score 0.6751 Xval loss: 0.6940 Xval f_score 0.6604\n",
      "00:35:55 epoch 40 of 100 Train loss: 0.6927 Train f_score 0.6751 Xval loss: 0.6938 Xval f_score 0.6604\n",
      "00:35:55 epoch 50 of 100 Train loss: 0.6922 Train f_score 0.6751 Xval loss: 0.6936 Xval f_score 0.6604\n",
      "00:35:55 epoch 60 of 100 Train loss: 0.6917 Train f_score 0.6794 Xval loss: 0.6934 Xval f_score 0.6476\n",
      "00:35:55 epoch 70 of 100 Train loss: 0.6910 Train f_score 0.6710 Xval loss: 0.6932 Xval f_score 0.6275\n",
      "00:35:55 epoch 80 of 100 Train loss: 0.6902 Train f_score 0.6667 Xval loss: 0.6929 Xval f_score 0.6327\n",
      "00:35:55 epoch 90 of 100 Train loss: 0.6892 Train f_score 0.6944 Xval loss: 0.6927 Xval f_score 0.6154\n",
      "Best Xval loss epoch 99, value 0.692468\n",
      "NN units 4\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.595, Train F1 0.700\n",
      "[[26  8]\n",
      " [77 99]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.423\n",
      "[[30 24]\n",
      " [ 6 11]]\n",
      "00:35:55 Starting\n",
      "00:35:56 epoch 0 of 100 Train loss: 0.6964 Train f_score 0.0000 Xval loss: 0.6944 Xval f_score 0.0000\n",
      "00:35:57 epoch 10 of 100 Train loss: 0.6951 Train f_score 0.0000 Xval loss: 0.6940 Xval f_score 0.0000\n",
      "00:35:57 epoch 20 of 100 Train loss: 0.6942 Train f_score 0.0000 Xval loss: 0.6938 Xval f_score 0.0000\n",
      "00:35:57 epoch 30 of 100 Train loss: 0.6936 Train f_score 0.6304 Xval loss: 0.6938 Xval f_score 0.6465\n",
      "00:35:57 epoch 40 of 100 Train loss: 0.6933 Train f_score 0.6751 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:35:57 epoch 50 of 100 Train loss: 0.6931 Train f_score 0.6751 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:35:57 epoch 60 of 100 Train loss: 0.6929 Train f_score 0.6751 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:35:57 epoch 70 of 100 Train loss: 0.6928 Train f_score 0.6751 Xval loss: 0.6940 Xval f_score 0.6604\n",
      "00:35:57 epoch 80 of 100 Train loss: 0.6926 Train f_score 0.6751 Xval loss: 0.6940 Xval f_score 0.6604\n",
      "00:35:57 epoch 90 of 100 Train loss: 0.6924 Train f_score 0.6751 Xval loss: 0.6941 Xval f_score 0.6476\n",
      "Best Xval loss epoch 26, value 0.693793\n",
      "NN units 4\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.595, Train F1 0.647\n",
      "[[47 29]\n",
      " [56 78]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.595\n",
      "[[19 13]\n",
      " [17 22]]\n",
      "00:35:57 Starting\n",
      "00:35:58 epoch 0 of 100 Train loss: 0.6975 Train f_score 0.6751 Xval loss: 0.6978 Xval f_score 0.6604\n",
      "00:35:58 epoch 10 of 100 Train loss: 0.6964 Train f_score 0.6751 Xval loss: 0.6969 Xval f_score 0.6604\n",
      "00:35:58 epoch 20 of 100 Train loss: 0.6955 Train f_score 0.6751 Xval loss: 0.6960 Xval f_score 0.6604\n",
      "00:35:58 epoch 30 of 100 Train loss: 0.6948 Train f_score 0.6751 Xval loss: 0.6953 Xval f_score 0.6604\n",
      "00:35:59 epoch 40 of 100 Train loss: 0.6942 Train f_score 0.6751 Xval loss: 0.6947 Xval f_score 0.6604\n",
      "00:35:59 epoch 50 of 100 Train loss: 0.6938 Train f_score 0.6751 Xval loss: 0.6944 Xval f_score 0.6604\n",
      "00:35:59 epoch 60 of 100 Train loss: 0.6934 Train f_score 0.6751 Xval loss: 0.6941 Xval f_score 0.6604\n",
      "00:35:59 epoch 70 of 100 Train loss: 0.6932 Train f_score 0.6751 Xval loss: 0.6938 Xval f_score 0.6604\n",
      "00:35:59 epoch 80 of 100 Train loss: 0.6931 Train f_score 0.6751 Xval loss: 0.6938 Xval f_score 0.6604\n",
      "00:35:59 epoch 90 of 100 Train loss: 0.6930 Train f_score 0.6751 Xval loss: 0.6937 Xval f_score 0.6604\n",
      "Best Xval loss epoch 97, value 0.693618\n",
      "NN units 4\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.519, Train F1 0.284\n",
      "[[89 87]\n",
      " [14 20]]\n",
      "Final Xval Accuracy 0.535, Xval F1 0.233\n",
      "[[33 30]\n",
      " [ 3  5]]\n",
      "00:35:59 Starting\n",
      "00:36:01 epoch 0 of 100 Train loss: 0.7057 Train f_score 0.6751 Xval loss: 0.7061 Xval f_score 0.6604\n",
      "00:36:01 epoch 10 of 100 Train loss: 0.7023 Train f_score 0.6751 Xval loss: 0.7026 Xval f_score 0.6604\n",
      "00:36:01 epoch 20 of 100 Train loss: 0.6997 Train f_score 0.6751 Xval loss: 0.7000 Xval f_score 0.6604\n",
      "00:36:01 epoch 30 of 100 Train loss: 0.6976 Train f_score 0.6751 Xval loss: 0.6981 Xval f_score 0.6604\n",
      "00:36:01 epoch 40 of 100 Train loss: 0.6960 Train f_score 0.6751 Xval loss: 0.6965 Xval f_score 0.6604\n",
      "00:36:01 epoch 50 of 100 Train loss: 0.6949 Train f_score 0.6751 Xval loss: 0.6954 Xval f_score 0.6604\n",
      "00:36:01 epoch 60 of 100 Train loss: 0.6940 Train f_score 0.6751 Xval loss: 0.6946 Xval f_score 0.6604\n",
      "00:36:01 epoch 70 of 100 Train loss: 0.6936 Train f_score 0.6751 Xval loss: 0.6942 Xval f_score 0.6604\n",
      "00:36:01 epoch 80 of 100 Train loss: 0.6933 Train f_score 0.6751 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:36:01 epoch 90 of 100 Train loss: 0.6931 Train f_score 0.6751 Xval loss: 0.6937 Xval f_score 0.6604\n",
      "Best Xval loss epoch 98, value 0.693692\n",
      "NN units 4\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.529, Train F1 0.680\n",
      "[[  6   2]\n",
      " [ 97 105]]\n",
      "Final Xval Accuracy 0.648, Xval F1 0.667\n",
      "[[21 10]\n",
      " [15 25]]\n",
      "00:36:02 Starting\n",
      "00:36:03 epoch 0 of 100 Train loss: 0.6933 Train f_score 0.0185 Xval loss: 0.6932 Xval f_score 0.0000\n",
      "00:36:03 epoch 10 of 100 Train loss: 0.6921 Train f_score 0.6795 Xval loss: 0.6929 Xval f_score 0.6346\n",
      "00:36:03 epoch 20 of 100 Train loss: 0.6913 Train f_score 0.6772 Xval loss: 0.6927 Xval f_score 0.6604\n",
      "00:36:03 epoch 30 of 100 Train loss: 0.6904 Train f_score 0.6710 Xval loss: 0.6923 Xval f_score 0.6538\n",
      "00:36:03 epoch 40 of 100 Train loss: 0.6894 Train f_score 0.6754 Xval loss: 0.6918 Xval f_score 0.6042\n",
      "00:36:03 epoch 50 of 100 Train loss: 0.6883 Train f_score 0.6851 Xval loss: 0.6914 Xval f_score 0.6022\n",
      "00:36:03 epoch 60 of 100 Train loss: 0.6870 Train f_score 0.6667 Xval loss: 0.6911 Xval f_score 0.5843\n",
      "00:36:03 epoch 70 of 100 Train loss: 0.6856 Train f_score 0.6691 Xval loss: 0.6910 Xval f_score 0.5814\n",
      "00:36:03 epoch 80 of 100 Train loss: 0.6841 Train f_score 0.6431 Xval loss: 0.6911 Xval f_score 0.5882\n",
      "00:36:03 epoch 90 of 100 Train loss: 0.6824 Train f_score 0.6508 Xval loss: 0.6913 Xval f_score 0.5882\n",
      "Best Xval loss epoch 71, value 0.691007\n",
      "NN units 8\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.595, Train F1 0.644\n",
      "[[48 30]\n",
      " [55 77]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.615\n",
      "[[17 11]\n",
      " [19 24]]\n",
      "00:36:04 Starting\n",
      "00:36:05 epoch 0 of 100 Train loss: 0.6940 Train f_score 0.6751 Xval loss: 0.6941 Xval f_score 0.6604\n",
      "00:36:05 epoch 10 of 100 Train loss: 0.6934 Train f_score 0.6751 Xval loss: 0.6935 Xval f_score 0.6604\n",
      "00:36:05 epoch 20 of 100 Train loss: 0.6927 Train f_score 0.6751 Xval loss: 0.6934 Xval f_score 0.6604\n",
      "00:36:05 epoch 30 of 100 Train loss: 0.6919 Train f_score 0.6730 Xval loss: 0.6932 Xval f_score 0.6538\n",
      "00:36:05 epoch 40 of 100 Train loss: 0.6909 Train f_score 0.6753 Xval loss: 0.6928 Xval f_score 0.6400\n",
      "00:36:05 epoch 50 of 100 Train loss: 0.6896 Train f_score 0.6783 Xval loss: 0.6925 Xval f_score 0.6170\n",
      "00:36:05 epoch 60 of 100 Train loss: 0.6880 Train f_score 0.6444 Xval loss: 0.6921 Xval f_score 0.6154\n",
      "00:36:05 epoch 70 of 100 Train loss: 0.6863 Train f_score 0.6324 Xval loss: 0.6919 Xval f_score 0.6207\n",
      "00:36:05 epoch 80 of 100 Train loss: 0.6845 Train f_score 0.6390 Xval loss: 0.6919 Xval f_score 0.5926\n",
      "00:36:05 epoch 90 of 100 Train loss: 0.6826 Train f_score 0.6180 Xval loss: 0.6921 Xval f_score 0.6076\n",
      "Best Xval loss epoch 76, value 0.691836\n",
      "NN units 8\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.605, Train F1 0.556\n",
      "[[75 55]\n",
      " [28 52]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.643\n",
      "[[14  8]\n",
      " [22 27]]\n",
      "00:36:06 Starting\n",
      "00:36:07 epoch 0 of 100 Train loss: 0.6966 Train f_score 0.6751 Xval loss: 0.6969 Xval f_score 0.6604\n",
      "00:36:07 epoch 10 of 100 Train loss: 0.6954 Train f_score 0.6751 Xval loss: 0.6960 Xval f_score 0.6604\n",
      "00:36:07 epoch 20 of 100 Train loss: 0.6943 Train f_score 0.6751 Xval loss: 0.6954 Xval f_score 0.6604\n",
      "00:36:08 epoch 30 of 100 Train loss: 0.6934 Train f_score 0.6772 Xval loss: 0.6949 Xval f_score 0.6604\n",
      "00:36:08 epoch 40 of 100 Train loss: 0.6926 Train f_score 0.6773 Xval loss: 0.6946 Xval f_score 0.6408\n",
      "00:36:08 epoch 50 of 100 Train loss: 0.6918 Train f_score 0.6775 Xval loss: 0.6945 Xval f_score 0.6465\n",
      "00:36:08 epoch 60 of 100 Train loss: 0.6909 Train f_score 0.6781 Xval loss: 0.6944 Xval f_score 0.6105\n",
      "00:36:08 epoch 70 of 100 Train loss: 0.6898 Train f_score 0.6715 Xval loss: 0.6944 Xval f_score 0.6000\n",
      "00:36:08 epoch 80 of 100 Train loss: 0.6887 Train f_score 0.6540 Xval loss: 0.6946 Xval f_score 0.6190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:36:08 epoch 90 of 100 Train loss: 0.6874 Train f_score 0.6345 Xval loss: 0.6948 Xval f_score 0.5926\n",
      "Best Xval loss epoch 58, value 0.694420\n",
      "NN units 8\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.590, Train F1 0.636\n",
      "[[49 32]\n",
      " [54 75]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.628\n",
      "[[12  8]\n",
      " [24 27]]\n",
      "00:36:09 Starting\n",
      "00:36:10 epoch 0 of 100 Train loss: 0.7051 Train f_score 0.0000 Xval loss: 0.7029 Xval f_score 0.0000\n",
      "00:36:10 epoch 10 of 100 Train loss: 0.7011 Train f_score 0.0000 Xval loss: 0.7000 Xval f_score 0.0000\n",
      "00:36:10 epoch 20 of 100 Train loss: 0.6983 Train f_score 0.0000 Xval loss: 0.6983 Xval f_score 0.0000\n",
      "00:36:10 epoch 30 of 100 Train loss: 0.6966 Train f_score 0.6275 Xval loss: 0.6972 Xval f_score 0.5979\n",
      "00:36:10 epoch 40 of 100 Train loss: 0.6954 Train f_score 0.6751 Xval loss: 0.6965 Xval f_score 0.6604\n",
      "00:36:10 epoch 50 of 100 Train loss: 0.6947 Train f_score 0.6751 Xval loss: 0.6960 Xval f_score 0.6604\n",
      "00:36:10 epoch 60 of 100 Train loss: 0.6941 Train f_score 0.6751 Xval loss: 0.6955 Xval f_score 0.6604\n",
      "00:36:10 epoch 70 of 100 Train loss: 0.6937 Train f_score 0.6751 Xval loss: 0.6950 Xval f_score 0.6604\n",
      "00:36:10 epoch 80 of 100 Train loss: 0.6935 Train f_score 0.6751 Xval loss: 0.6947 Xval f_score 0.6604\n",
      "00:36:10 epoch 90 of 100 Train loss: 0.6933 Train f_score 0.6751 Xval loss: 0.6946 Xval f_score 0.6604\n",
      "Best Xval loss epoch 97, value 0.694581\n",
      "NN units 8\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.619, Train F1 0.667\n",
      "[[50 27]\n",
      " [53 80]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.644\n",
      "[[10  6]\n",
      " [26 29]]\n",
      "00:36:11 Starting\n",
      "00:36:12 epoch 0 of 100 Train loss: 0.7194 Train f_score 0.0000 Xval loss: 0.7188 Xval f_score 0.0000\n",
      "00:36:12 epoch 10 of 100 Train loss: 0.7121 Train f_score 0.1832 Xval loss: 0.7123 Xval f_score 0.1923\n",
      "00:36:12 epoch 20 of 100 Train loss: 0.7069 Train f_score 0.6751 Xval loss: 0.7076 Xval f_score 0.6604\n",
      "00:36:12 epoch 30 of 100 Train loss: 0.7030 Train f_score 0.6751 Xval loss: 0.7037 Xval f_score 0.6604\n",
      "00:36:12 epoch 40 of 100 Train loss: 0.7000 Train f_score 0.6751 Xval loss: 0.7005 Xval f_score 0.6604\n",
      "00:36:12 epoch 50 of 100 Train loss: 0.6976 Train f_score 0.6751 Xval loss: 0.6981 Xval f_score 0.6604\n",
      "00:36:12 epoch 60 of 100 Train loss: 0.6958 Train f_score 0.6751 Xval loss: 0.6964 Xval f_score 0.6604\n",
      "00:36:12 epoch 70 of 100 Train loss: 0.6947 Train f_score 0.6751 Xval loss: 0.6952 Xval f_score 0.6604\n",
      "00:36:12 epoch 80 of 100 Train loss: 0.6939 Train f_score 0.6751 Xval loss: 0.6945 Xval f_score 0.6604\n",
      "00:36:12 epoch 90 of 100 Train loss: 0.6935 Train f_score 0.6751 Xval loss: 0.6941 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.693910\n",
      "NN units 8\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.538, Train F1 0.405\n",
      "[[80 74]\n",
      " [23 33]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.333\n",
      "[[31 27]\n",
      " [ 5  8]]\n",
      "00:36:13 Starting\n",
      "00:36:15 epoch 0 of 100 Train loss: 0.6947 Train f_score 0.6751 Xval loss: 0.6954 Xval f_score 0.6604\n",
      "00:36:15 epoch 10 of 100 Train loss: 0.6928 Train f_score 0.6772 Xval loss: 0.6928 Xval f_score 0.6476\n",
      "00:36:15 epoch 20 of 100 Train loss: 0.6917 Train f_score 0.6207 Xval loss: 0.6920 Xval f_score 0.6118\n",
      "00:36:15 epoch 30 of 100 Train loss: 0.6905 Train f_score 0.6620 Xval loss: 0.6917 Xval f_score 0.5934\n",
      "00:36:15 epoch 40 of 100 Train loss: 0.6891 Train f_score 0.6517 Xval loss: 0.6912 Xval f_score 0.5714\n",
      "00:36:15 epoch 50 of 100 Train loss: 0.6874 Train f_score 0.5941 Xval loss: 0.6908 Xval f_score 0.5500\n",
      "00:36:15 epoch 60 of 100 Train loss: 0.6852 Train f_score 0.5913 Xval loss: 0.6906 Xval f_score 0.5455\n",
      "00:36:15 epoch 70 of 100 Train loss: 0.6828 Train f_score 0.5778 Xval loss: 0.6908 Xval f_score 0.5455\n",
      "00:36:15 epoch 80 of 100 Train loss: 0.6802 Train f_score 0.5804 Xval loss: 0.6915 Xval f_score 0.5526\n",
      "00:36:15 epoch 90 of 100 Train loss: 0.6773 Train f_score 0.5753 Xval loss: 0.6926 Xval f_score 0.5789\n",
      "Best Xval loss epoch 59, value 0.690603\n",
      "NN units 16\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.595, Train F1 0.585\n",
      "[[65 47]\n",
      " [38 60]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.617\n",
      "[[15 10]\n",
      " [21 25]]\n",
      "00:36:16 Starting\n",
      "00:36:17 epoch 0 of 100 Train loss: 0.6953 Train f_score 0.6645 Xval loss: 0.6952 Xval f_score 0.6604\n",
      "00:36:17 epoch 10 of 100 Train loss: 0.6939 Train f_score 0.6751 Xval loss: 0.6948 Xval f_score 0.6604\n",
      "00:36:17 epoch 20 of 100 Train loss: 0.6926 Train f_score 0.6709 Xval loss: 0.6940 Xval f_score 0.6538\n",
      "00:36:17 epoch 30 of 100 Train loss: 0.6911 Train f_score 0.6826 Xval loss: 0.6933 Xval f_score 0.6327\n",
      "00:36:17 epoch 40 of 100 Train loss: 0.6893 Train f_score 0.6692 Xval loss: 0.6927 Xval f_score 0.6207\n",
      "00:36:17 epoch 50 of 100 Train loss: 0.6871 Train f_score 0.6290 Xval loss: 0.6922 Xval f_score 0.5542\n",
      "00:36:17 epoch 60 of 100 Train loss: 0.6848 Train f_score 0.6068 Xval loss: 0.6920 Xval f_score 0.5455\n",
      "00:36:17 epoch 70 of 100 Train loss: 0.6823 Train f_score 0.5982 Xval loss: 0.6923 Xval f_score 0.5455\n",
      "00:36:17 epoch 80 of 100 Train loss: 0.6797 Train f_score 0.5909 Xval loss: 0.6931 Xval f_score 0.5714\n",
      "00:36:17 epoch 90 of 100 Train loss: 0.6771 Train f_score 0.6009 Xval loss: 0.6946 Xval f_score 0.5600\n",
      "Best Xval loss epoch 59, value 0.691964\n",
      "NN units 16\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.624, Train F1 0.669\n",
      "[[51 27]\n",
      " [52 80]]\n",
      "Final Xval Accuracy 0.606, Xval F1 0.667\n",
      "[[15  7]\n",
      " [21 28]]\n",
      "00:36:18 Starting\n",
      "00:36:19 epoch 0 of 100 Train loss: 0.6979 Train f_score 0.6815 Xval loss: 0.6995 Xval f_score 0.6476\n",
      "00:36:20 epoch 10 of 100 Train loss: 0.6962 Train f_score 0.6772 Xval loss: 0.6988 Xval f_score 0.6346\n",
      "00:36:20 epoch 20 of 100 Train loss: 0.6945 Train f_score 0.6903 Xval loss: 0.6981 Xval f_score 0.6078\n",
      "00:36:20 epoch 30 of 100 Train loss: 0.6928 Train f_score 0.6644 Xval loss: 0.6978 Xval f_score 0.6022\n",
      "00:36:20 epoch 40 of 100 Train loss: 0.6911 Train f_score 0.6304 Xval loss: 0.6975 Xval f_score 0.5843\n",
      "00:36:20 epoch 50 of 100 Train loss: 0.6892 Train f_score 0.6275 Xval loss: 0.6976 Xval f_score 0.5610\n",
      "00:36:20 epoch 60 of 100 Train loss: 0.6873 Train f_score 0.6204 Xval loss: 0.6979 Xval f_score 0.5823\n",
      "00:36:20 epoch 70 of 100 Train loss: 0.6852 Train f_score 0.6186 Xval loss: 0.6984 Xval f_score 0.5897\n",
      "00:36:20 epoch 80 of 100 Train loss: 0.6832 Train f_score 0.6261 Xval loss: 0.6992 Xval f_score 0.5714\n",
      "00:36:20 epoch 90 of 100 Train loss: 0.6811 Train f_score 0.6404 Xval loss: 0.7003 Xval f_score 0.5714\n",
      "Best Xval loss epoch 43, value 0.697516\n",
      "NN units 16\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.610, Train F1 0.655\n",
      "[[50 29]\n",
      " [53 78]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.634\n",
      "[[15  9]\n",
      " [21 26]]\n",
      "00:36:21 Starting\n",
      "00:36:22 epoch 0 of 100 Train loss: 0.7130 Train f_score 0.0000 Xval loss: 0.7108 Xval f_score 0.0556\n",
      "00:36:22 epoch 10 of 100 Train loss: 0.7075 Train f_score 0.6772 Xval loss: 0.7072 Xval f_score 0.6604\n",
      "00:36:22 epoch 20 of 100 Train loss: 0.7038 Train f_score 0.6751 Xval loss: 0.7044 Xval f_score 0.6604\n",
      "00:36:22 epoch 30 of 100 Train loss: 0.7006 Train f_score 0.6751 Xval loss: 0.7013 Xval f_score 0.6604\n",
      "00:36:22 epoch 40 of 100 Train loss: 0.6982 Train f_score 0.6751 Xval loss: 0.6991 Xval f_score 0.6604\n",
      "00:36:22 epoch 50 of 100 Train loss: 0.6964 Train f_score 0.6751 Xval loss: 0.6977 Xval f_score 0.6604\n",
      "00:36:22 epoch 60 of 100 Train loss: 0.6950 Train f_score 0.6751 Xval loss: 0.6966 Xval f_score 0.6604\n",
      "00:36:22 epoch 70 of 100 Train loss: 0.6941 Train f_score 0.6751 Xval loss: 0.6959 Xval f_score 0.6604\n",
      "00:36:22 epoch 80 of 100 Train loss: 0.6935 Train f_score 0.6751 Xval loss: 0.6955 Xval f_score 0.6476\n",
      "00:36:22 epoch 90 of 100 Train loss: 0.6930 Train f_score 0.6794 Xval loss: 0.6954 Xval f_score 0.6476\n",
      "Best Xval loss epoch 89, value 0.695405\n",
      "NN units 16\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.581, Train F1 0.679\n",
      "[[29 14]\n",
      " [74 93]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.583\n",
      "[[20 14]\n",
      " [16 21]]\n",
      "00:36:23 Starting\n",
      "00:36:25 epoch 0 of 100 Train loss: 0.7494 Train f_score 0.1705 Xval loss: 0.7481 Xval f_score 0.4242\n",
      "00:36:25 epoch 10 of 100 Train loss: 0.7359 Train f_score 0.6751 Xval loss: 0.7356 Xval f_score 0.6604\n",
      "00:36:25 epoch 20 of 100 Train loss: 0.7244 Train f_score 0.6751 Xval loss: 0.7241 Xval f_score 0.6604\n",
      "00:36:25 epoch 30 of 100 Train loss: 0.7151 Train f_score 0.6751 Xval loss: 0.7149 Xval f_score 0.6604\n",
      "00:36:25 epoch 40 of 100 Train loss: 0.7079 Train f_score 0.6751 Xval loss: 0.7078 Xval f_score 0.6604\n",
      "00:36:25 epoch 50 of 100 Train loss: 0.7027 Train f_score 0.6751 Xval loss: 0.7029 Xval f_score 0.6604\n",
      "00:36:25 epoch 60 of 100 Train loss: 0.6989 Train f_score 0.6751 Xval loss: 0.6992 Xval f_score 0.6604\n",
      "00:36:25 epoch 70 of 100 Train loss: 0.6963 Train f_score 0.6751 Xval loss: 0.6967 Xval f_score 0.6604\n",
      "00:36:25 epoch 80 of 100 Train loss: 0.6946 Train f_score 0.6751 Xval loss: 0.6951 Xval f_score 0.6604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:36:25 epoch 90 of 100 Train loss: 0.6937 Train f_score 0.6751 Xval loss: 0.6943 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.694009\n",
      "NN units 16\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.519, Train F1 0.673\n",
      "[[  5   3]\n",
      " [ 98 104]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.652\n",
      "[[ 9  5]\n",
      " [27 30]]\n",
      "00:36:26 Starting\n",
      "00:36:27 epoch 0 of 100 Train loss: 0.6958 Train f_score 0.0000 Xval loss: 0.6934 Xval f_score 0.0000\n",
      "00:36:27 epoch 10 of 100 Train loss: 0.6912 Train f_score 0.6667 Xval loss: 0.6928 Xval f_score 0.6476\n",
      "00:36:27 epoch 20 of 100 Train loss: 0.6894 Train f_score 0.6731 Xval loss: 0.6920 Xval f_score 0.6538\n",
      "00:36:27 epoch 30 of 100 Train loss: 0.6872 Train f_score 0.6198 Xval loss: 0.6904 Xval f_score 0.5854\n",
      "00:36:27 epoch 40 of 100 Train loss: 0.6850 Train f_score 0.5727 Xval loss: 0.6900 Xval f_score 0.5641\n",
      "00:36:27 epoch 50 of 100 Train loss: 0.6824 Train f_score 0.6034 Xval loss: 0.6901 Xval f_score 0.5679\n",
      "00:36:27 epoch 60 of 100 Train loss: 0.6795 Train f_score 0.5830 Xval loss: 0.6905 Xval f_score 0.5714\n",
      "00:36:27 epoch 70 of 100 Train loss: 0.6763 Train f_score 0.5845 Xval loss: 0.6917 Xval f_score 0.5405\n",
      "00:36:27 epoch 80 of 100 Train loss: 0.6730 Train f_score 0.6027 Xval loss: 0.6937 Xval f_score 0.5600\n",
      "00:36:27 epoch 90 of 100 Train loss: 0.6695 Train f_score 0.6147 Xval loss: 0.6964 Xval f_score 0.5789\n",
      "Best Xval loss epoch 41, value 0.689999\n",
      "NN units 32\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.629, Train F1 0.618\n",
      "[[69 44]\n",
      " [34 63]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.651\n",
      "[[13  7]\n",
      " [23 28]]\n",
      "00:36:28 Starting\n",
      "00:36:30 epoch 0 of 100 Train loss: 0.7032 Train f_score 0.0000 Xval loss: 0.6996 Xval f_score 0.0000\n",
      "00:36:30 epoch 10 of 100 Train loss: 0.6949 Train f_score 0.1653 Xval loss: 0.6957 Xval f_score 0.2381\n",
      "00:36:30 epoch 20 of 100 Train loss: 0.6927 Train f_score 0.6859 Xval loss: 0.6962 Xval f_score 0.6408\n",
      "00:36:30 epoch 30 of 100 Train loss: 0.6911 Train f_score 0.6796 Xval loss: 0.6956 Xval f_score 0.6200\n",
      "00:36:30 epoch 40 of 100 Train loss: 0.6894 Train f_score 0.6541 Xval loss: 0.6946 Xval f_score 0.5814\n",
      "00:36:30 epoch 50 of 100 Train loss: 0.6878 Train f_score 0.6167 Xval loss: 0.6943 Xval f_score 0.5455\n",
      "00:36:30 epoch 60 of 100 Train loss: 0.6861 Train f_score 0.6393 Xval loss: 0.6945 Xval f_score 0.5926\n",
      "00:36:30 epoch 70 of 100 Train loss: 0.6843 Train f_score 0.6250 Xval loss: 0.6948 Xval f_score 0.6000\n",
      "00:36:30 epoch 80 of 100 Train loss: 0.6823 Train f_score 0.6383 Xval loss: 0.6952 Xval f_score 0.5714\n",
      "00:36:30 epoch 90 of 100 Train loss: 0.6802 Train f_score 0.6288 Xval loss: 0.6960 Xval f_score 0.5714\n",
      "Best Xval loss epoch 50, value 0.694266\n",
      "NN units 32\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.614, Train F1 0.652\n",
      "[[53 31]\n",
      " [50 76]]\n",
      "Final Xval Accuracy 0.592, Xval F1 0.642\n",
      "[[16  9]\n",
      " [20 26]]\n",
      "00:36:32 Starting\n",
      "00:36:33 epoch 0 of 100 Train loss: 0.7036 Train f_score 0.6751 Xval loss: 0.7049 Xval f_score 0.6275\n",
      "00:36:33 epoch 10 of 100 Train loss: 0.7001 Train f_score 0.6601 Xval loss: 0.7028 Xval f_score 0.5833\n",
      "00:36:33 epoch 20 of 100 Train loss: 0.6967 Train f_score 0.6471 Xval loss: 0.7012 Xval f_score 0.5581\n",
      "00:36:33 epoch 30 of 100 Train loss: 0.6935 Train f_score 0.6353 Xval loss: 0.7004 Xval f_score 0.5854\n",
      "00:36:33 epoch 40 of 100 Train loss: 0.6904 Train f_score 0.6134 Xval loss: 0.7001 Xval f_score 0.5750\n",
      "00:36:33 epoch 50 of 100 Train loss: 0.6874 Train f_score 0.6061 Xval loss: 0.7003 Xval f_score 0.5897\n",
      "00:36:33 epoch 60 of 100 Train loss: 0.6844 Train f_score 0.6114 Xval loss: 0.7011 Xval f_score 0.5714\n",
      "00:36:33 epoch 70 of 100 Train loss: 0.6815 Train f_score 0.6099 Xval loss: 0.7024 Xval f_score 0.5789\n",
      "00:36:33 epoch 80 of 100 Train loss: 0.6788 Train f_score 0.6099 Xval loss: 0.7045 Xval f_score 0.5974\n",
      "00:36:33 epoch 90 of 100 Train loss: 0.6763 Train f_score 0.6283 Xval loss: 0.7073 Xval f_score 0.5897\n",
      "Best Xval loss epoch 40, value 0.700072\n",
      "NN units 32\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.624, Train F1 0.672\n",
      "[[50 26]\n",
      " [53 81]]\n",
      "Final Xval Accuracy 0.606, Xval F1 0.667\n",
      "[[15  7]\n",
      " [21 28]]\n",
      "00:36:34 Starting\n",
      "00:36:36 epoch 0 of 100 Train loss: 0.7285 Train f_score 0.6751 Xval loss: 0.7294 Xval f_score 0.6604\n",
      "00:36:36 epoch 10 of 100 Train loss: 0.7190 Train f_score 0.6510 Xval loss: 0.7193 Xval f_score 0.6105\n",
      "00:36:36 epoch 20 of 100 Train loss: 0.7117 Train f_score 0.6772 Xval loss: 0.7131 Xval f_score 0.6604\n",
      "00:36:36 epoch 30 of 100 Train loss: 0.7059 Train f_score 0.6794 Xval loss: 0.7074 Xval f_score 0.6476\n",
      "00:36:36 epoch 40 of 100 Train loss: 0.7018 Train f_score 0.6794 Xval loss: 0.7037 Xval f_score 0.6604\n",
      "00:36:36 epoch 50 of 100 Train loss: 0.6986 Train f_score 0.6815 Xval loss: 0.7009 Xval f_score 0.6476\n",
      "00:36:36 epoch 60 of 100 Train loss: 0.6963 Train f_score 0.6815 Xval loss: 0.6991 Xval f_score 0.6476\n",
      "00:36:36 epoch 70 of 100 Train loss: 0.6947 Train f_score 0.6837 Xval loss: 0.6981 Xval f_score 0.6408\n",
      "00:36:36 epoch 80 of 100 Train loss: 0.6936 Train f_score 0.6837 Xval loss: 0.6977 Xval f_score 0.6408\n",
      "00:36:36 epoch 90 of 100 Train loss: 0.6927 Train f_score 0.6797 Xval loss: 0.6977 Xval f_score 0.6408\n",
      "Best Xval loss epoch 84, value 0.697623\n",
      "NN units 32\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.595, Train F1 0.644\n",
      "[[48 30]\n",
      " [55 77]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.619\n",
      "[[13  9]\n",
      " [23 26]]\n",
      "00:36:37 Starting\n",
      "00:36:38 epoch 0 of 100 Train loss: 0.8009 Train f_score 0.6751 Xval loss: 0.7998 Xval f_score 0.6604\n",
      "00:36:38 epoch 10 of 100 Train loss: 0.7745 Train f_score 0.6709 Xval loss: 0.7730 Xval f_score 0.6061\n",
      "00:36:38 epoch 20 of 100 Train loss: 0.7529 Train f_score 0.6751 Xval loss: 0.7523 Xval f_score 0.6604\n",
      "00:36:38 epoch 30 of 100 Train loss: 0.7358 Train f_score 0.6751 Xval loss: 0.7353 Xval f_score 0.6604\n",
      "00:36:38 epoch 40 of 100 Train loss: 0.7225 Train f_score 0.6751 Xval loss: 0.7222 Xval f_score 0.6604\n",
      "00:36:38 epoch 50 of 100 Train loss: 0.7121 Train f_score 0.6751 Xval loss: 0.7120 Xval f_score 0.6604\n",
      "00:36:39 epoch 60 of 100 Train loss: 0.7049 Train f_score 0.6751 Xval loss: 0.7049 Xval f_score 0.6604\n",
      "00:36:39 epoch 70 of 100 Train loss: 0.6998 Train f_score 0.6751 Xval loss: 0.6998 Xval f_score 0.6604\n",
      "00:36:39 epoch 80 of 100 Train loss: 0.6964 Train f_score 0.6751 Xval loss: 0.6968 Xval f_score 0.6604\n",
      "00:36:39 epoch 90 of 100 Train loss: 0.6947 Train f_score 0.6751 Xval loss: 0.6950 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.694469\n",
      "NN units 32\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.0000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.552, Train F1 0.615\n",
      "[[41 32]\n",
      " [62 75]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.400\n",
      "[[31 25]\n",
      " [ 5 10]]\n",
      "00:36:40 Starting\n",
      "00:36:41 epoch 0 of 100 Train loss: 0.6929 Train f_score 0.2774 Xval loss: 0.6935 Xval f_score 0.4638\n",
      "00:36:41 epoch 10 of 100 Train loss: 0.6925 Train f_score 0.6735 Xval loss: 0.6939 Xval f_score 0.6078\n",
      "00:36:41 epoch 20 of 100 Train loss: 0.6914 Train f_score 0.6840 Xval loss: 0.6945 Xval f_score 0.6214\n",
      "00:36:41 epoch 30 of 100 Train loss: 0.6906 Train f_score 0.6755 Xval loss: 0.6953 Xval f_score 0.6214\n",
      "00:36:41 epoch 40 of 100 Train loss: 0.6907 Train f_score 0.6758 Xval loss: 0.6962 Xval f_score 0.6000\n",
      "00:36:41 epoch 50 of 100 Train loss: 0.6861 Train f_score 0.6877 Xval loss: 0.6972 Xval f_score 0.6061\n",
      "00:36:41 epoch 60 of 100 Train loss: 0.6867 Train f_score 0.6715 Xval loss: 0.6988 Xval f_score 0.5833\n",
      "00:36:41 epoch 70 of 100 Train loss: 0.6832 Train f_score 0.6515 Xval loss: 0.7005 Xval f_score 0.5745\n",
      "00:36:41 epoch 80 of 100 Train loss: 0.6793 Train f_score 0.6844 Xval loss: 0.7024 Xval f_score 0.5745\n",
      "00:36:41 epoch 90 of 100 Train loss: 0.6764 Train f_score 0.6250 Xval loss: 0.7043 Xval f_score 0.5652\n",
      "Best Xval loss epoch 0, value 0.693517\n",
      "NN units 4\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.614, Train F1 0.612\n",
      "[[65 43]\n",
      " [38 64]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.360\n",
      "[[30 26]\n",
      " [ 6  9]]\n",
      "00:36:43 Starting\n",
      "00:36:44 epoch 0 of 100 Train loss: 0.6938 Train f_score 0.3418 Xval loss: 0.6937 Xval f_score 0.3934\n",
      "00:36:44 epoch 10 of 100 Train loss: 0.6935 Train f_score 0.6022 Xval loss: 0.6935 Xval f_score 0.6374\n",
      "00:36:44 epoch 20 of 100 Train loss: 0.6919 Train f_score 0.6621 Xval loss: 0.6934 Xval f_score 0.6531\n",
      "00:36:44 epoch 30 of 100 Train loss: 0.6917 Train f_score 0.6757 Xval loss: 0.6933 Xval f_score 0.6531\n",
      "00:36:44 epoch 40 of 100 Train loss: 0.6912 Train f_score 0.6598 Xval loss: 0.6933 Xval f_score 0.6465\n",
      "00:36:44 epoch 50 of 100 Train loss: 0.6882 Train f_score 0.6737 Xval loss: 0.6934 Xval f_score 0.6042\n",
      "00:36:44 epoch 60 of 100 Train loss: 0.6904 Train f_score 0.6400 Xval loss: 0.6937 Xval f_score 0.5714\n",
      "00:36:44 epoch 70 of 100 Train loss: 0.6905 Train f_score 0.6148 Xval loss: 0.6940 Xval f_score 0.5747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:36:44 epoch 80 of 100 Train loss: 0.6832 Train f_score 0.6332 Xval loss: 0.6942 Xval f_score 0.5647\n",
      "00:36:44 epoch 90 of 100 Train loss: 0.6774 Train f_score 0.6560 Xval loss: 0.6945 Xval f_score 0.5301\n",
      "Best Xval loss epoch 43, value 0.693259\n",
      "NN units 4\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.614, Train F1 0.675\n",
      "[[45 23]\n",
      " [58 84]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.400\n",
      "[[31 25]\n",
      " [ 5 10]]\n",
      "00:36:46 Starting\n",
      "00:36:47 epoch 0 of 100 Train loss: 0.6933 Train f_score 0.5893 Xval loss: 0.6956 Xval f_score 0.6087\n",
      "00:36:47 epoch 10 of 100 Train loss: 0.6925 Train f_score 0.6689 Xval loss: 0.6959 Xval f_score 0.6731\n",
      "00:36:47 epoch 20 of 100 Train loss: 0.6924 Train f_score 0.6796 Xval loss: 0.6961 Xval f_score 0.6667\n",
      "00:36:47 epoch 30 of 100 Train loss: 0.6911 Train f_score 0.6861 Xval loss: 0.6963 Xval f_score 0.6667\n",
      "00:36:47 epoch 40 of 100 Train loss: 0.6883 Train f_score 0.6908 Xval loss: 0.6967 Xval f_score 0.6731\n",
      "00:36:47 epoch 50 of 100 Train loss: 0.6877 Train f_score 0.6871 Xval loss: 0.6969 Xval f_score 0.6465\n",
      "00:36:47 epoch 60 of 100 Train loss: 0.6854 Train f_score 0.6877 Xval loss: 0.6969 Xval f_score 0.6105\n",
      "00:36:47 epoch 70 of 100 Train loss: 0.6842 Train f_score 0.6738 Xval loss: 0.6970 Xval f_score 0.5870\n",
      "00:36:47 epoch 80 of 100 Train loss: 0.6771 Train f_score 0.6838 Xval loss: 0.6971 Xval f_score 0.6000\n",
      "00:36:47 epoch 90 of 100 Train loss: 0.6794 Train f_score 0.6390 Xval loss: 0.6975 Xval f_score 0.6047\n",
      "Best Xval loss epoch 0, value 0.695597\n",
      "NN units 4\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.662, Train F1 0.710\n",
      "[[52 20]\n",
      " [51 87]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.590\n",
      "[[16 12]\n",
      " [20 23]]\n",
      "00:36:49 Starting\n",
      "00:36:50 epoch 0 of 100 Train loss: 0.6980 Train f_score 0.3056 Xval loss: 0.6973 Xval f_score 0.4074\n",
      "00:36:50 epoch 10 of 100 Train loss: 0.6965 Train f_score 0.5976 Xval loss: 0.6967 Xval f_score 0.5909\n",
      "00:36:50 epoch 20 of 100 Train loss: 0.6957 Train f_score 0.6547 Xval loss: 0.6963 Xval f_score 0.6263\n",
      "00:36:50 epoch 30 of 100 Train loss: 0.6949 Train f_score 0.6755 Xval loss: 0.6961 Xval f_score 0.6275\n",
      "00:36:50 epoch 40 of 100 Train loss: 0.6935 Train f_score 0.6971 Xval loss: 0.6960 Xval f_score 0.6408\n",
      "00:36:50 epoch 50 of 100 Train loss: 0.6929 Train f_score 0.7020 Xval loss: 0.6959 Xval f_score 0.6538\n",
      "00:36:50 epoch 60 of 100 Train loss: 0.6920 Train f_score 0.6910 Xval loss: 0.6960 Xval f_score 0.6337\n",
      "00:36:50 epoch 70 of 100 Train loss: 0.6917 Train f_score 0.6869 Xval loss: 0.6960 Xval f_score 0.6531\n",
      "00:36:50 epoch 80 of 100 Train loss: 0.6891 Train f_score 0.7088 Xval loss: 0.6965 Xval f_score 0.6598\n",
      "00:36:50 epoch 90 of 100 Train loss: 0.6866 Train f_score 0.6816 Xval loss: 0.6972 Xval f_score 0.5977\n",
      "Best Xval loss epoch 52, value 0.695891\n",
      "NN units 4\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.662, Train F1 0.647\n",
      "[[74 42]\n",
      " [29 65]]\n",
      "Final Xval Accuracy 0.535, Xval F1 0.660\n",
      "[[ 6  3]\n",
      " [30 32]]\n",
      "00:36:52 Starting\n",
      "00:36:54 epoch 0 of 100 Train loss: 0.7073 Train f_score 0.5691 Xval loss: 0.7054 Xval f_score 0.6337\n",
      "00:36:54 epoch 10 of 100 Train loss: 0.7034 Train f_score 0.6709 Xval loss: 0.7026 Xval f_score 0.6604\n",
      "00:36:54 epoch 20 of 100 Train loss: 0.7010 Train f_score 0.6751 Xval loss: 0.7004 Xval f_score 0.6604\n",
      "00:36:54 epoch 30 of 100 Train loss: 0.6987 Train f_score 0.6751 Xval loss: 0.6985 Xval f_score 0.6604\n",
      "00:36:54 epoch 40 of 100 Train loss: 0.6968 Train f_score 0.6751 Xval loss: 0.6971 Xval f_score 0.6604\n",
      "00:36:54 epoch 50 of 100 Train loss: 0.6954 Train f_score 0.6751 Xval loss: 0.6959 Xval f_score 0.6604\n",
      "00:36:54 epoch 60 of 100 Train loss: 0.6943 Train f_score 0.6751 Xval loss: 0.6951 Xval f_score 0.6604\n",
      "00:36:54 epoch 70 of 100 Train loss: 0.6939 Train f_score 0.6751 Xval loss: 0.6946 Xval f_score 0.6604\n",
      "00:36:54 epoch 80 of 100 Train loss: 0.6937 Train f_score 0.6751 Xval loss: 0.6943 Xval f_score 0.6604\n",
      "00:36:54 epoch 90 of 100 Train loss: 0.6934 Train f_score 0.6751 Xval loss: 0.6942 Xval f_score 0.6604\n",
      "Best Xval loss epoch 92, value 0.694165\n",
      "NN units 4\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.590, Train F1 0.570\n",
      "[[67 50]\n",
      " [36 57]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.375\n",
      "[[32 26]\n",
      " [ 4  9]]\n",
      "00:36:55 Starting\n",
      "00:36:57 epoch 0 of 100 Train loss: 0.6918 Train f_score 0.5787 Xval loss: 0.6932 Xval f_score 0.5070\n",
      "00:36:57 epoch 10 of 100 Train loss: 0.6902 Train f_score 0.6097 Xval loss: 0.6926 Xval f_score 0.5977\n",
      "00:36:57 epoch 20 of 100 Train loss: 0.6875 Train f_score 0.6496 Xval loss: 0.6920 Xval f_score 0.6154\n",
      "00:36:57 epoch 30 of 100 Train loss: 0.6857 Train f_score 0.6391 Xval loss: 0.6914 Xval f_score 0.6374\n",
      "00:36:57 epoch 40 of 100 Train loss: 0.6838 Train f_score 0.6284 Xval loss: 0.6910 Xval f_score 0.6374\n",
      "00:36:57 epoch 50 of 100 Train loss: 0.6802 Train f_score 0.6270 Xval loss: 0.6906 Xval f_score 0.6279\n",
      "00:36:57 epoch 60 of 100 Train loss: 0.6774 Train f_score 0.6585 Xval loss: 0.6903 Xval f_score 0.6420\n",
      "00:36:57 epoch 70 of 100 Train loss: 0.6749 Train f_score 0.6213 Xval loss: 0.6901 Xval f_score 0.6154\n",
      "00:36:57 epoch 80 of 100 Train loss: 0.6643 Train f_score 0.6544 Xval loss: 0.6900 Xval f_score 0.5867\n",
      "00:36:57 epoch 90 of 100 Train loss: 0.6644 Train f_score 0.6400 Xval loss: 0.6901 Xval f_score 0.6053\n",
      "Best Xval loss epoch 79, value 0.690007\n",
      "NN units 8\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.657, Train F1 0.667\n",
      "[[66 35]\n",
      " [37 72]]\n",
      "Final Xval Accuracy 0.606, Xval F1 0.632\n",
      "[[19 11]\n",
      " [17 24]]\n",
      "00:36:58 Starting\n",
      "00:37:00 epoch 0 of 100 Train loss: 0.6935 Train f_score 0.6202 Xval loss: 0.6942 Xval f_score 0.6602\n",
      "00:37:00 epoch 10 of 100 Train loss: 0.6930 Train f_score 0.6772 Xval loss: 0.6943 Xval f_score 0.6604\n",
      "00:37:00 epoch 20 of 100 Train loss: 0.6908 Train f_score 0.6709 Xval loss: 0.6944 Xval f_score 0.6604\n",
      "00:37:00 epoch 30 of 100 Train loss: 0.6896 Train f_score 0.6881 Xval loss: 0.6945 Xval f_score 0.6604\n",
      "00:37:00 epoch 40 of 100 Train loss: 0.6885 Train f_score 0.6839 Xval loss: 0.6946 Xval f_score 0.6538\n",
      "00:37:00 epoch 50 of 100 Train loss: 0.6842 Train f_score 0.6851 Xval loss: 0.6947 Xval f_score 0.6667\n",
      "00:37:00 epoch 60 of 100 Train loss: 0.6804 Train f_score 0.6790 Xval loss: 0.6952 Xval f_score 0.6383\n",
      "00:37:00 epoch 70 of 100 Train loss: 0.6794 Train f_score 0.6383 Xval loss: 0.6963 Xval f_score 0.5750\n",
      "00:37:00 epoch 80 of 100 Train loss: 0.6751 Train f_score 0.6695 Xval loss: 0.6981 Xval f_score 0.5000\n",
      "00:37:00 epoch 90 of 100 Train loss: 0.6710 Train f_score 0.6278 Xval loss: 0.6999 Xval f_score 0.5263\n",
      "Best Xval loss epoch 0, value 0.694250\n",
      "NN units 8\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.671, Train F1 0.716\n",
      "[[54 20]\n",
      " [49 87]]\n",
      "Final Xval Accuracy 0.535, Xval F1 0.548\n",
      "[[18 15]\n",
      " [18 20]]\n",
      "00:37:02 Starting\n",
      "00:37:03 epoch 0 of 100 Train loss: 0.6974 Train f_score 0.2222 Xval loss: 0.6954 Xval f_score 0.2979\n",
      "00:37:03 epoch 10 of 100 Train loss: 0.6953 Train f_score 0.5726 Xval loss: 0.6952 Xval f_score 0.5676\n",
      "00:37:03 epoch 20 of 100 Train loss: 0.6931 Train f_score 0.6434 Xval loss: 0.6950 Xval f_score 0.6047\n",
      "00:37:03 epoch 30 of 100 Train loss: 0.6921 Train f_score 0.6418 Xval loss: 0.6946 Xval f_score 0.5909\n",
      "00:37:03 epoch 40 of 100 Train loss: 0.6902 Train f_score 0.6543 Xval loss: 0.6941 Xval f_score 0.6364\n",
      "00:37:03 epoch 50 of 100 Train loss: 0.6866 Train f_score 0.6564 Xval loss: 0.6936 Xval f_score 0.5783\n",
      "00:37:03 epoch 60 of 100 Train loss: 0.6869 Train f_score 0.6486 Xval loss: 0.6932 Xval f_score 0.5610\n",
      "00:37:04 epoch 70 of 100 Train loss: 0.6811 Train f_score 0.6349 Xval loss: 0.6931 Xval f_score 0.5455\n",
      "00:37:04 epoch 80 of 100 Train loss: 0.6786 Train f_score 0.6667 Xval loss: 0.6930 Xval f_score 0.5333\n",
      "00:37:04 epoch 90 of 100 Train loss: 0.6727 Train f_score 0.6154 Xval loss: 0.6931 Xval f_score 0.5405\n",
      "Best Xval loss epoch 83, value 0.692993\n",
      "NN units 8\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.643, Train F1 0.691\n",
      "[[51 23]\n",
      " [52 84]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.559\n",
      "[[22 16]\n",
      " [14 19]]\n",
      "00:37:05 Starting\n",
      "00:37:07 epoch 0 of 100 Train loss: 0.7007 Train f_score 0.6034 Xval loss: 0.7020 Xval f_score 0.5854\n",
      "00:37:07 epoch 10 of 100 Train loss: 0.6997 Train f_score 0.6535 Xval loss: 0.7006 Xval f_score 0.6538\n",
      "00:37:07 epoch 20 of 100 Train loss: 0.6962 Train f_score 0.6839 Xval loss: 0.6992 Xval f_score 0.6604\n",
      "00:37:07 epoch 30 of 100 Train loss: 0.6940 Train f_score 0.6753 Xval loss: 0.6978 Xval f_score 0.6346\n",
      "00:37:07 epoch 40 of 100 Train loss: 0.6905 Train f_score 0.6826 Xval loss: 0.6967 Xval f_score 0.6263\n",
      "00:37:07 epoch 50 of 100 Train loss: 0.6906 Train f_score 0.6475 Xval loss: 0.6960 Xval f_score 0.6437\n",
      "00:37:07 epoch 60 of 100 Train loss: 0.6884 Train f_score 0.6494 Xval loss: 0.6957 Xval f_score 0.6341\n",
      "00:37:07 epoch 70 of 100 Train loss: 0.6856 Train f_score 0.6344 Xval loss: 0.6954 Xval f_score 0.5823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:37:07 epoch 80 of 100 Train loss: 0.6865 Train f_score 0.6306 Xval loss: 0.6953 Xval f_score 0.6133\n",
      "00:37:07 epoch 90 of 100 Train loss: 0.6825 Train f_score 0.6204 Xval loss: 0.6950 Xval f_score 0.5946\n",
      "Best Xval loss epoch 97, value 0.694943\n",
      "NN units 8\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.624, Train F1 0.661\n",
      "[[54 30]\n",
      " [49 77]]\n",
      "Final Xval Accuracy 0.606, Xval F1 0.632\n",
      "[[19 11]\n",
      " [17 24]]\n",
      "00:37:08 Starting\n",
      "00:37:10 epoch 0 of 100 Train loss: 0.7215 Train f_score 0.5098 Xval loss: 0.7215 Xval f_score 0.5753\n",
      "00:37:10 epoch 10 of 100 Train loss: 0.7145 Train f_score 0.6758 Xval loss: 0.7153 Xval f_score 0.6535\n",
      "00:37:10 epoch 20 of 100 Train loss: 0.7095 Train f_score 0.6709 Xval loss: 0.7101 Xval f_score 0.6604\n",
      "00:37:10 epoch 30 of 100 Train loss: 0.7047 Train f_score 0.6730 Xval loss: 0.7058 Xval f_score 0.6604\n",
      "00:37:10 epoch 40 of 100 Train loss: 0.7014 Train f_score 0.6772 Xval loss: 0.7027 Xval f_score 0.6604\n",
      "00:37:10 epoch 50 of 100 Train loss: 0.6985 Train f_score 0.6794 Xval loss: 0.7003 Xval f_score 0.6604\n",
      "00:37:10 epoch 60 of 100 Train loss: 0.6962 Train f_score 0.6794 Xval loss: 0.6984 Xval f_score 0.6604\n",
      "00:37:10 epoch 70 of 100 Train loss: 0.6948 Train f_score 0.6815 Xval loss: 0.6971 Xval f_score 0.6604\n",
      "00:37:10 epoch 80 of 100 Train loss: 0.6939 Train f_score 0.6752 Xval loss: 0.6963 Xval f_score 0.6476\n",
      "00:37:10 epoch 90 of 100 Train loss: 0.6945 Train f_score 0.6883 Xval loss: 0.6959 Xval f_score 0.6538\n",
      "Best Xval loss epoch 99, value 0.695766\n",
      "NN units 8\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.595, Train F1 0.679\n",
      "[[35 17]\n",
      " [68 90]]\n",
      "Final Xval Accuracy 0.606, Xval F1 0.632\n",
      "[[19 11]\n",
      " [17 24]]\n",
      "00:37:12 Starting\n",
      "00:37:13 epoch 0 of 100 Train loss: 0.6941 Train f_score 0.3676 Xval loss: 0.6936 Xval f_score 0.4590\n",
      "00:37:13 epoch 10 of 100 Train loss: 0.6907 Train f_score 0.5907 Xval loss: 0.6935 Xval f_score 0.5432\n",
      "00:37:13 epoch 20 of 100 Train loss: 0.6888 Train f_score 0.6462 Xval loss: 0.6934 Xval f_score 0.5122\n",
      "00:37:13 epoch 30 of 100 Train loss: 0.6841 Train f_score 0.6535 Xval loss: 0.6933 Xval f_score 0.5301\n",
      "00:37:13 epoch 40 of 100 Train loss: 0.6817 Train f_score 0.6586 Xval loss: 0.6934 Xval f_score 0.5500\n",
      "00:37:13 epoch 50 of 100 Train loss: 0.6777 Train f_score 0.6420 Xval loss: 0.6936 Xval f_score 0.5570\n",
      "00:37:13 epoch 60 of 100 Train loss: 0.6667 Train f_score 0.6667 Xval loss: 0.6941 Xval f_score 0.5333\n",
      "00:37:14 epoch 70 of 100 Train loss: 0.6608 Train f_score 0.6175 Xval loss: 0.6954 Xval f_score 0.5278\n",
      "00:37:14 epoch 80 of 100 Train loss: 0.6604 Train f_score 0.6168 Xval loss: 0.6975 Xval f_score 0.5217\n",
      "00:37:14 epoch 90 of 100 Train loss: 0.6548 Train f_score 0.6667 Xval loss: 0.7004 Xval f_score 0.5000\n",
      "Best Xval loss epoch 28, value 0.693287\n",
      "NN units 16\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.676, Train F1 0.685\n",
      "[[68 33]\n",
      " [35 74]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.652\n",
      "[[ 9  5]\n",
      " [27 30]]\n",
      "00:37:15 Starting\n",
      "00:37:17 epoch 0 of 100 Train loss: 0.6926 Train f_score 0.6387 Xval loss: 0.6958 Xval f_score 0.5000\n",
      "00:37:17 epoch 10 of 100 Train loss: 0.6911 Train f_score 0.6304 Xval loss: 0.6956 Xval f_score 0.6000\n",
      "00:37:17 epoch 20 of 100 Train loss: 0.6857 Train f_score 0.6513 Xval loss: 0.6955 Xval f_score 0.6237\n",
      "00:37:17 epoch 30 of 100 Train loss: 0.6830 Train f_score 0.6371 Xval loss: 0.6956 Xval f_score 0.6452\n",
      "00:37:17 epoch 40 of 100 Train loss: 0.6774 Train f_score 0.6853 Xval loss: 0.6958 Xval f_score 0.5909\n",
      "00:37:17 epoch 50 of 100 Train loss: 0.6681 Train f_score 0.6812 Xval loss: 0.6963 Xval f_score 0.5854\n",
      "00:37:17 epoch 60 of 100 Train loss: 0.6678 Train f_score 0.6425 Xval loss: 0.6973 Xval f_score 0.5641\n",
      "00:37:17 epoch 70 of 100 Train loss: 0.6580 Train f_score 0.6449 Xval loss: 0.6986 Xval f_score 0.5526\n",
      "00:37:17 epoch 80 of 100 Train loss: 0.6564 Train f_score 0.6545 Xval loss: 0.7000 Xval f_score 0.5600\n",
      "00:37:17 epoch 90 of 100 Train loss: 0.6508 Train f_score 0.6184 Xval loss: 0.7022 Xval f_score 0.5405\n",
      "Best Xval loss epoch 22, value 0.695483\n",
      "NN units 16\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.676, Train F1 0.685\n",
      "[[68 33]\n",
      " [35 74]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.667\n",
      "[[ 7  3]\n",
      " [29 32]]\n",
      "00:37:19 Starting\n",
      "00:37:20 epoch 0 of 100 Train loss: 0.6975 Train f_score 0.4883 Xval loss: 0.6972 Xval f_score 0.5352\n",
      "00:37:20 epoch 10 of 100 Train loss: 0.6941 Train f_score 0.6073 Xval loss: 0.6967 Xval f_score 0.5542\n",
      "00:37:20 epoch 20 of 100 Train loss: 0.6922 Train f_score 0.6613 Xval loss: 0.6969 Xval f_score 0.5783\n",
      "00:37:20 epoch 30 of 100 Train loss: 0.6897 Train f_score 0.6400 Xval loss: 0.6974 Xval f_score 0.5714\n",
      "00:37:20 epoch 40 of 100 Train loss: 0.6853 Train f_score 0.6639 Xval loss: 0.6980 Xval f_score 0.5783\n",
      "00:37:20 epoch 50 of 100 Train loss: 0.6793 Train f_score 0.6695 Xval loss: 0.6992 Xval f_score 0.5570\n",
      "00:37:21 epoch 60 of 100 Train loss: 0.6756 Train f_score 0.6432 Xval loss: 0.7011 Xval f_score 0.5333\n",
      "00:37:21 epoch 70 of 100 Train loss: 0.6713 Train f_score 0.6812 Xval loss: 0.7035 Xval f_score 0.5333\n",
      "00:37:21 epoch 80 of 100 Train loss: 0.6605 Train f_score 0.6516 Xval loss: 0.7063 Xval f_score 0.5135\n",
      "00:37:21 epoch 90 of 100 Train loss: 0.6552 Train f_score 0.6791 Xval loss: 0.7097 Xval f_score 0.5135\n",
      "Best Xval loss epoch 12, value 0.696669\n",
      "NN units 16\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.681, Train F1 0.656\n",
      "[[79 43]\n",
      " [24 64]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.333\n",
      "[[31 27]\n",
      " [ 5  8]]\n",
      "00:37:22 Starting\n",
      "00:37:25 epoch 0 of 100 Train loss: 0.7084 Train f_score 0.4246 Xval loss: 0.7097 Xval f_score 0.4308\n",
      "00:37:25 epoch 10 of 100 Train loss: 0.7045 Train f_score 0.5665 Xval loss: 0.7066 Xval f_score 0.5682\n",
      "00:37:25 epoch 20 of 100 Train loss: 0.6984 Train f_score 0.6970 Xval loss: 0.7044 Xval f_score 0.6136\n",
      "00:37:25 epoch 30 of 100 Train loss: 0.6970 Train f_score 0.6790 Xval loss: 0.7030 Xval f_score 0.6170\n",
      "00:37:25 epoch 40 of 100 Train loss: 0.6946 Train f_score 0.6493 Xval loss: 0.7018 Xval f_score 0.6222\n",
      "00:37:25 epoch 50 of 100 Train loss: 0.6868 Train f_score 0.6920 Xval loss: 0.7006 Xval f_score 0.6444\n",
      "00:37:25 epoch 60 of 100 Train loss: 0.6853 Train f_score 0.6719 Xval loss: 0.7005 Xval f_score 0.6047\n",
      "00:37:25 epoch 70 of 100 Train loss: 0.6831 Train f_score 0.6751 Xval loss: 0.7007 Xval f_score 0.5952\n",
      "00:37:25 epoch 80 of 100 Train loss: 0.6805 Train f_score 0.6979 Xval loss: 0.7012 Xval f_score 0.5783\n",
      "00:37:25 epoch 90 of 100 Train loss: 0.6738 Train f_score 0.6636 Xval loss: 0.7018 Xval f_score 0.5854\n",
      "Best Xval loss epoch 62, value 0.700504\n",
      "NN units 16\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.662, Train F1 0.693\n",
      "[[59 27]\n",
      " [44 80]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.636\n",
      "[[11  7]\n",
      " [25 28]]\n",
      "00:37:27 Starting\n",
      "00:37:28 epoch 0 of 100 Train loss: 0.7474 Train f_score 0.5121 Xval loss: 0.7448 Xval f_score 0.5641\n",
      "00:37:28 epoch 10 of 100 Train loss: 0.7330 Train f_score 0.6056 Xval loss: 0.7330 Xval f_score 0.6000\n",
      "00:37:28 epoch 20 of 100 Train loss: 0.7227 Train f_score 0.6503 Xval loss: 0.7234 Xval f_score 0.6337\n",
      "00:37:28 epoch 30 of 100 Train loss: 0.7140 Train f_score 0.6711 Xval loss: 0.7158 Xval f_score 0.6471\n",
      "00:37:28 epoch 40 of 100 Train loss: 0.7076 Train f_score 0.6667 Xval loss: 0.7100 Xval f_score 0.6471\n",
      "00:37:28 epoch 50 of 100 Train loss: 0.7022 Train f_score 0.6821 Xval loss: 0.7053 Xval f_score 0.6602\n",
      "00:37:28 epoch 60 of 100 Train loss: 0.6985 Train f_score 0.6867 Xval loss: 0.7015 Xval f_score 0.6600\n",
      "00:37:29 epoch 70 of 100 Train loss: 0.6962 Train f_score 0.6847 Xval loss: 0.6991 Xval f_score 0.6667\n",
      "00:37:29 epoch 80 of 100 Train loss: 0.6965 Train f_score 0.6643 Xval loss: 0.6974 Xval f_score 0.6170\n",
      "00:37:29 epoch 90 of 100 Train loss: 0.6906 Train f_score 0.6934 Xval loss: 0.6968 Xval f_score 0.6222\n",
      "Best Xval loss epoch 99, value 0.696391\n",
      "NN units 16\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.624, Train F1 0.672\n",
      "[[50 26]\n",
      " [53 81]]\n",
      "Final Xval Accuracy 0.592, Xval F1 0.356\n",
      "[[34 27]\n",
      " [ 2  8]]\n",
      "00:37:30 Starting\n",
      "00:37:32 epoch 0 of 100 Train loss: 0.6943 Train f_score 0.4503 Xval loss: 0.6926 Xval f_score 0.4308\n",
      "00:37:32 epoch 10 of 100 Train loss: 0.6848 Train f_score 0.5789 Xval loss: 0.6916 Xval f_score 0.4167\n",
      "00:37:32 epoch 20 of 100 Train loss: 0.6778 Train f_score 0.6121 Xval loss: 0.6913 Xval f_score 0.5135\n",
      "00:37:32 epoch 30 of 100 Train loss: 0.6717 Train f_score 0.6266 Xval loss: 0.6917 Xval f_score 0.5479\n",
      "00:37:32 epoch 40 of 100 Train loss: 0.6626 Train f_score 0.6126 Xval loss: 0.6925 Xval f_score 0.5479\n",
      "00:37:32 epoch 50 of 100 Train loss: 0.6525 Train f_score 0.6516 Xval loss: 0.6939 Xval f_score 0.5634\n",
      "00:37:32 epoch 60 of 100 Train loss: 0.6455 Train f_score 0.6514 Xval loss: 0.6956 Xval f_score 0.5278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:37:32 epoch 70 of 100 Train loss: 0.6308 Train f_score 0.6667 Xval loss: 0.6981 Xval f_score 0.5143\n",
      "00:37:32 epoch 80 of 100 Train loss: 0.6206 Train f_score 0.6727 Xval loss: 0.7020 Xval f_score 0.5000\n",
      "00:37:32 epoch 90 of 100 Train loss: 0.6159 Train f_score 0.6759 Xval loss: 0.7066 Xval f_score 0.4658\n",
      "Best Xval loss epoch 20, value 0.691316\n",
      "NN units 32\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.705, Train F1 0.696\n",
      "[[77 36]\n",
      " [26 71]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.311\n",
      "[[33 28]\n",
      " [ 3  7]]\n",
      "00:37:34 Starting\n",
      "00:37:35 epoch 0 of 100 Train loss: 0.6945 Train f_score 0.6577 Xval loss: 0.6963 Xval f_score 0.6538\n",
      "00:37:35 epoch 10 of 100 Train loss: 0.6890 Train f_score 0.6667 Xval loss: 0.6965 Xval f_score 0.6327\n",
      "00:37:35 epoch 20 of 100 Train loss: 0.6853 Train f_score 0.6667 Xval loss: 0.6968 Xval f_score 0.6067\n",
      "00:37:36 epoch 30 of 100 Train loss: 0.6789 Train f_score 0.6773 Xval loss: 0.6976 Xval f_score 0.5432\n",
      "00:37:36 epoch 40 of 100 Train loss: 0.6708 Train f_score 0.6724 Xval loss: 0.6985 Xval f_score 0.5195\n",
      "00:37:36 epoch 50 of 100 Train loss: 0.6606 Train f_score 0.6754 Xval loss: 0.7002 Xval f_score 0.5385\n",
      "00:37:36 epoch 60 of 100 Train loss: 0.6544 Train f_score 0.6933 Xval loss: 0.7029 Xval f_score 0.5385\n",
      "00:37:36 epoch 70 of 100 Train loss: 0.6464 Train f_score 0.6573 Xval loss: 0.7063 Xval f_score 0.5385\n",
      "00:37:36 epoch 80 of 100 Train loss: 0.6371 Train f_score 0.6514 Xval loss: 0.7113 Xval f_score 0.5679\n",
      "00:37:36 epoch 90 of 100 Train loss: 0.6244 Train f_score 0.6789 Xval loss: 0.7175 Xval f_score 0.5500\n",
      "Best Xval loss epoch 0, value 0.696277\n",
      "NN units 32\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.710, Train F1 0.736\n",
      "[[64 22]\n",
      " [39 85]]\n",
      "Final Xval Accuracy 0.535, Xval F1 0.673\n",
      "[[ 4  1]\n",
      " [32 34]]\n",
      "00:37:37 Starting\n",
      "00:37:39 epoch 0 of 100 Train loss: 0.7061 Train f_score 0.1854 Xval loss: 0.7073 Xval f_score 0.2222\n",
      "00:37:39 epoch 10 of 100 Train loss: 0.6986 Train f_score 0.5373 Xval loss: 0.7060 Xval f_score 0.4444\n",
      "00:37:39 epoch 20 of 100 Train loss: 0.6927 Train f_score 0.6276 Xval loss: 0.7051 Xval f_score 0.5385\n",
      "00:37:39 epoch 30 of 100 Train loss: 0.6869 Train f_score 0.6410 Xval loss: 0.7048 Xval f_score 0.5823\n",
      "00:37:39 epoch 40 of 100 Train loss: 0.6841 Train f_score 0.6390 Xval loss: 0.7050 Xval f_score 0.5789\n",
      "00:37:39 epoch 50 of 100 Train loss: 0.6741 Train f_score 0.6637 Xval loss: 0.7057 Xval f_score 0.5333\n",
      "00:37:39 epoch 60 of 100 Train loss: 0.6649 Train f_score 0.6758 Xval loss: 0.7074 Xval f_score 0.5333\n",
      "00:37:39 epoch 70 of 100 Train loss: 0.6528 Train f_score 0.6759 Xval loss: 0.7102 Xval f_score 0.5333\n",
      "00:37:39 epoch 80 of 100 Train loss: 0.6466 Train f_score 0.6818 Xval loss: 0.7147 Xval f_score 0.5263\n",
      "00:37:39 epoch 90 of 100 Train loss: 0.6375 Train f_score 0.6573 Xval loss: 0.7206 Xval f_score 0.5263\n",
      "Best Xval loss epoch 31, value 0.704845\n",
      "NN units 32\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.705, Train F1 0.728\n",
      "[[65 24]\n",
      " [38 83]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.158\n",
      "[[36 32]\n",
      " [ 0  3]]\n",
      "00:37:41 Starting\n",
      "00:37:43 epoch 0 of 100 Train loss: 0.7297 Train f_score 0.6316 Xval loss: 0.7248 Xval f_score 0.6200\n",
      "00:37:43 epoch 10 of 100 Train loss: 0.7182 Train f_score 0.6222 Xval loss: 0.7190 Xval f_score 0.6087\n",
      "00:37:43 epoch 20 of 100 Train loss: 0.7078 Train f_score 0.6590 Xval loss: 0.7141 Xval f_score 0.6444\n",
      "00:37:43 epoch 30 of 100 Train loss: 0.7020 Train f_score 0.6471 Xval loss: 0.7100 Xval f_score 0.6353\n",
      "00:37:43 epoch 40 of 100 Train loss: 0.6999 Train f_score 0.6348 Xval loss: 0.7071 Xval f_score 0.5714\n",
      "00:37:43 epoch 50 of 100 Train loss: 0.6922 Train f_score 0.5963 Xval loss: 0.7053 Xval f_score 0.5600\n",
      "00:37:43 epoch 60 of 100 Train loss: 0.6842 Train f_score 0.6140 Xval loss: 0.7043 Xval f_score 0.5135\n",
      "00:37:43 epoch 70 of 100 Train loss: 0.6815 Train f_score 0.6161 Xval loss: 0.7044 Xval f_score 0.5135\n",
      "00:37:43 epoch 80 of 100 Train loss: 0.6730 Train f_score 0.6226 Xval loss: 0.7049 Xval f_score 0.5000\n",
      "00:37:43 epoch 90 of 100 Train loss: 0.6745 Train f_score 0.6176 Xval loss: 0.7066 Xval f_score 0.5000\n",
      "Best Xval loss epoch 64, value 0.704255\n",
      "NN units 32\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.652, Train F1 0.695\n",
      "[[54 24]\n",
      " [49 83]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.667\n",
      "[[11  5]\n",
      " [25 30]]\n",
      "00:37:45 Starting\n",
      "00:37:47 epoch 0 of 100 Train loss: 0.8058 Train f_score 0.6294 Xval loss: 0.8058 Xval f_score 0.6214\n",
      "00:37:47 epoch 10 of 100 Train loss: 0.7798 Train f_score 0.6644 Xval loss: 0.7813 Xval f_score 0.6275\n",
      "00:37:47 epoch 20 of 100 Train loss: 0.7556 Train f_score 0.6871 Xval loss: 0.7606 Xval f_score 0.6400\n",
      "00:37:47 epoch 30 of 100 Train loss: 0.7396 Train f_score 0.6711 Xval loss: 0.7437 Xval f_score 0.6465\n",
      "00:37:47 epoch 40 of 100 Train loss: 0.7254 Train f_score 0.6780 Xval loss: 0.7303 Xval f_score 0.6042\n",
      "00:37:47 epoch 50 of 100 Train loss: 0.7143 Train f_score 0.7031 Xval loss: 0.7199 Xval f_score 0.6250\n",
      "00:37:47 epoch 60 of 100 Train loss: 0.7062 Train f_score 0.6735 Xval loss: 0.7122 Xval f_score 0.6392\n",
      "00:37:47 epoch 70 of 100 Train loss: 0.7009 Train f_score 0.6855 Xval loss: 0.7068 Xval f_score 0.6458\n",
      "00:37:47 epoch 80 of 100 Train loss: 0.6954 Train f_score 0.6667 Xval loss: 0.7032 Xval f_score 0.6154\n",
      "00:37:47 epoch 90 of 100 Train loss: 0.6931 Train f_score 0.6116 Xval loss: 0.7014 Xval f_score 0.5823\n",
      "Best Xval loss epoch 99, value 0.700316\n",
      "NN units 32\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.3330\n",
      "Activation relu\n",
      "Final Train Accuracy 0.600, Train F1 0.669\n",
      "[[41 22]\n",
      " [62 85]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.244\n",
      "[[35 30]\n",
      " [ 1  5]]\n",
      "00:37:49 Starting\n",
      "00:37:51 epoch 0 of 100 Train loss: 0.6946 Train f_score 0.6445 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:37:51 epoch 10 of 100 Train loss: 0.6920 Train f_score 0.6599 Xval loss: 0.6937 Xval f_score 0.6604\n",
      "00:37:51 epoch 20 of 100 Train loss: 0.6929 Train f_score 0.6397 Xval loss: 0.6937 Xval f_score 0.6604\n",
      "00:37:51 epoch 30 of 100 Train loss: 0.6922 Train f_score 0.6510 Xval loss: 0.6937 Xval f_score 0.6604\n",
      "00:37:51 epoch 40 of 100 Train loss: 0.6938 Train f_score 0.6469 Xval loss: 0.6936 Xval f_score 0.6604\n",
      "00:37:51 epoch 50 of 100 Train loss: 0.6912 Train f_score 0.6408 Xval loss: 0.6935 Xval f_score 0.6604\n",
      "00:37:51 epoch 60 of 100 Train loss: 0.6899 Train f_score 0.6829 Xval loss: 0.6934 Xval f_score 0.6604\n",
      "00:37:51 epoch 70 of 100 Train loss: 0.6926 Train f_score 0.6209 Xval loss: 0.6933 Xval f_score 0.6476\n",
      "00:37:51 epoch 80 of 100 Train loss: 0.6900 Train f_score 0.6667 Xval loss: 0.6932 Xval f_score 0.6408\n",
      "00:37:51 epoch 90 of 100 Train loss: 0.6905 Train f_score 0.6544 Xval loss: 0.6930 Xval f_score 0.6535\n",
      "Best Xval loss epoch 99, value 0.692876\n",
      "NN units 4\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.581, Train F1 0.617\n",
      "[[51 36]\n",
      " [52 71]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.367\n",
      "[[31 26]\n",
      " [ 5  9]]\n",
      "00:37:53 Starting\n",
      "00:37:54 epoch 0 of 100 Train loss: 0.6921 Train f_score 0.5611 Xval loss: 0.6937 Xval f_score 0.6392\n",
      "00:37:55 epoch 10 of 100 Train loss: 0.6948 Train f_score 0.5560 Xval loss: 0.6937 Xval f_score 0.6604\n",
      "00:37:55 epoch 20 of 100 Train loss: 0.6931 Train f_score 0.6360 Xval loss: 0.6938 Xval f_score 0.6604\n",
      "00:37:55 epoch 30 of 100 Train loss: 0.6929 Train f_score 0.6460 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:37:55 epoch 40 of 100 Train loss: 0.6928 Train f_score 0.6465 Xval loss: 0.6940 Xval f_score 0.6604\n",
      "00:37:55 epoch 50 of 100 Train loss: 0.6917 Train f_score 0.6556 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:37:55 epoch 60 of 100 Train loss: 0.6921 Train f_score 0.6207 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:37:55 epoch 70 of 100 Train loss: 0.6916 Train f_score 0.6574 Xval loss: 0.6938 Xval f_score 0.6476\n",
      "00:37:55 epoch 80 of 100 Train loss: 0.6909 Train f_score 0.6616 Xval loss: 0.6937 Xval f_score 0.6408\n",
      "00:37:55 epoch 90 of 100 Train loss: 0.6928 Train f_score 0.5970 Xval loss: 0.6937 Xval f_score 0.6471\n",
      "Best Xval loss epoch 84, value 0.693670\n",
      "NN units 4\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.586, Train F1 0.623\n",
      "[[51 35]\n",
      " [52 72]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.340\n",
      "[[32 27]\n",
      " [ 4  8]]\n",
      "00:37:57 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:37:58 epoch 0 of 100 Train loss: 0.6940 Train f_score 0.5182 Xval loss: 0.6946 Xval f_score 0.5870\n",
      "00:37:58 epoch 10 of 100 Train loss: 0.6945 Train f_score 0.5899 Xval loss: 0.6944 Xval f_score 0.6604\n",
      "00:37:58 epoch 20 of 100 Train loss: 0.6941 Train f_score 0.6751 Xval loss: 0.6943 Xval f_score 0.6604\n",
      "00:37:58 epoch 30 of 100 Train loss: 0.6935 Train f_score 0.6751 Xval loss: 0.6942 Xval f_score 0.6604\n",
      "00:37:58 epoch 40 of 100 Train loss: 0.6936 Train f_score 0.6751 Xval loss: 0.6941 Xval f_score 0.6604\n",
      "00:37:59 epoch 50 of 100 Train loss: 0.6926 Train f_score 0.6751 Xval loss: 0.6940 Xval f_score 0.6604\n",
      "00:37:59 epoch 60 of 100 Train loss: 0.6943 Train f_score 0.6751 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:37:59 epoch 70 of 100 Train loss: 0.6933 Train f_score 0.6751 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:37:59 epoch 80 of 100 Train loss: 0.6916 Train f_score 0.6751 Xval loss: 0.6938 Xval f_score 0.6604\n",
      "00:37:59 epoch 90 of 100 Train loss: 0.6928 Train f_score 0.6751 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "Best Xval loss epoch 78, value 0.693847\n",
      "NN units 4\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.586, Train F1 0.446\n",
      "[[88 72]\n",
      " [15 35]]\n",
      "Final Xval Accuracy 0.606, Xval F1 0.641\n",
      "[[18 10]\n",
      " [18 25]]\n",
      "00:38:01 Starting\n",
      "00:38:03 epoch 0 of 100 Train loss: 0.6985 Train f_score 0.2256 Xval loss: 0.6980 Xval f_score 0.0000\n",
      "00:38:03 epoch 10 of 100 Train loss: 0.6958 Train f_score 0.4151 Xval loss: 0.6969 Xval f_score 0.0000\n",
      "00:38:03 epoch 20 of 100 Train loss: 0.6952 Train f_score 0.5196 Xval loss: 0.6960 Xval f_score 0.4675\n",
      "00:38:03 epoch 30 of 100 Train loss: 0.6955 Train f_score 0.5425 Xval loss: 0.6953 Xval f_score 0.6604\n",
      "00:38:04 epoch 40 of 100 Train loss: 0.6933 Train f_score 0.6354 Xval loss: 0.6948 Xval f_score 0.6604\n",
      "00:38:04 epoch 50 of 100 Train loss: 0.6931 Train f_score 0.6621 Xval loss: 0.6944 Xval f_score 0.6604\n",
      "00:38:04 epoch 60 of 100 Train loss: 0.6942 Train f_score 0.6308 Xval loss: 0.6941 Xval f_score 0.6604\n",
      "00:38:04 epoch 70 of 100 Train loss: 0.6931 Train f_score 0.6643 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:38:04 epoch 80 of 100 Train loss: 0.6937 Train f_score 0.6710 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:38:04 epoch 90 of 100 Train loss: 0.6932 Train f_score 0.6556 Xval loss: 0.6938 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.693821\n",
      "NN units 4\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.567, Train F1 0.442\n",
      "[[83 71]\n",
      " [20 36]]\n",
      "Final Xval Accuracy 0.620, Xval F1 0.658\n",
      "[[18  9]\n",
      " [18 26]]\n",
      "00:38:06 Starting\n",
      "00:38:07 epoch 0 of 100 Train loss: 0.7088 Train f_score 0.1270 Xval loss: 0.7073 Xval f_score 0.0000\n",
      "00:38:07 epoch 10 of 100 Train loss: 0.7051 Train f_score 0.2676 Xval loss: 0.7039 Xval f_score 0.0000\n",
      "00:38:07 epoch 20 of 100 Train loss: 0.7014 Train f_score 0.4333 Xval loss: 0.7013 Xval f_score 0.0000\n",
      "00:38:07 epoch 30 of 100 Train loss: 0.6976 Train f_score 0.5204 Xval loss: 0.6991 Xval f_score 0.6604\n",
      "00:38:08 epoch 40 of 100 Train loss: 0.6979 Train f_score 0.6000 Xval loss: 0.6976 Xval f_score 0.6604\n",
      "00:38:08 epoch 50 of 100 Train loss: 0.6956 Train f_score 0.6645 Xval loss: 0.6964 Xval f_score 0.6604\n",
      "00:38:08 epoch 60 of 100 Train loss: 0.6951 Train f_score 0.6751 Xval loss: 0.6954 Xval f_score 0.6604\n",
      "00:38:08 epoch 70 of 100 Train loss: 0.6938 Train f_score 0.6751 Xval loss: 0.6947 Xval f_score 0.6604\n",
      "00:38:08 epoch 80 of 100 Train loss: 0.6939 Train f_score 0.6751 Xval loss: 0.6943 Xval f_score 0.6604\n",
      "00:38:08 epoch 90 of 100 Train loss: 0.6939 Train f_score 0.6751 Xval loss: 0.6940 Xval f_score 0.6604\n",
      "Best Xval loss epoch 98, value 0.693881\n",
      "NN units 4\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.548, Train F1 0.532\n",
      "[[61 53]\n",
      " [42 54]]\n",
      "Final Xval Accuracy 0.535, Xval F1 0.233\n",
      "[[33 30]\n",
      " [ 3  5]]\n",
      "00:38:10 Starting\n",
      "00:38:11 epoch 0 of 100 Train loss: 0.6951 Train f_score 0.4887 Xval loss: 0.6930 Xval f_score 0.6105\n",
      "00:38:11 epoch 10 of 100 Train loss: 0.6933 Train f_score 0.5514 Xval loss: 0.6930 Xval f_score 0.6471\n",
      "00:38:11 epoch 20 of 100 Train loss: 0.6894 Train f_score 0.6378 Xval loss: 0.6930 Xval f_score 0.6538\n",
      "00:38:11 epoch 30 of 100 Train loss: 0.6914 Train f_score 0.6442 Xval loss: 0.6929 Xval f_score 0.6408\n",
      "00:38:11 epoch 40 of 100 Train loss: 0.6904 Train f_score 0.6316 Xval loss: 0.6927 Xval f_score 0.6200\n",
      "00:38:11 epoch 50 of 100 Train loss: 0.6914 Train f_score 0.6118 Xval loss: 0.6925 Xval f_score 0.6316\n",
      "00:38:11 epoch 60 of 100 Train loss: 0.6903 Train f_score 0.5887 Xval loss: 0.6923 Xval f_score 0.6000\n",
      "00:38:12 epoch 70 of 100 Train loss: 0.6897 Train f_score 0.5882 Xval loss: 0.6921 Xval f_score 0.5977\n",
      "00:38:12 epoch 80 of 100 Train loss: 0.6881 Train f_score 0.6512 Xval loss: 0.6919 Xval f_score 0.5714\n",
      "00:38:12 epoch 90 of 100 Train loss: 0.6875 Train f_score 0.6154 Xval loss: 0.6918 Xval f_score 0.5500\n",
      "Best Xval loss epoch 98, value 0.691704\n",
      "NN units 8\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.581, Train F1 0.690\n",
      "[[24  9]\n",
      " [79 98]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.545\n",
      "[[23 17]\n",
      " [13 18]]\n",
      "00:38:14 Starting\n",
      "00:38:15 epoch 0 of 100 Train loss: 0.6928 Train f_score 0.6113 Xval loss: 0.6943 Xval f_score 0.6604\n",
      "00:38:15 epoch 10 of 100 Train loss: 0.6941 Train f_score 0.6577 Xval loss: 0.6942 Xval f_score 0.6604\n",
      "00:38:16 epoch 20 of 100 Train loss: 0.6924 Train f_score 0.6689 Xval loss: 0.6941 Xval f_score 0.6604\n",
      "00:38:16 epoch 30 of 100 Train loss: 0.6932 Train f_score 0.6644 Xval loss: 0.6940 Xval f_score 0.6604\n",
      "00:38:16 epoch 40 of 100 Train loss: 0.6931 Train f_score 0.6529 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:38:16 epoch 50 of 100 Train loss: 0.6910 Train f_score 0.6851 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:38:16 epoch 60 of 100 Train loss: 0.6919 Train f_score 0.6712 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:38:16 epoch 70 of 100 Train loss: 0.6917 Train f_score 0.6526 Xval loss: 0.6939 Xval f_score 0.6346\n",
      "00:38:16 epoch 80 of 100 Train loss: 0.6900 Train f_score 0.6762 Xval loss: 0.6939 Xval f_score 0.6346\n",
      "00:38:16 epoch 90 of 100 Train loss: 0.6915 Train f_score 0.6370 Xval loss: 0.6939 Xval f_score 0.6471\n",
      "Best Xval loss epoch 99, value 0.693797\n",
      "NN units 8\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.576, Train F1 0.411\n",
      "[[90 76]\n",
      " [13 31]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.423\n",
      "[[30 24]\n",
      " [ 6 11]]\n",
      "00:38:18 Starting\n",
      "00:38:20 epoch 0 of 100 Train loss: 0.6965 Train f_score 0.6050 Xval loss: 0.6965 Xval f_score 0.6604\n",
      "00:38:20 epoch 10 of 100 Train loss: 0.6943 Train f_score 0.6522 Xval loss: 0.6959 Xval f_score 0.6604\n",
      "00:38:20 epoch 20 of 100 Train loss: 0.6950 Train f_score 0.6148 Xval loss: 0.6954 Xval f_score 0.6604\n",
      "00:38:20 epoch 30 of 100 Train loss: 0.6944 Train f_score 0.6341 Xval loss: 0.6952 Xval f_score 0.6604\n",
      "00:38:20 epoch 40 of 100 Train loss: 0.6931 Train f_score 0.6000 Xval loss: 0.6948 Xval f_score 0.6604\n",
      "00:38:20 epoch 50 of 100 Train loss: 0.6909 Train f_score 0.6517 Xval loss: 0.6946 Xval f_score 0.6476\n",
      "00:38:20 epoch 60 of 100 Train loss: 0.6930 Train f_score 0.6236 Xval loss: 0.6946 Xval f_score 0.6346\n",
      "00:38:20 epoch 70 of 100 Train loss: 0.6919 Train f_score 0.6618 Xval loss: 0.6945 Xval f_score 0.6346\n",
      "00:38:20 epoch 80 of 100 Train loss: 0.6918 Train f_score 0.6215 Xval loss: 0.6945 Xval f_score 0.6337\n",
      "00:38:20 epoch 90 of 100 Train loss: 0.6908 Train f_score 0.6304 Xval loss: 0.6946 Xval f_score 0.6061\n",
      "Best Xval loss epoch 80, value 0.694489\n",
      "NN units 8\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.581, Train F1 0.664\n",
      "[[35 20]\n",
      " [68 87]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.605\n",
      "[[18 12]\n",
      " [18 23]]\n",
      "00:38:22 Starting\n",
      "00:38:24 epoch 0 of 100 Train loss: 0.7006 Train f_score 0.3333 Xval loss: 0.7017 Xval f_score 0.0000\n",
      "00:38:24 epoch 10 of 100 Train loss: 0.6995 Train f_score 0.4749 Xval loss: 0.6998 Xval f_score 0.6408\n",
      "00:38:24 epoch 20 of 100 Train loss: 0.6988 Train f_score 0.5747 Xval loss: 0.6985 Xval f_score 0.6604\n",
      "00:38:24 epoch 30 of 100 Train loss: 0.6959 Train f_score 0.6416 Xval loss: 0.6974 Xval f_score 0.6604\n",
      "00:38:24 epoch 40 of 100 Train loss: 0.6967 Train f_score 0.6429 Xval loss: 0.6963 Xval f_score 0.6604\n",
      "00:38:24 epoch 50 of 100 Train loss: 0.6958 Train f_score 0.6154 Xval loss: 0.6956 Xval f_score 0.6604\n",
      "00:38:24 epoch 60 of 100 Train loss: 0.6948 Train f_score 0.6529 Xval loss: 0.6950 Xval f_score 0.6604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:38:24 epoch 70 of 100 Train loss: 0.6953 Train f_score 0.6022 Xval loss: 0.6945 Xval f_score 0.6604\n",
      "00:38:24 epoch 80 of 100 Train loss: 0.6943 Train f_score 0.6392 Xval loss: 0.6942 Xval f_score 0.6604\n",
      "00:38:24 epoch 90 of 100 Train loss: 0.6939 Train f_score 0.6338 Xval loss: 0.6941 Xval f_score 0.6604\n",
      "Best Xval loss epoch 98, value 0.694069\n",
      "NN units 8\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.595, Train F1 0.653\n",
      "[[45 27]\n",
      " [58 80]]\n",
      "Final Xval Accuracy 0.606, Xval F1 0.674\n",
      "[[14  6]\n",
      " [22 29]]\n",
      "00:38:26 Starting\n",
      "00:38:28 epoch 0 of 100 Train loss: 0.7210 Train f_score 0.6667 Xval loss: 0.7196 Xval f_score 0.6604\n",
      "00:38:28 epoch 10 of 100 Train loss: 0.7147 Train f_score 0.6309 Xval loss: 0.7129 Xval f_score 0.6604\n",
      "00:38:28 epoch 20 of 100 Train loss: 0.7061 Train f_score 0.6426 Xval loss: 0.7075 Xval f_score 0.6604\n",
      "00:38:28 epoch 30 of 100 Train loss: 0.7038 Train f_score 0.6192 Xval loss: 0.7035 Xval f_score 0.6604\n",
      "00:38:28 epoch 40 of 100 Train loss: 0.7028 Train f_score 0.5979 Xval loss: 0.7005 Xval f_score 0.6604\n",
      "00:38:28 epoch 50 of 100 Train loss: 0.6988 Train f_score 0.6014 Xval loss: 0.6982 Xval f_score 0.6604\n",
      "00:38:28 epoch 60 of 100 Train loss: 0.6956 Train f_score 0.6594 Xval loss: 0.6966 Xval f_score 0.6604\n",
      "00:38:28 epoch 70 of 100 Train loss: 0.6942 Train f_score 0.6481 Xval loss: 0.6955 Xval f_score 0.6604\n",
      "00:38:28 epoch 80 of 100 Train loss: 0.6947 Train f_score 0.6532 Xval loss: 0.6948 Xval f_score 0.6604\n",
      "00:38:28 epoch 90 of 100 Train loss: 0.6939 Train f_score 0.6800 Xval loss: 0.6943 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.694029\n",
      "NN units 8\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.519, Train F1 0.633\n",
      "[[22 20]\n",
      " [81 87]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.158\n",
      "[[36 32]\n",
      " [ 0  3]]\n",
      "00:38:31 Starting\n",
      "00:38:32 epoch 0 of 100 Train loss: 0.6920 Train f_score 0.5126 Xval loss: 0.6930 Xval f_score 0.5556\n",
      "00:38:32 epoch 10 of 100 Train loss: 0.6918 Train f_score 0.5976 Xval loss: 0.6930 Xval f_score 0.6604\n",
      "00:38:32 epoch 20 of 100 Train loss: 0.6913 Train f_score 0.6250 Xval loss: 0.6930 Xval f_score 0.6476\n",
      "00:38:33 epoch 30 of 100 Train loss: 0.6924 Train f_score 0.6107 Xval loss: 0.6927 Xval f_score 0.6538\n",
      "00:38:33 epoch 40 of 100 Train loss: 0.6900 Train f_score 0.6320 Xval loss: 0.6924 Xval f_score 0.6186\n",
      "00:38:33 epoch 50 of 100 Train loss: 0.6907 Train f_score 0.5941 Xval loss: 0.6921 Xval f_score 0.6067\n",
      "00:38:33 epoch 60 of 100 Train loss: 0.6871 Train f_score 0.6179 Xval loss: 0.6919 Xval f_score 0.6067\n",
      "00:38:33 epoch 70 of 100 Train loss: 0.6896 Train f_score 0.5690 Xval loss: 0.6915 Xval f_score 0.6207\n",
      "00:38:33 epoch 80 of 100 Train loss: 0.6897 Train f_score 0.5630 Xval loss: 0.6912 Xval f_score 0.5854\n",
      "00:38:33 epoch 90 of 100 Train loss: 0.6882 Train f_score 0.6160 Xval loss: 0.6911 Xval f_score 0.5750\n",
      "Best Xval loss epoch 99, value 0.691061\n",
      "NN units 16\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.600, Train F1 0.677\n",
      "[[38 19]\n",
      " [65 88]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.318\n",
      "[[34 28]\n",
      " [ 2  7]]\n",
      "00:38:35 Starting\n",
      "00:38:37 epoch 0 of 100 Train loss: 0.6993 Train f_score 0.1805 Xval loss: 0.6949 Xval f_score 0.0000\n",
      "00:38:37 epoch 10 of 100 Train loss: 0.6927 Train f_score 0.4865 Xval loss: 0.6945 Xval f_score 0.2000\n",
      "00:38:37 epoch 20 of 100 Train loss: 0.6935 Train f_score 0.6215 Xval loss: 0.6947 Xval f_score 0.6538\n",
      "00:38:37 epoch 30 of 100 Train loss: 0.6938 Train f_score 0.6282 Xval loss: 0.6946 Xval f_score 0.6471\n",
      "00:38:37 epoch 40 of 100 Train loss: 0.6906 Train f_score 0.6532 Xval loss: 0.6943 Xval f_score 0.6061\n",
      "00:38:37 epoch 50 of 100 Train loss: 0.6902 Train f_score 0.6142 Xval loss: 0.6941 Xval f_score 0.6105\n",
      "00:38:37 epoch 60 of 100 Train loss: 0.6912 Train f_score 0.5738 Xval loss: 0.6939 Xval f_score 0.5977\n",
      "00:38:37 epoch 70 of 100 Train loss: 0.6869 Train f_score 0.5975 Xval loss: 0.6938 Xval f_score 0.6047\n",
      "00:38:37 epoch 80 of 100 Train loss: 0.6886 Train f_score 0.5974 Xval loss: 0.6938 Xval f_score 0.5952\n",
      "00:38:37 epoch 90 of 100 Train loss: 0.6900 Train f_score 0.5882 Xval loss: 0.6938 Xval f_score 0.5854\n",
      "Best Xval loss epoch 83, value 0.693783\n",
      "NN units 16\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.590, Train F1 0.636\n",
      "[[49 32]\n",
      " [54 75]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.608\n",
      "[[16 11]\n",
      " [20 24]]\n",
      "00:38:39 Starting\n",
      "00:38:41 epoch 0 of 100 Train loss: 0.6982 Train f_score 0.3721 Xval loss: 0.6987 Xval f_score 0.0556\n",
      "00:38:41 epoch 10 of 100 Train loss: 0.6986 Train f_score 0.5401 Xval loss: 0.6982 Xval f_score 0.6408\n",
      "00:38:41 epoch 20 of 100 Train loss: 0.6981 Train f_score 0.6320 Xval loss: 0.6978 Xval f_score 0.6346\n",
      "00:38:41 epoch 30 of 100 Train loss: 0.6969 Train f_score 0.6107 Xval loss: 0.6972 Xval f_score 0.6408\n",
      "00:38:41 epoch 40 of 100 Train loss: 0.6935 Train f_score 0.6312 Xval loss: 0.6967 Xval f_score 0.6275\n",
      "00:38:41 epoch 50 of 100 Train loss: 0.6914 Train f_score 0.6080 Xval loss: 0.6965 Xval f_score 0.6400\n",
      "00:38:41 epoch 60 of 100 Train loss: 0.6949 Train f_score 0.6327 Xval loss: 0.6964 Xval f_score 0.6400\n",
      "00:38:41 epoch 70 of 100 Train loss: 0.6937 Train f_score 0.6260 Xval loss: 0.6964 Xval f_score 0.6186\n",
      "00:38:41 epoch 80 of 100 Train loss: 0.6957 Train f_score 0.5976 Xval loss: 0.6963 Xval f_score 0.5957\n",
      "00:38:42 epoch 90 of 100 Train loss: 0.6874 Train f_score 0.6210 Xval loss: 0.6965 Xval f_score 0.5957\n",
      "Best Xval loss epoch 77, value 0.696318\n",
      "NN units 16\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.595, Train F1 0.647\n",
      "[[47 29]\n",
      " [56 78]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.340\n",
      "[[32 27]\n",
      " [ 4  8]]\n",
      "00:38:44 Starting\n",
      "00:38:46 epoch 0 of 100 Train loss: 0.7125 Train f_score 0.6137 Xval loss: 0.7114 Xval f_score 0.6476\n",
      "00:38:46 epoch 10 of 100 Train loss: 0.7036 Train f_score 0.6570 Xval loss: 0.7072 Xval f_score 0.6604\n",
      "00:38:46 epoch 20 of 100 Train loss: 0.7031 Train f_score 0.6491 Xval loss: 0.7038 Xval f_score 0.6604\n",
      "00:38:46 epoch 30 of 100 Train loss: 0.6982 Train f_score 0.6790 Xval loss: 0.7010 Xval f_score 0.6604\n",
      "00:38:46 epoch 40 of 100 Train loss: 0.6949 Train f_score 0.6415 Xval loss: 0.6988 Xval f_score 0.6604\n",
      "00:38:46 epoch 50 of 100 Train loss: 0.6958 Train f_score 0.6194 Xval loss: 0.6972 Xval f_score 0.6604\n",
      "00:38:46 epoch 60 of 100 Train loss: 0.6956 Train f_score 0.6444 Xval loss: 0.6961 Xval f_score 0.6604\n",
      "00:38:46 epoch 70 of 100 Train loss: 0.6998 Train f_score 0.5606 Xval loss: 0.6954 Xval f_score 0.6604\n",
      "00:38:46 epoch 80 of 100 Train loss: 0.6963 Train f_score 0.6178 Xval loss: 0.6950 Xval f_score 0.6604\n",
      "00:38:46 epoch 90 of 100 Train loss: 0.6955 Train f_score 0.5758 Xval loss: 0.6948 Xval f_score 0.6604\n",
      "Best Xval loss epoch 97, value 0.694651\n",
      "NN units 16\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.576, Train F1 0.637\n",
      "[[43 29]\n",
      " [60 78]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.615\n",
      "[[17 11]\n",
      " [19 24]]\n",
      "00:38:48 Starting\n",
      "00:38:50 epoch 0 of 100 Train loss: 0.7443 Train f_score 0.4571 Xval loss: 0.7448 Xval f_score 0.0000\n",
      "00:38:50 epoch 10 of 100 Train loss: 0.7319 Train f_score 0.6104 Xval loss: 0.7324 Xval f_score 0.6604\n",
      "00:38:50 epoch 20 of 100 Train loss: 0.7212 Train f_score 0.6620 Xval loss: 0.7227 Xval f_score 0.6604\n",
      "00:38:50 epoch 30 of 100 Train loss: 0.7149 Train f_score 0.6263 Xval loss: 0.7146 Xval f_score 0.6604\n",
      "00:38:50 epoch 40 of 100 Train loss: 0.7088 Train f_score 0.6130 Xval loss: 0.7082 Xval f_score 0.6604\n",
      "00:38:51 epoch 50 of 100 Train loss: 0.7014 Train f_score 0.6468 Xval loss: 0.7033 Xval f_score 0.6604\n",
      "00:38:51 epoch 60 of 100 Train loss: 0.7013 Train f_score 0.5585 Xval loss: 0.6997 Xval f_score 0.6604\n",
      "00:38:51 epoch 70 of 100 Train loss: 0.6981 Train f_score 0.5993 Xval loss: 0.6974 Xval f_score 0.6604\n",
      "00:38:51 epoch 80 of 100 Train loss: 0.6988 Train f_score 0.6154 Xval loss: 0.6959 Xval f_score 0.6604\n",
      "00:38:51 epoch 90 of 100 Train loss: 0.6918 Train f_score 0.6620 Xval loss: 0.6950 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.694699\n",
      "NN units 16\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.529, Train F1 0.684\n",
      "[[  4   0]\n",
      " [ 99 107]]\n",
      "Final Xval Accuracy 0.606, Xval F1 0.562\n",
      "[[25 17]\n",
      " [11 18]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:38:53 Starting\n",
      "00:38:56 epoch 0 of 100 Train loss: 0.6932 Train f_score 0.2388 Xval loss: 0.6938 Xval f_score 0.0000\n",
      "00:38:56 epoch 10 of 100 Train loss: 0.6914 Train f_score 0.5882 Xval loss: 0.6934 Xval f_score 0.6275\n",
      "00:38:56 epoch 20 of 100 Train loss: 0.6946 Train f_score 0.6143 Xval loss: 0.6936 Xval f_score 0.6408\n",
      "00:38:56 epoch 30 of 100 Train loss: 0.6933 Train f_score 0.5918 Xval loss: 0.6928 Xval f_score 0.6105\n",
      "00:38:56 epoch 40 of 100 Train loss: 0.6910 Train f_score 0.5579 Xval loss: 0.6922 Xval f_score 0.5714\n",
      "00:38:56 epoch 50 of 100 Train loss: 0.6893 Train f_score 0.5422 Xval loss: 0.6920 Xval f_score 0.5679\n",
      "00:38:56 epoch 60 of 100 Train loss: 0.6878 Train f_score 0.5714 Xval loss: 0.6920 Xval f_score 0.5610\n",
      "00:38:56 epoch 70 of 100 Train loss: 0.6871 Train f_score 0.6076 Xval loss: 0.6919 Xval f_score 0.5750\n",
      "00:38:56 epoch 80 of 100 Train loss: 0.6847 Train f_score 0.5714 Xval loss: 0.6919 Xval f_score 0.5823\n",
      "00:38:56 epoch 90 of 100 Train loss: 0.6792 Train f_score 0.5752 Xval loss: 0.6922 Xval f_score 0.5823\n",
      "Best Xval loss epoch 75, value 0.691865\n",
      "NN units 32\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.586, Train F1 0.633\n",
      "[[48 32]\n",
      " [55 75]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.628\n",
      "[[12  8]\n",
      " [24 27]]\n",
      "00:38:59 Starting\n",
      "00:39:01 epoch 0 of 100 Train loss: 0.6995 Train f_score 0.6178 Xval loss: 0.6988 Xval f_score 0.6604\n",
      "00:39:01 epoch 10 of 100 Train loss: 0.6975 Train f_score 0.5476 Xval loss: 0.6973 Xval f_score 0.6337\n",
      "00:39:01 epoch 20 of 100 Train loss: 0.6885 Train f_score 0.5956 Xval loss: 0.6966 Xval f_score 0.5581\n",
      "00:39:01 epoch 30 of 100 Train loss: 0.6891 Train f_score 0.5992 Xval loss: 0.6964 Xval f_score 0.5747\n",
      "00:39:01 epoch 40 of 100 Train loss: 0.6945 Train f_score 0.5862 Xval loss: 0.6961 Xval f_score 0.5952\n",
      "00:39:01 epoch 50 of 100 Train loss: 0.6839 Train f_score 0.6410 Xval loss: 0.6961 Xval f_score 0.5679\n",
      "00:39:01 epoch 60 of 100 Train loss: 0.6901 Train f_score 0.5545 Xval loss: 0.6963 Xval f_score 0.5854\n",
      "00:39:01 epoch 70 of 100 Train loss: 0.6919 Train f_score 0.5603 Xval loss: 0.6965 Xval f_score 0.5926\n",
      "00:39:01 epoch 80 of 100 Train loss: 0.6839 Train f_score 0.5750 Xval loss: 0.6969 Xval f_score 0.5897\n",
      "00:39:01 epoch 90 of 100 Train loss: 0.6784 Train f_score 0.5982 Xval loss: 0.6973 Xval f_score 0.5897\n",
      "Best Xval loss epoch 47, value 0.696033\n",
      "NN units 32\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.605, Train F1 0.664\n",
      "[[45 25]\n",
      " [58 82]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.608\n",
      "[[16 11]\n",
      " [20 24]]\n",
      "00:39:03 Starting\n",
      "00:39:06 epoch 0 of 100 Train loss: 0.7116 Train f_score 0.0180 Xval loss: 0.7055 Xval f_score 0.0000\n",
      "00:39:06 epoch 10 of 100 Train loss: 0.6995 Train f_score 0.4649 Xval loss: 0.7018 Xval f_score 0.3333\n",
      "00:39:06 epoch 20 of 100 Train loss: 0.6961 Train f_score 0.6691 Xval loss: 0.7015 Xval f_score 0.6604\n",
      "00:39:06 epoch 30 of 100 Train loss: 0.7014 Train f_score 0.6290 Xval loss: 0.7003 Xval f_score 0.6346\n",
      "00:39:06 epoch 40 of 100 Train loss: 0.6930 Train f_score 0.6615 Xval loss: 0.6989 Xval f_score 0.6186\n",
      "00:39:06 epoch 50 of 100 Train loss: 0.6969 Train f_score 0.5783 Xval loss: 0.6982 Xval f_score 0.5747\n",
      "00:39:06 epoch 60 of 100 Train loss: 0.6929 Train f_score 0.6050 Xval loss: 0.6978 Xval f_score 0.5952\n",
      "00:39:06 epoch 70 of 100 Train loss: 0.6859 Train f_score 0.6446 Xval loss: 0.6977 Xval f_score 0.5882\n",
      "00:39:06 epoch 80 of 100 Train loss: 0.6900 Train f_score 0.5957 Xval loss: 0.6976 Xval f_score 0.6024\n",
      "00:39:06 epoch 90 of 100 Train loss: 0.6894 Train f_score 0.5893 Xval loss: 0.6976 Xval f_score 0.6000\n",
      "Best Xval loss epoch 88, value 0.697582\n",
      "NN units 32\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.595, Train F1 0.632\n",
      "[[52 34]\n",
      " [51 73]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.311\n",
      "[[33 28]\n",
      " [ 3  7]]\n",
      "00:39:08 Starting\n",
      "00:39:10 epoch 0 of 100 Train loss: 0.7261 Train f_score 0.3288 Xval loss: 0.7292 Xval f_score 0.0000\n",
      "00:39:10 epoch 10 of 100 Train loss: 0.7248 Train f_score 0.4957 Xval loss: 0.7214 Xval f_score 0.6346\n",
      "00:39:10 epoch 20 of 100 Train loss: 0.7091 Train f_score 0.6388 Xval loss: 0.7151 Xval f_score 0.6604\n",
      "00:39:10 epoch 30 of 100 Train loss: 0.7105 Train f_score 0.5726 Xval loss: 0.7090 Xval f_score 0.6476\n",
      "00:39:10 epoch 40 of 100 Train loss: 0.7024 Train f_score 0.6000 Xval loss: 0.7045 Xval f_score 0.6476\n",
      "00:39:10 epoch 50 of 100 Train loss: 0.7011 Train f_score 0.5882 Xval loss: 0.7013 Xval f_score 0.6476\n",
      "00:39:10 epoch 60 of 100 Train loss: 0.6948 Train f_score 0.5882 Xval loss: 0.6989 Xval f_score 0.6476\n",
      "00:39:11 epoch 70 of 100 Train loss: 0.7042 Train f_score 0.5565 Xval loss: 0.6974 Xval f_score 0.6476\n",
      "00:39:11 epoch 80 of 100 Train loss: 0.6949 Train f_score 0.5772 Xval loss: 0.6965 Xval f_score 0.6476\n",
      "00:39:11 epoch 90 of 100 Train loss: 0.6951 Train f_score 0.6107 Xval loss: 0.6962 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.696001\n",
      "NN units 32\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.610, Train F1 0.667\n",
      "[[46 25]\n",
      " [57 82]]\n",
      "Final Xval Accuracy 0.535, Xval F1 0.660\n",
      "[[ 6  3]\n",
      " [30 32]]\n",
      "00:39:13 Starting\n",
      "00:39:15 epoch 0 of 100 Train loss: 0.8039 Train f_score 0.0000 Xval loss: 0.7961 Xval f_score 0.0000\n",
      "00:39:15 epoch 10 of 100 Train loss: 0.7708 Train f_score 0.4324 Xval loss: 0.7682 Xval f_score 0.0000\n",
      "00:39:15 epoch 20 of 100 Train loss: 0.7475 Train f_score 0.6496 Xval loss: 0.7492 Xval f_score 0.6604\n",
      "00:39:15 epoch 30 of 100 Train loss: 0.7324 Train f_score 0.6599 Xval loss: 0.7331 Xval f_score 0.6604\n",
      "00:39:15 epoch 40 of 100 Train loss: 0.7169 Train f_score 0.6397 Xval loss: 0.7201 Xval f_score 0.6604\n",
      "00:39:15 epoch 50 of 100 Train loss: 0.7142 Train f_score 0.5424 Xval loss: 0.7107 Xval f_score 0.6604\n",
      "00:39:15 epoch 60 of 100 Train loss: 0.7025 Train f_score 0.5863 Xval loss: 0.7044 Xval f_score 0.6604\n",
      "00:39:15 epoch 70 of 100 Train loss: 0.6990 Train f_score 0.6439 Xval loss: 0.7007 Xval f_score 0.6604\n",
      "00:39:15 epoch 80 of 100 Train loss: 0.6953 Train f_score 0.6296 Xval loss: 0.6981 Xval f_score 0.6604\n",
      "00:39:15 epoch 90 of 100 Train loss: 0.6988 Train f_score 0.6115 Xval loss: 0.6966 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.696113\n",
      "NN units 32\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.3330\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.562, Train F1 0.617\n",
      "[[44 33]\n",
      " [59 74]]\n",
      "Final Xval Accuracy 0.606, Xval F1 0.588\n",
      "[[23 15]\n",
      " [13 20]]\n",
      "00:39:18 Starting\n",
      "00:39:20 epoch 0 of 100 Train loss: 0.6927 Train f_score 0.4880 Xval loss: 0.6931 Xval f_score 0.5843\n",
      "00:39:20 epoch 10 of 100 Train loss: 0.6931 Train f_score 0.6397 Xval loss: 0.6932 Xval f_score 0.6602\n",
      "00:39:20 epoch 20 of 100 Train loss: 0.6917 Train f_score 0.6667 Xval loss: 0.6932 Xval f_score 0.6667\n",
      "00:39:20 epoch 30 of 100 Train loss: 0.6910 Train f_score 0.6645 Xval loss: 0.6930 Xval f_score 0.6667\n",
      "00:39:20 epoch 40 of 100 Train loss: 0.6899 Train f_score 0.6710 Xval loss: 0.6929 Xval f_score 0.6667\n",
      "00:39:20 epoch 50 of 100 Train loss: 0.6879 Train f_score 0.6755 Xval loss: 0.6925 Xval f_score 0.6538\n",
      "00:39:20 epoch 60 of 100 Train loss: 0.6866 Train f_score 0.6598 Xval loss: 0.6919 Xval f_score 0.6400\n",
      "00:39:20 epoch 70 of 100 Train loss: 0.6816 Train f_score 0.6975 Xval loss: 0.6913 Xval f_score 0.6250\n",
      "00:39:20 epoch 80 of 100 Train loss: 0.6777 Train f_score 0.6931 Xval loss: 0.6908 Xval f_score 0.6154\n",
      "00:39:20 epoch 90 of 100 Train loss: 0.6784 Train f_score 0.5437 Xval loss: 0.6904 Xval f_score 0.5926\n",
      "Best Xval loss epoch 94, value 0.690334\n",
      "NN units 4\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.619, Train F1 0.630\n",
      "[[62 39]\n",
      " [41 68]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.600\n",
      "[[15 11]\n",
      " [21 24]]\n",
      "00:39:23 Starting\n",
      "00:39:25 epoch 0 of 100 Train loss: 0.6934 Train f_score 0.3886 Xval loss: 0.6929 Xval f_score 0.5429\n",
      "00:39:25 epoch 10 of 100 Train loss: 0.6934 Train f_score 0.6219 Xval loss: 0.6931 Xval f_score 0.6222\n",
      "00:39:25 epoch 20 of 100 Train loss: 0.6916 Train f_score 0.6502 Xval loss: 0.6931 Xval f_score 0.6327\n",
      "00:39:25 epoch 30 of 100 Train loss: 0.6909 Train f_score 0.6667 Xval loss: 0.6933 Xval f_score 0.6327\n",
      "00:39:25 epoch 40 of 100 Train loss: 0.6889 Train f_score 0.6761 Xval loss: 0.6931 Xval f_score 0.6250\n",
      "00:39:25 epoch 50 of 100 Train loss: 0.6901 Train f_score 0.6286 Xval loss: 0.6925 Xval f_score 0.6316\n",
      "00:39:25 epoch 60 of 100 Train loss: 0.6887 Train f_score 0.6787 Xval loss: 0.6920 Xval f_score 0.6237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:39:25 epoch 70 of 100 Train loss: 0.6839 Train f_score 0.6643 Xval loss: 0.6916 Xval f_score 0.6136\n",
      "00:39:25 epoch 80 of 100 Train loss: 0.6810 Train f_score 0.6561 Xval loss: 0.6914 Xval f_score 0.5882\n",
      "00:39:25 epoch 90 of 100 Train loss: 0.6778 Train f_score 0.6667 Xval loss: 0.6909 Xval f_score 0.5854\n",
      "Best Xval loss epoch 99, value 0.690434\n",
      "NN units 4\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.633, Train F1 0.678\n",
      "[[52 26]\n",
      " [51 81]]\n",
      "Final Xval Accuracy 0.592, Xval F1 0.431\n",
      "[[31 24]\n",
      " [ 5 11]]\n",
      "00:39:27 Starting\n",
      "00:39:30 epoch 0 of 100 Train loss: 0.6941 Train f_score 0.5859 Xval loss: 0.6942 Xval f_score 0.6604\n",
      "00:39:30 epoch 10 of 100 Train loss: 0.6933 Train f_score 0.6751 Xval loss: 0.6940 Xval f_score 0.6604\n",
      "00:39:30 epoch 20 of 100 Train loss: 0.6932 Train f_score 0.6751 Xval loss: 0.6937 Xval f_score 0.6604\n",
      "00:39:30 epoch 30 of 100 Train loss: 0.6931 Train f_score 0.6751 Xval loss: 0.6934 Xval f_score 0.6604\n",
      "00:39:30 epoch 40 of 100 Train loss: 0.6903 Train f_score 0.6751 Xval loss: 0.6930 Xval f_score 0.6604\n",
      "00:39:30 epoch 50 of 100 Train loss: 0.6883 Train f_score 0.6751 Xval loss: 0.6927 Xval f_score 0.6604\n",
      "00:39:30 epoch 60 of 100 Train loss: 0.6875 Train f_score 0.6751 Xval loss: 0.6922 Xval f_score 0.6604\n",
      "00:39:30 epoch 70 of 100 Train loss: 0.6831 Train f_score 0.6751 Xval loss: 0.6916 Xval f_score 0.6604\n",
      "00:39:30 epoch 80 of 100 Train loss: 0.6833 Train f_score 0.6154 Xval loss: 0.6909 Xval f_score 0.5977\n",
      "00:39:30 epoch 90 of 100 Train loss: 0.6798 Train f_score 0.5446 Xval loss: 0.6904 Xval f_score 0.5405\n",
      "Best Xval loss epoch 99, value 0.690046\n",
      "NN units 4\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.581, Train F1 0.664\n",
      "[[35 20]\n",
      " [68 87]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.375\n",
      "[[32 26]\n",
      " [ 4  9]]\n",
      "00:39:32 Starting\n",
      "00:39:35 epoch 0 of 100 Train loss: 0.6984 Train f_score 0.3657 Xval loss: 0.6965 Xval f_score 0.5352\n",
      "00:39:35 epoch 10 of 100 Train loss: 0.6968 Train f_score 0.6475 Xval loss: 0.6962 Xval f_score 0.6136\n",
      "00:39:35 epoch 20 of 100 Train loss: 0.6962 Train f_score 0.6370 Xval loss: 0.6959 Xval f_score 0.6598\n",
      "00:39:35 epoch 30 of 100 Train loss: 0.6972 Train f_score 0.6373 Xval loss: 0.6953 Xval f_score 0.6535\n",
      "00:39:35 epoch 40 of 100 Train loss: 0.6938 Train f_score 0.6578 Xval loss: 0.6948 Xval f_score 0.6535\n",
      "00:39:35 epoch 50 of 100 Train loss: 0.6922 Train f_score 0.6645 Xval loss: 0.6945 Xval f_score 0.6408\n",
      "00:39:35 epoch 60 of 100 Train loss: 0.6924 Train f_score 0.6667 Xval loss: 0.6942 Xval f_score 0.6408\n",
      "00:39:35 epoch 70 of 100 Train loss: 0.6922 Train f_score 0.6735 Xval loss: 0.6942 Xval f_score 0.6471\n",
      "00:39:35 epoch 80 of 100 Train loss: 0.6913 Train f_score 0.6667 Xval loss: 0.6945 Xval f_score 0.6327\n",
      "00:39:35 epoch 90 of 100 Train loss: 0.6867 Train f_score 0.6812 Xval loss: 0.6948 Xval f_score 0.5806\n",
      "Best Xval loss epoch 67, value 0.694164\n",
      "NN units 4\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.638, Train F1 0.635\n",
      "[[68 41]\n",
      " [35 66]]\n",
      "Final Xval Accuracy 0.592, Xval F1 0.408\n",
      "[[32 25]\n",
      " [ 4 10]]\n",
      "00:39:38 Starting\n",
      "00:39:40 epoch 0 of 100 Train loss: 0.7067 Train f_score 0.4138 Xval loss: 0.7065 Xval f_score 0.4533\n",
      "00:39:40 epoch 10 of 100 Train loss: 0.7038 Train f_score 0.5799 Xval loss: 0.7040 Xval f_score 0.6316\n",
      "00:39:40 epoch 20 of 100 Train loss: 0.7010 Train f_score 0.6711 Xval loss: 0.7019 Xval f_score 0.6346\n",
      "00:39:40 epoch 30 of 100 Train loss: 0.6988 Train f_score 0.6731 Xval loss: 0.7002 Xval f_score 0.6476\n",
      "00:39:40 epoch 40 of 100 Train loss: 0.6974 Train f_score 0.6794 Xval loss: 0.6986 Xval f_score 0.6604\n",
      "00:39:40 epoch 50 of 100 Train loss: 0.6962 Train f_score 0.6772 Xval loss: 0.6973 Xval f_score 0.6604\n",
      "00:39:40 epoch 60 of 100 Train loss: 0.6952 Train f_score 0.6751 Xval loss: 0.6963 Xval f_score 0.6604\n",
      "00:39:40 epoch 70 of 100 Train loss: 0.6946 Train f_score 0.6751 Xval loss: 0.6954 Xval f_score 0.6604\n",
      "00:39:40 epoch 80 of 100 Train loss: 0.6940 Train f_score 0.6751 Xval loss: 0.6948 Xval f_score 0.6604\n",
      "00:39:40 epoch 90 of 100 Train loss: 0.6936 Train f_score 0.6751 Xval loss: 0.6943 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.694043\n",
      "NN units 4\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.576, Train F1 0.669\n",
      "[[31 17]\n",
      " [72 90]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.617\n",
      "[[15 10]\n",
      " [21 25]]\n",
      "00:39:42 Starting\n",
      "00:39:45 epoch 0 of 100 Train loss: 0.6946 Train f_score 0.0000 Xval loss: 0.6934 Xval f_score 0.1538\n",
      "00:39:45 epoch 10 of 100 Train loss: 0.6928 Train f_score 0.5422 Xval loss: 0.6935 Xval f_score 0.5217\n",
      "00:39:45 epoch 20 of 100 Train loss: 0.6914 Train f_score 0.6250 Xval loss: 0.6936 Xval f_score 0.6154\n",
      "00:39:45 epoch 30 of 100 Train loss: 0.6912 Train f_score 0.6284 Xval loss: 0.6935 Xval f_score 0.6022\n",
      "00:39:45 epoch 40 of 100 Train loss: 0.6892 Train f_score 0.6642 Xval loss: 0.6934 Xval f_score 0.6154\n",
      "00:39:45 epoch 50 of 100 Train loss: 0.6895 Train f_score 0.6444 Xval loss: 0.6933 Xval f_score 0.5909\n",
      "00:39:45 epoch 60 of 100 Train loss: 0.6852 Train f_score 0.6489 Xval loss: 0.6930 Xval f_score 0.5882\n",
      "00:39:45 epoch 70 of 100 Train loss: 0.6865 Train f_score 0.6080 Xval loss: 0.6929 Xval f_score 0.6024\n",
      "00:39:45 epoch 80 of 100 Train loss: 0.6816 Train f_score 0.6776 Xval loss: 0.6930 Xval f_score 0.5679\n",
      "00:39:45 epoch 90 of 100 Train loss: 0.6777 Train f_score 0.6325 Xval loss: 0.6931 Xval f_score 0.5385\n",
      "Best Xval loss epoch 73, value 0.692833\n",
      "NN units 8\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.633, Train F1 0.709\n",
      "[[39 13]\n",
      " [64 94]]\n",
      "Final Xval Accuracy 0.592, Xval F1 0.356\n",
      "[[34 27]\n",
      " [ 2  8]]\n",
      "00:39:48 Starting\n",
      "00:39:50 epoch 0 of 100 Train loss: 0.6917 Train f_score 0.6271 Xval loss: 0.6923 Xval f_score 0.6591\n",
      "00:39:50 epoch 10 of 100 Train loss: 0.6920 Train f_score 0.6429 Xval loss: 0.6925 Xval f_score 0.6465\n",
      "00:39:50 epoch 20 of 100 Train loss: 0.6912 Train f_score 0.6408 Xval loss: 0.6924 Xval f_score 0.6400\n",
      "00:39:50 epoch 30 of 100 Train loss: 0.6892 Train f_score 0.6736 Xval loss: 0.6920 Xval f_score 0.6400\n",
      "00:39:50 epoch 40 of 100 Train loss: 0.6864 Train f_score 0.6620 Xval loss: 0.6914 Xval f_score 0.6526\n",
      "00:39:50 epoch 50 of 100 Train loss: 0.6848 Train f_score 0.6496 Xval loss: 0.6908 Xval f_score 0.6522\n",
      "00:39:50 epoch 60 of 100 Train loss: 0.6800 Train f_score 0.5974 Xval loss: 0.6903 Xval f_score 0.6250\n",
      "00:39:50 epoch 70 of 100 Train loss: 0.6751 Train f_score 0.6111 Xval loss: 0.6900 Xval f_score 0.5405\n",
      "00:39:50 epoch 80 of 100 Train loss: 0.6715 Train f_score 0.5427 Xval loss: 0.6900 Xval f_score 0.5070\n",
      "00:39:50 epoch 90 of 100 Train loss: 0.6736 Train f_score 0.5888 Xval loss: 0.6900 Xval f_score 0.5000\n",
      "Best Xval loss epoch 88, value 0.689971\n",
      "NN units 8\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.629, Train F1 0.705\n",
      "[[39 14]\n",
      " [64 93]]\n",
      "Final Xval Accuracy 0.592, Xval F1 0.674\n",
      "[[12  5]\n",
      " [24 30]]\n",
      "00:39:53 Starting\n",
      "00:39:55 epoch 0 of 100 Train loss: 0.6961 Train f_score 0.4873 Xval loss: 0.6960 Xval f_score 0.5714\n",
      "00:39:55 epoch 10 of 100 Train loss: 0.6939 Train f_score 0.6468 Xval loss: 0.6960 Xval f_score 0.5934\n",
      "00:39:55 epoch 20 of 100 Train loss: 0.6945 Train f_score 0.6595 Xval loss: 0.6960 Xval f_score 0.5957\n",
      "00:39:55 epoch 30 of 100 Train loss: 0.6920 Train f_score 0.6354 Xval loss: 0.6960 Xval f_score 0.6250\n",
      "00:39:55 epoch 40 of 100 Train loss: 0.6886 Train f_score 0.6595 Xval loss: 0.6957 Xval f_score 0.6087\n",
      "00:39:55 epoch 50 of 100 Train loss: 0.6834 Train f_score 0.6866 Xval loss: 0.6954 Xval f_score 0.6067\n",
      "00:39:55 epoch 60 of 100 Train loss: 0.6851 Train f_score 0.6618 Xval loss: 0.6954 Xval f_score 0.6136\n",
      "00:39:55 epoch 70 of 100 Train loss: 0.6803 Train f_score 0.6615 Xval loss: 0.6954 Xval f_score 0.6047\n",
      "00:39:55 epoch 80 of 100 Train loss: 0.6811 Train f_score 0.6535 Xval loss: 0.6955 Xval f_score 0.6024\n",
      "00:39:55 epoch 90 of 100 Train loss: 0.6709 Train f_score 0.6316 Xval loss: 0.6958 Xval f_score 0.6098\n",
      "Best Xval loss epoch 72, value 0.695399\n",
      "NN units 8\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.638, Train F1 0.627\n",
      "[[70 43]\n",
      " [33 64]]\n",
      "Final Xval Accuracy 0.592, Xval F1 0.580\n",
      "[[22 15]\n",
      " [14 20]]\n",
      "00:39:58 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:40:00 epoch 0 of 100 Train loss: 0.7013 Train f_score 0.6340 Xval loss: 0.7019 Xval f_score 0.6535\n",
      "00:40:00 epoch 10 of 100 Train loss: 0.6992 Train f_score 0.6579 Xval loss: 0.7003 Xval f_score 0.6604\n",
      "00:40:00 epoch 20 of 100 Train loss: 0.6962 Train f_score 0.6688 Xval loss: 0.6990 Xval f_score 0.6604\n",
      "00:40:00 epoch 30 of 100 Train loss: 0.6943 Train f_score 0.6752 Xval loss: 0.6981 Xval f_score 0.6604\n",
      "00:40:00 epoch 40 of 100 Train loss: 0.6950 Train f_score 0.6773 Xval loss: 0.6972 Xval f_score 0.6476\n",
      "00:40:01 epoch 50 of 100 Train loss: 0.6925 Train f_score 0.6688 Xval loss: 0.6965 Xval f_score 0.6538\n",
      "00:40:01 epoch 60 of 100 Train loss: 0.6929 Train f_score 0.6490 Xval loss: 0.6960 Xval f_score 0.6400\n",
      "00:40:01 epoch 70 of 100 Train loss: 0.6899 Train f_score 0.6722 Xval loss: 0.6957 Xval f_score 0.6667\n",
      "00:40:01 epoch 80 of 100 Train loss: 0.6841 Train f_score 0.6154 Xval loss: 0.6953 Xval f_score 0.6512\n",
      "00:40:01 epoch 90 of 100 Train loss: 0.6899 Train f_score 0.5577 Xval loss: 0.6951 Xval f_score 0.6154\n",
      "Best Xval loss epoch 99, value 0.694846\n",
      "NN units 8\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.610, Train F1 0.653\n",
      "[[51 30]\n",
      " [52 77]]\n",
      "Final Xval Accuracy 0.620, Xval F1 0.675\n",
      "[[16  7]\n",
      " [20 28]]\n",
      "00:40:03 Starting\n",
      "00:40:06 epoch 0 of 100 Train loss: 0.7207 Train f_score 0.4645 Xval loss: 0.7195 Xval f_score 0.5405\n",
      "00:40:06 epoch 10 of 100 Train loss: 0.7136 Train f_score 0.6762 Xval loss: 0.7133 Xval f_score 0.6337\n",
      "00:40:06 epoch 20 of 100 Train loss: 0.7083 Train f_score 0.6710 Xval loss: 0.7083 Xval f_score 0.6476\n",
      "00:40:06 epoch 30 of 100 Train loss: 0.7040 Train f_score 0.6752 Xval loss: 0.7045 Xval f_score 0.6604\n",
      "00:40:06 epoch 40 of 100 Train loss: 0.7009 Train f_score 0.6751 Xval loss: 0.7014 Xval f_score 0.6604\n",
      "00:40:06 epoch 50 of 100 Train loss: 0.6986 Train f_score 0.6709 Xval loss: 0.6990 Xval f_score 0.6604\n",
      "00:40:06 epoch 60 of 100 Train loss: 0.6960 Train f_score 0.6751 Xval loss: 0.6972 Xval f_score 0.6604\n",
      "00:40:06 epoch 70 of 100 Train loss: 0.6946 Train f_score 0.6751 Xval loss: 0.6960 Xval f_score 0.6604\n",
      "00:40:06 epoch 80 of 100 Train loss: 0.6940 Train f_score 0.6751 Xval loss: 0.6953 Xval f_score 0.6604\n",
      "00:40:06 epoch 90 of 100 Train loss: 0.6941 Train f_score 0.6751 Xval loss: 0.6949 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.694726\n",
      "NN units 8\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.600, Train F1 0.650\n",
      "[[48 29]\n",
      " [55 78]]\n",
      "Final Xval Accuracy 0.634, Xval F1 0.690\n",
      "[[16  6]\n",
      " [20 29]]\n",
      "00:40:09 Starting\n",
      "00:40:13 epoch 0 of 100 Train loss: 0.6934 Train f_score 0.2550 Xval loss: 0.6928 Xval f_score 0.4286\n",
      "00:40:13 epoch 10 of 100 Train loss: 0.6898 Train f_score 0.5616 Xval loss: 0.6929 Xval f_score 0.5588\n",
      "00:40:13 epoch 20 of 100 Train loss: 0.6890 Train f_score 0.6271 Xval loss: 0.6932 Xval f_score 0.6000\n",
      "00:40:13 epoch 30 of 100 Train loss: 0.6849 Train f_score 0.6053 Xval loss: 0.6937 Xval f_score 0.5926\n",
      "00:40:13 epoch 40 of 100 Train loss: 0.6778 Train f_score 0.6754 Xval loss: 0.6940 Xval f_score 0.5679\n",
      "00:40:13 epoch 50 of 100 Train loss: 0.6747 Train f_score 0.6638 Xval loss: 0.6946 Xval f_score 0.5316\n",
      "00:40:13 epoch 60 of 100 Train loss: 0.6720 Train f_score 0.6316 Xval loss: 0.6955 Xval f_score 0.5385\n",
      "00:40:13 epoch 70 of 100 Train loss: 0.6667 Train f_score 0.6784 Xval loss: 0.6966 Xval f_score 0.5385\n",
      "00:40:13 epoch 80 of 100 Train loss: 0.6593 Train f_score 0.6607 Xval loss: 0.6986 Xval f_score 0.5641\n",
      "00:40:13 epoch 90 of 100 Train loss: 0.6373 Train f_score 0.7100 Xval loss: 0.7011 Xval f_score 0.5385\n",
      "Best Xval loss epoch 5, value 0.692671\n",
      "NN units 16\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.695, Train F1 0.686\n",
      "[[76 37]\n",
      " [27 70]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.660\n",
      "[[ 8  4]\n",
      " [28 31]]\n",
      "00:40:16 Starting\n",
      "00:40:18 epoch 0 of 100 Train loss: 0.6925 Train f_score 0.6641 Xval loss: 0.6966 Xval f_score 0.5745\n",
      "00:40:18 epoch 10 of 100 Train loss: 0.6915 Train f_score 0.6304 Xval loss: 0.6962 Xval f_score 0.5652\n",
      "00:40:18 epoch 20 of 100 Train loss: 0.6859 Train f_score 0.6457 Xval loss: 0.6957 Xval f_score 0.5647\n",
      "00:40:18 epoch 30 of 100 Train loss: 0.6859 Train f_score 0.6154 Xval loss: 0.6951 Xval f_score 0.5500\n",
      "00:40:18 epoch 40 of 100 Train loss: 0.6792 Train f_score 0.6119 Xval loss: 0.6948 Xval f_score 0.5385\n",
      "00:40:18 epoch 50 of 100 Train loss: 0.6810 Train f_score 0.6239 Xval loss: 0.6946 Xval f_score 0.5455\n",
      "00:40:18 epoch 60 of 100 Train loss: 0.6667 Train f_score 0.6481 Xval loss: 0.6948 Xval f_score 0.5135\n",
      "00:40:18 epoch 70 of 100 Train loss: 0.6588 Train f_score 0.6100 Xval loss: 0.6956 Xval f_score 0.5070\n",
      "00:40:18 epoch 80 of 100 Train loss: 0.6687 Train f_score 0.6214 Xval loss: 0.6965 Xval f_score 0.5278\n",
      "00:40:18 epoch 90 of 100 Train loss: 0.6613 Train f_score 0.5941 Xval loss: 0.6978 Xval f_score 0.5278\n",
      "Best Xval loss epoch 51, value 0.694608\n",
      "NN units 16\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.676, Train F1 0.699\n",
      "[[63 28]\n",
      " [40 79]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.311\n",
      "[[33 28]\n",
      " [ 3  7]]\n",
      "00:40:21 Starting\n",
      "00:40:23 epoch 0 of 100 Train loss: 0.6986 Train f_score 0.4896 Xval loss: 0.7005 Xval f_score 0.3448\n",
      "00:40:23 epoch 10 of 100 Train loss: 0.6944 Train f_score 0.6048 Xval loss: 0.7002 Xval f_score 0.4533\n",
      "00:40:24 epoch 20 of 100 Train loss: 0.6936 Train f_score 0.6265 Xval loss: 0.7002 Xval f_score 0.5301\n",
      "00:40:24 epoch 30 of 100 Train loss: 0.6890 Train f_score 0.6403 Xval loss: 0.7005 Xval f_score 0.5238\n",
      "00:40:24 epoch 40 of 100 Train loss: 0.6833 Train f_score 0.6746 Xval loss: 0.7006 Xval f_score 0.5432\n",
      "00:40:24 epoch 50 of 100 Train loss: 0.6834 Train f_score 0.6721 Xval loss: 0.7012 Xval f_score 0.5432\n",
      "00:40:24 epoch 60 of 100 Train loss: 0.6764 Train f_score 0.6639 Xval loss: 0.7022 Xval f_score 0.5500\n",
      "00:40:24 epoch 70 of 100 Train loss: 0.6727 Train f_score 0.6444 Xval loss: 0.7037 Xval f_score 0.5385\n",
      "00:40:24 epoch 80 of 100 Train loss: 0.6654 Train f_score 0.6725 Xval loss: 0.7057 Xval f_score 0.5195\n",
      "00:40:24 epoch 90 of 100 Train loss: 0.6663 Train f_score 0.6609 Xval loss: 0.7080 Xval f_score 0.5135\n",
      "Best Xval loss epoch 16, value 0.700157\n",
      "NN units 16\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.671, Train F1 0.693\n",
      "[[63 29]\n",
      " [40 78]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.158\n",
      "[[36 32]\n",
      " [ 0  3]]\n",
      "00:40:27 Starting\n",
      "00:40:29 epoch 0 of 100 Train loss: 0.7114 Train f_score 0.4124 Xval loss: 0.7091 Xval f_score 0.5000\n",
      "00:40:29 epoch 10 of 100 Train loss: 0.7031 Train f_score 0.6525 Xval loss: 0.7066 Xval f_score 0.6173\n",
      "00:40:29 epoch 20 of 100 Train loss: 0.6991 Train f_score 0.6840 Xval loss: 0.7048 Xval f_score 0.6512\n",
      "00:40:29 epoch 30 of 100 Train loss: 0.6962 Train f_score 0.6764 Xval loss: 0.7032 Xval f_score 0.6593\n",
      "00:40:29 epoch 40 of 100 Train loss: 0.6942 Train f_score 0.6347 Xval loss: 0.7021 Xval f_score 0.6444\n",
      "00:40:29 epoch 50 of 100 Train loss: 0.6947 Train f_score 0.6360 Xval loss: 0.7013 Xval f_score 0.5714\n",
      "00:40:29 epoch 60 of 100 Train loss: 0.6882 Train f_score 0.6615 Xval loss: 0.7006 Xval f_score 0.5385\n",
      "00:40:29 epoch 70 of 100 Train loss: 0.6820 Train f_score 0.6774 Xval loss: 0.7005 Xval f_score 0.5455\n",
      "00:40:29 epoch 80 of 100 Train loss: 0.6859 Train f_score 0.5966 Xval loss: 0.7009 Xval f_score 0.5526\n",
      "00:40:29 epoch 90 of 100 Train loss: 0.6783 Train f_score 0.6667 Xval loss: 0.7012 Xval f_score 0.5405\n",
      "Best Xval loss epoch 72, value 0.700494\n",
      "NN units 16\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.652, Train F1 0.681\n",
      "[[59 29]\n",
      " [44 78]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.205\n",
      "[[36 31]\n",
      " [ 0  4]]\n",
      "00:40:32 Starting\n",
      "00:40:35 epoch 0 of 100 Train loss: 0.7462 Train f_score 0.6119 Xval loss: 0.7437 Xval f_score 0.6327\n",
      "00:40:35 epoch 10 of 100 Train loss: 0.7323 Train f_score 0.6621 Xval loss: 0.7321 Xval f_score 0.6667\n",
      "00:40:35 epoch 20 of 100 Train loss: 0.7223 Train f_score 0.6710 Xval loss: 0.7227 Xval f_score 0.6667\n",
      "00:40:35 epoch 30 of 100 Train loss: 0.7139 Train f_score 0.6688 Xval loss: 0.7149 Xval f_score 0.6604\n",
      "00:40:35 epoch 40 of 100 Train loss: 0.7072 Train f_score 0.6839 Xval loss: 0.7090 Xval f_score 0.6604\n",
      "00:40:35 epoch 50 of 100 Train loss: 0.7030 Train f_score 0.6795 Xval loss: 0.7046 Xval f_score 0.6476\n",
      "00:40:35 epoch 60 of 100 Train loss: 0.6997 Train f_score 0.6796 Xval loss: 0.7013 Xval f_score 0.6538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:40:35 epoch 70 of 100 Train loss: 0.6969 Train f_score 0.6861 Xval loss: 0.6992 Xval f_score 0.6538\n",
      "00:40:35 epoch 80 of 100 Train loss: 0.6950 Train f_score 0.6775 Xval loss: 0.6979 Xval f_score 0.6471\n",
      "00:40:35 epoch 90 of 100 Train loss: 0.6949 Train f_score 0.6732 Xval loss: 0.6972 Xval f_score 0.6471\n",
      "Best Xval loss epoch 99, value 0.696882\n",
      "NN units 16\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.595, Train F1 0.693\n",
      "[[29 11]\n",
      " [74 96]]\n",
      "Final Xval Accuracy 0.521, Xval F1 0.630\n",
      "[[ 8  6]\n",
      " [28 29]]\n",
      "00:40:38 Starting\n",
      "00:40:40 epoch 0 of 100 Train loss: 0.6947 Train f_score 0.4179 Xval loss: 0.6946 Xval f_score 0.4127\n",
      "00:40:40 epoch 10 of 100 Train loss: 0.6875 Train f_score 0.6471 Xval loss: 0.6944 Xval f_score 0.5250\n",
      "00:40:40 epoch 20 of 100 Train loss: 0.6827 Train f_score 0.5983 Xval loss: 0.6943 Xval f_score 0.5432\n",
      "00:40:40 epoch 30 of 100 Train loss: 0.6750 Train f_score 0.6463 Xval loss: 0.6944 Xval f_score 0.5000\n",
      "00:40:40 epoch 40 of 100 Train loss: 0.6744 Train f_score 0.6210 Xval loss: 0.6947 Xval f_score 0.5067\n",
      "00:40:40 epoch 50 of 100 Train loss: 0.6636 Train f_score 0.6233 Xval loss: 0.6959 Xval f_score 0.4865\n",
      "00:40:40 epoch 60 of 100 Train loss: 0.6562 Train f_score 0.6518 Xval loss: 0.6979 Xval f_score 0.5135\n",
      "00:40:41 epoch 70 of 100 Train loss: 0.6504 Train f_score 0.6481 Xval loss: 0.7006 Xval f_score 0.5135\n",
      "00:40:41 epoch 80 of 100 Train loss: 0.6358 Train f_score 0.6574 Xval loss: 0.7047 Xval f_score 0.5385\n",
      "00:40:41 epoch 90 of 100 Train loss: 0.6387 Train f_score 0.6696 Xval loss: 0.7106 Xval f_score 0.5385\n",
      "Best Xval loss epoch 19, value 0.694289\n",
      "NN units 32\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.710, Train F1 0.705\n",
      "[[76 34]\n",
      " [27 73]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.628\n",
      "[[12  8]\n",
      " [24 27]]\n",
      "00:40:43 Starting\n",
      "00:40:46 epoch 0 of 100 Train loss: 0.6969 Train f_score 0.2797 Xval loss: 0.6960 Xval f_score 0.0000\n",
      "00:40:46 epoch 10 of 100 Train loss: 0.6936 Train f_score 0.4494 Xval loss: 0.6960 Xval f_score 0.4746\n",
      "00:40:46 epoch 20 of 100 Train loss: 0.6859 Train f_score 0.6095 Xval loss: 0.6957 Xval f_score 0.5429\n",
      "00:40:46 epoch 30 of 100 Train loss: 0.6811 Train f_score 0.6552 Xval loss: 0.6954 Xval f_score 0.5070\n",
      "00:40:46 epoch 40 of 100 Train loss: 0.6779 Train f_score 0.6234 Xval loss: 0.6955 Xval f_score 0.5205\n",
      "00:40:46 epoch 50 of 100 Train loss: 0.6695 Train f_score 0.6608 Xval loss: 0.6965 Xval f_score 0.5405\n",
      "00:40:46 epoch 60 of 100 Train loss: 0.6556 Train f_score 0.6816 Xval loss: 0.6983 Xval f_score 0.5600\n",
      "00:40:46 epoch 70 of 100 Train loss: 0.6514 Train f_score 0.6903 Xval loss: 0.7009 Xval f_score 0.5600\n",
      "00:40:46 epoch 80 of 100 Train loss: 0.6456 Train f_score 0.6696 Xval loss: 0.7039 Xval f_score 0.5526\n",
      "00:40:46 epoch 90 of 100 Train loss: 0.6362 Train f_score 0.7022 Xval loss: 0.7078 Xval f_score 0.5333\n",
      "Best Xval loss epoch 35, value 0.695328\n",
      "NN units 32\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.700, Train F1 0.699\n",
      "[[74 34]\n",
      " [29 73]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.619\n",
      "[[13  9]\n",
      " [23 26]]\n",
      "00:40:49 Starting\n",
      "00:40:52 epoch 0 of 100 Train loss: 0.7103 Train f_score 0.4602 Xval loss: 0.7056 Xval f_score 0.5122\n",
      "00:40:52 epoch 10 of 100 Train loss: 0.6990 Train f_score 0.6617 Xval loss: 0.7038 Xval f_score 0.5778\n",
      "00:40:52 epoch 20 of 100 Train loss: 0.6929 Train f_score 0.6840 Xval loss: 0.7025 Xval f_score 0.5952\n",
      "00:40:52 epoch 30 of 100 Train loss: 0.6844 Train f_score 0.6667 Xval loss: 0.7013 Xval f_score 0.5500\n",
      "00:40:52 epoch 40 of 100 Train loss: 0.6800 Train f_score 0.6638 Xval loss: 0.7005 Xval f_score 0.5455\n",
      "00:40:52 epoch 50 of 100 Train loss: 0.6742 Train f_score 0.6273 Xval loss: 0.7006 Xval f_score 0.5263\n",
      "00:40:52 epoch 60 of 100 Train loss: 0.6677 Train f_score 0.6516 Xval loss: 0.7020 Xval f_score 0.5000\n",
      "00:40:52 epoch 70 of 100 Train loss: 0.6510 Train f_score 0.6377 Xval loss: 0.7040 Xval f_score 0.4789\n",
      "00:40:52 epoch 80 of 100 Train loss: 0.6504 Train f_score 0.6419 Xval loss: 0.7064 Xval f_score 0.4507\n",
      "00:40:52 epoch 90 of 100 Train loss: 0.6449 Train f_score 0.6570 Xval loss: 0.7098 Xval f_score 0.4932\n",
      "Best Xval loss epoch 44, value 0.700407\n",
      "NN units 32\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.676, Train F1 0.709\n",
      "[[59 24]\n",
      " [44 83]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.652\n",
      "[[ 9  5]\n",
      " [27 30]]\n",
      "00:40:55 Starting\n",
      "00:40:57 epoch 0 of 100 Train loss: 0.7312 Train f_score 0.4422 Xval loss: 0.7281 Xval f_score 0.4062\n",
      "00:40:58 epoch 10 of 100 Train loss: 0.7213 Train f_score 0.5969 Xval loss: 0.7217 Xval f_score 0.5843\n",
      "00:40:58 epoch 20 of 100 Train loss: 0.7136 Train f_score 0.6452 Xval loss: 0.7169 Xval f_score 0.6465\n",
      "00:40:58 epoch 30 of 100 Train loss: 0.7068 Train f_score 0.6760 Xval loss: 0.7130 Xval f_score 0.6465\n",
      "00:40:58 epoch 40 of 100 Train loss: 0.7008 Train f_score 0.6786 Xval loss: 0.7100 Xval f_score 0.6250\n",
      "00:40:58 epoch 50 of 100 Train loss: 0.6977 Train f_score 0.6792 Xval loss: 0.7082 Xval f_score 0.6087\n",
      "00:40:58 epoch 60 of 100 Train loss: 0.6932 Train f_score 0.6719 Xval loss: 0.7070 Xval f_score 0.5952\n",
      "00:40:58 epoch 70 of 100 Train loss: 0.6872 Train f_score 0.6611 Xval loss: 0.7065 Xval f_score 0.5750\n",
      "00:40:58 epoch 80 of 100 Train loss: 0.6858 Train f_score 0.6553 Xval loss: 0.7068 Xval f_score 0.5405\n",
      "00:40:58 epoch 90 of 100 Train loss: 0.6815 Train f_score 0.6455 Xval loss: 0.7075 Xval f_score 0.5070\n",
      "Best Xval loss epoch 70, value 0.706503\n",
      "NN units 32\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.667, Train F1 0.696\n",
      "[[60 27]\n",
      " [43 80]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.681\n",
      "[[ 9  3]\n",
      " [27 32]]\n",
      "00:41:01 Starting\n",
      "00:41:03 epoch 0 of 100 Train loss: 0.8010 Train f_score 0.4950 Xval loss: 0.8048 Xval f_score 0.3548\n",
      "00:41:03 epoch 10 of 100 Train loss: 0.7764 Train f_score 0.5818 Xval loss: 0.7806 Xval f_score 0.4267\n",
      "00:41:04 epoch 20 of 100 Train loss: 0.7557 Train f_score 0.6412 Xval loss: 0.7601 Xval f_score 0.5581\n",
      "00:41:04 epoch 30 of 100 Train loss: 0.7386 Train f_score 0.6277 Xval loss: 0.7437 Xval f_score 0.5843\n",
      "00:41:04 epoch 40 of 100 Train loss: 0.7237 Train f_score 0.6738 Xval loss: 0.7306 Xval f_score 0.6087\n",
      "00:41:04 epoch 50 of 100 Train loss: 0.7128 Train f_score 0.6691 Xval loss: 0.7203 Xval f_score 0.5934\n",
      "00:41:04 epoch 60 of 100 Train loss: 0.7082 Train f_score 0.6643 Xval loss: 0.7128 Xval f_score 0.6237\n",
      "00:41:04 epoch 70 of 100 Train loss: 0.7013 Train f_score 0.6761 Xval loss: 0.7078 Xval f_score 0.6087\n",
      "00:41:04 epoch 80 of 100 Train loss: 0.6970 Train f_score 0.6934 Xval loss: 0.7045 Xval f_score 0.6304\n",
      "00:41:04 epoch 90 of 100 Train loss: 0.6955 Train f_score 0.6593 Xval loss: 0.7030 Xval f_score 0.6292\n",
      "Best Xval loss epoch 99, value 0.702485\n",
      "NN units 32\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.5000\n",
      "Activation relu\n",
      "Final Train Accuracy 0.614, Train F1 0.667\n",
      "[[48 26]\n",
      " [55 81]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.304\n",
      "[[32 28]\n",
      " [ 4  7]]\n",
      "00:41:07 Starting\n",
      "00:41:09 epoch 0 of 100 Train loss: 0.6916 Train f_score 0.4674 Xval loss: 0.6930 Xval f_score 0.0000\n",
      "00:41:09 epoch 10 of 100 Train loss: 0.6911 Train f_score 0.4737 Xval loss: 0.6930 Xval f_score 0.1081\n",
      "00:41:09 epoch 20 of 100 Train loss: 0.6934 Train f_score 0.5116 Xval loss: 0.6929 Xval f_score 0.4262\n",
      "00:41:09 epoch 30 of 100 Train loss: 0.6928 Train f_score 0.5388 Xval loss: 0.6930 Xval f_score 0.6337\n",
      "00:41:09 epoch 40 of 100 Train loss: 0.6932 Train f_score 0.6212 Xval loss: 0.6931 Xval f_score 0.6476\n",
      "00:41:09 epoch 50 of 100 Train loss: 0.6916 Train f_score 0.6183 Xval loss: 0.6931 Xval f_score 0.6476\n",
      "00:41:09 epoch 60 of 100 Train loss: 0.6905 Train f_score 0.6124 Xval loss: 0.6932 Xval f_score 0.6476\n",
      "00:41:09 epoch 70 of 100 Train loss: 0.6939 Train f_score 0.5748 Xval loss: 0.6931 Xval f_score 0.6476\n",
      "00:41:09 epoch 80 of 100 Train loss: 0.6900 Train f_score 0.6284 Xval loss: 0.6931 Xval f_score 0.6476\n",
      "00:41:09 epoch 90 of 100 Train loss: 0.6941 Train f_score 0.5859 Xval loss: 0.6930 Xval f_score 0.6408\n",
      "Best Xval loss epoch 17, value 0.692937\n",
      "NN units 4\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.576, Train F1 0.662\n",
      "[[34 20]\n",
      " [69 87]]\n",
      "Final Xval Accuracy 0.592, Xval F1 0.580\n",
      "[[22 15]\n",
      " [14 20]]\n",
      "00:41:12 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:41:15 epoch 0 of 100 Train loss: 0.6944 Train f_score 0.3415 Xval loss: 0.6938 Xval f_score 0.0000\n",
      "00:41:15 epoch 10 of 100 Train loss: 0.6928 Train f_score 0.5213 Xval loss: 0.6937 Xval f_score 0.2308\n",
      "00:41:15 epoch 20 of 100 Train loss: 0.6943 Train f_score 0.5656 Xval loss: 0.6938 Xval f_score 0.6604\n",
      "00:41:15 epoch 30 of 100 Train loss: 0.6904 Train f_score 0.6807 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:41:15 epoch 40 of 100 Train loss: 0.6941 Train f_score 0.6290 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:41:15 epoch 50 of 100 Train loss: 0.6945 Train f_score 0.6069 Xval loss: 0.6940 Xval f_score 0.6604\n",
      "00:41:15 epoch 60 of 100 Train loss: 0.6941 Train f_score 0.6411 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:41:15 epoch 70 of 100 Train loss: 0.6935 Train f_score 0.6176 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "00:41:15 epoch 80 of 100 Train loss: 0.6938 Train f_score 0.6282 Xval loss: 0.6938 Xval f_score 0.6346\n",
      "00:41:15 epoch 90 of 100 Train loss: 0.6941 Train f_score 0.5771 Xval loss: 0.6938 Xval f_score 0.6346\n",
      "Best Xval loss epoch 8, value 0.693708\n",
      "NN units 4\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.600, Train F1 0.641\n",
      "[[51 32]\n",
      " [52 75]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.608\n",
      "[[16 11]\n",
      " [20 24]]\n",
      "00:41:19 Starting\n",
      "00:41:21 epoch 0 of 100 Train loss: 0.6952 Train f_score 0.5921 Xval loss: 0.6951 Xval f_score 0.6604\n",
      "00:41:21 epoch 10 of 100 Train loss: 0.6926 Train f_score 0.6300 Xval loss: 0.6946 Xval f_score 0.6604\n",
      "00:41:21 epoch 20 of 100 Train loss: 0.6942 Train f_score 0.6130 Xval loss: 0.6944 Xval f_score 0.6604\n",
      "00:41:21 epoch 30 of 100 Train loss: 0.6945 Train f_score 0.6100 Xval loss: 0.6943 Xval f_score 0.6604\n",
      "00:41:21 epoch 40 of 100 Train loss: 0.6956 Train f_score 0.5926 Xval loss: 0.6943 Xval f_score 0.6604\n",
      "00:41:21 epoch 50 of 100 Train loss: 0.6901 Train f_score 0.6255 Xval loss: 0.6943 Xval f_score 0.6604\n",
      "00:41:21 epoch 60 of 100 Train loss: 0.6926 Train f_score 0.5961 Xval loss: 0.6942 Xval f_score 0.6476\n",
      "00:41:21 epoch 70 of 100 Train loss: 0.6949 Train f_score 0.5929 Xval loss: 0.6942 Xval f_score 0.6476\n",
      "00:41:21 epoch 80 of 100 Train loss: 0.6923 Train f_score 0.6178 Xval loss: 0.6943 Xval f_score 0.6476\n",
      "00:41:22 epoch 90 of 100 Train loss: 0.6936 Train f_score 0.6100 Xval loss: 0.6944 Xval f_score 0.6476\n",
      "Best Xval loss epoch 69, value 0.694213\n",
      "NN units 4\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.581, Train F1 0.651\n",
      "[[40 25]\n",
      " [63 82]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.600\n",
      "[[15 11]\n",
      " [21 24]]\n",
      "00:41:25 Starting\n",
      "00:41:27 epoch 0 of 100 Train loss: 0.6968 Train f_score 0.5098 Xval loss: 0.6975 Xval f_score 0.4000\n",
      "00:41:27 epoch 10 of 100 Train loss: 0.6983 Train f_score 0.5205 Xval loss: 0.6968 Xval f_score 0.6604\n",
      "00:41:27 epoch 20 of 100 Train loss: 0.6964 Train f_score 0.5333 Xval loss: 0.6962 Xval f_score 0.6604\n",
      "00:41:27 epoch 30 of 100 Train loss: 0.6919 Train f_score 0.6444 Xval loss: 0.6957 Xval f_score 0.6604\n",
      "00:41:27 epoch 40 of 100 Train loss: 0.6949 Train f_score 0.6118 Xval loss: 0.6953 Xval f_score 0.6604\n",
      "00:41:27 epoch 50 of 100 Train loss: 0.6961 Train f_score 0.6214 Xval loss: 0.6949 Xval f_score 0.6604\n",
      "00:41:27 epoch 60 of 100 Train loss: 0.6964 Train f_score 0.6268 Xval loss: 0.6946 Xval f_score 0.6604\n",
      "00:41:28 epoch 70 of 100 Train loss: 0.6941 Train f_score 0.6383 Xval loss: 0.6944 Xval f_score 0.6604\n",
      "00:41:28 epoch 80 of 100 Train loss: 0.6953 Train f_score 0.5965 Xval loss: 0.6942 Xval f_score 0.6604\n",
      "00:41:28 epoch 90 of 100 Train loss: 0.6937 Train f_score 0.6273 Xval loss: 0.6941 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.693964\n",
      "NN units 4\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.590, Train F1 0.661\n",
      "[[40 23]\n",
      " [63 84]]\n",
      "Final Xval Accuracy 0.606, Xval F1 0.632\n",
      "[[19 11]\n",
      " [17 24]]\n",
      "00:41:31 Starting\n",
      "00:41:33 epoch 0 of 100 Train loss: 0.7081 Train f_score 0.6350 Xval loss: 0.7079 Xval f_score 0.6604\n",
      "00:41:33 epoch 10 of 100 Train loss: 0.7026 Train f_score 0.6367 Xval loss: 0.7044 Xval f_score 0.6604\n",
      "00:41:33 epoch 20 of 100 Train loss: 0.7035 Train f_score 0.5891 Xval loss: 0.7016 Xval f_score 0.6604\n",
      "00:41:33 epoch 30 of 100 Train loss: 0.7015 Train f_score 0.5799 Xval loss: 0.6994 Xval f_score 0.6604\n",
      "00:41:33 epoch 40 of 100 Train loss: 0.6964 Train f_score 0.6294 Xval loss: 0.6977 Xval f_score 0.6604\n",
      "00:41:33 epoch 50 of 100 Train loss: 0.6943 Train f_score 0.6574 Xval loss: 0.6964 Xval f_score 0.6604\n",
      "00:41:33 epoch 60 of 100 Train loss: 0.6942 Train f_score 0.6735 Xval loss: 0.6953 Xval f_score 0.6604\n",
      "00:41:34 epoch 70 of 100 Train loss: 0.6948 Train f_score 0.6408 Xval loss: 0.6947 Xval f_score 0.6604\n",
      "00:41:34 epoch 80 of 100 Train loss: 0.6946 Train f_score 0.6414 Xval loss: 0.6943 Xval f_score 0.6604\n",
      "00:41:34 epoch 90 of 100 Train loss: 0.6935 Train f_score 0.6555 Xval loss: 0.6941 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.694005\n",
      "NN units 4\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.557, Train F1 0.614\n",
      "[[43 33]\n",
      " [60 74]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.311\n",
      "[[33 28]\n",
      " [ 3  7]]\n",
      "00:41:37 Starting\n",
      "00:41:39 epoch 0 of 100 Train loss: 0.6998 Train f_score 0.5120 Xval loss: 0.6936 Xval f_score 0.6604\n",
      "00:41:39 epoch 10 of 100 Train loss: 0.6928 Train f_score 0.6194 Xval loss: 0.6936 Xval f_score 0.6604\n",
      "00:41:39 epoch 20 of 100 Train loss: 0.6946 Train f_score 0.5846 Xval loss: 0.6934 Xval f_score 0.6476\n",
      "00:41:39 epoch 30 of 100 Train loss: 0.6893 Train f_score 0.6284 Xval loss: 0.6933 Xval f_score 0.6476\n",
      "00:41:39 epoch 40 of 100 Train loss: 0.6955 Train f_score 0.5569 Xval loss: 0.6932 Xval f_score 0.6476\n",
      "00:41:39 epoch 50 of 100 Train loss: 0.6916 Train f_score 0.6260 Xval loss: 0.6931 Xval f_score 0.6346\n",
      "00:41:39 epoch 60 of 100 Train loss: 0.6913 Train f_score 0.6220 Xval loss: 0.6930 Xval f_score 0.6408\n",
      "00:41:40 epoch 70 of 100 Train loss: 0.6903 Train f_score 0.6111 Xval loss: 0.6928 Xval f_score 0.6263\n",
      "00:41:40 epoch 80 of 100 Train loss: 0.6892 Train f_score 0.5787 Xval loss: 0.6927 Xval f_score 0.6392\n",
      "00:41:40 epoch 90 of 100 Train loss: 0.6891 Train f_score 0.5961 Xval loss: 0.6926 Xval f_score 0.6022\n",
      "Best Xval loss epoch 99, value 0.692533\n",
      "NN units 8\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.586, Train F1 0.639\n",
      "[[46 30]\n",
      " [57 77]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.375\n",
      "[[32 26]\n",
      " [ 4  9]]\n",
      "00:41:43 Starting\n",
      "00:41:45 epoch 0 of 100 Train loss: 0.6967 Train f_score 0.3145 Xval loss: 0.6928 Xval f_score 0.0000\n",
      "00:41:46 epoch 10 of 100 Train loss: 0.6937 Train f_score 0.5126 Xval loss: 0.6928 Xval f_score 0.2000\n",
      "00:41:46 epoch 20 of 100 Train loss: 0.6925 Train f_score 0.5600 Xval loss: 0.6929 Xval f_score 0.6250\n",
      "00:41:46 epoch 30 of 100 Train loss: 0.6880 Train f_score 0.6076 Xval loss: 0.6931 Xval f_score 0.6538\n",
      "00:41:46 epoch 40 of 100 Train loss: 0.6938 Train f_score 0.5654 Xval loss: 0.6932 Xval f_score 0.6471\n",
      "00:41:46 epoch 50 of 100 Train loss: 0.6934 Train f_score 0.5911 Xval loss: 0.6932 Xval f_score 0.6337\n",
      "00:41:46 epoch 60 of 100 Train loss: 0.6948 Train f_score 0.5508 Xval loss: 0.6932 Xval f_score 0.6327\n",
      "00:41:46 epoch 70 of 100 Train loss: 0.6922 Train f_score 0.6017 Xval loss: 0.6932 Xval f_score 0.6250\n",
      "00:41:46 epoch 80 of 100 Train loss: 0.6938 Train f_score 0.5417 Xval loss: 0.6933 Xval f_score 0.6392\n",
      "00:41:46 epoch 90 of 100 Train loss: 0.6921 Train f_score 0.5739 Xval loss: 0.6934 Xval f_score 0.6392\n",
      "Best Xval loss epoch 8, value 0.692747\n",
      "NN units 8\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.595, Train F1 0.635\n",
      "[[51 33]\n",
      " [52 74]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.348\n",
      "[[33 27]\n",
      " [ 3  8]]\n",
      "00:41:49 Starting\n",
      "00:41:53 epoch 0 of 100 Train loss: 0.6967 Train f_score 0.6429 Xval loss: 0.6975 Xval f_score 0.6604\n",
      "00:41:54 epoch 10 of 100 Train loss: 0.6917 Train f_score 0.6477 Xval loss: 0.6964 Xval f_score 0.6604\n",
      "00:41:54 epoch 20 of 100 Train loss: 0.6950 Train f_score 0.5944 Xval loss: 0.6957 Xval f_score 0.6604\n",
      "00:41:54 epoch 30 of 100 Train loss: 0.6962 Train f_score 0.5294 Xval loss: 0.6953 Xval f_score 0.6476\n",
      "00:41:54 epoch 40 of 100 Train loss: 0.6962 Train f_score 0.5600 Xval loss: 0.6951 Xval f_score 0.6476\n",
      "00:41:54 epoch 50 of 100 Train loss: 0.6957 Train f_score 0.5714 Xval loss: 0.6950 Xval f_score 0.6476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:41:54 epoch 60 of 100 Train loss: 0.6923 Train f_score 0.6290 Xval loss: 0.6949 Xval f_score 0.6476\n",
      "00:41:54 epoch 70 of 100 Train loss: 0.6920 Train f_score 0.5976 Xval loss: 0.6948 Xval f_score 0.6476\n",
      "00:41:54 epoch 80 of 100 Train loss: 0.6921 Train f_score 0.6264 Xval loss: 0.6948 Xval f_score 0.6346\n",
      "00:41:54 epoch 90 of 100 Train loss: 0.6889 Train f_score 0.6230 Xval loss: 0.6947 Xval f_score 0.6408\n",
      "Best Xval loss epoch 91, value 0.694703\n",
      "NN units 8\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.586, Train F1 0.599\n",
      "[[58 42]\n",
      " [45 65]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.595\n",
      "[[19 13]\n",
      " [17 22]]\n",
      "00:41:57 Starting\n",
      "00:42:00 epoch 0 of 100 Train loss: 0.7015 Train f_score 0.4000 Xval loss: 0.7023 Xval f_score 0.0000\n",
      "00:42:00 epoch 10 of 100 Train loss: 0.7001 Train f_score 0.4712 Xval loss: 0.7002 Xval f_score 0.3636\n",
      "00:42:00 epoch 20 of 100 Train loss: 0.6976 Train f_score 0.6245 Xval loss: 0.6986 Xval f_score 0.6604\n",
      "00:42:00 epoch 30 of 100 Train loss: 0.6980 Train f_score 0.6067 Xval loss: 0.6975 Xval f_score 0.6604\n",
      "00:42:00 epoch 40 of 100 Train loss: 0.6953 Train f_score 0.6502 Xval loss: 0.6965 Xval f_score 0.6604\n",
      "00:42:00 epoch 50 of 100 Train loss: 0.6925 Train f_score 0.6473 Xval loss: 0.6956 Xval f_score 0.6604\n",
      "00:42:00 epoch 60 of 100 Train loss: 0.6951 Train f_score 0.6595 Xval loss: 0.6949 Xval f_score 0.6604\n",
      "00:42:00 epoch 70 of 100 Train loss: 0.6945 Train f_score 0.6360 Xval loss: 0.6945 Xval f_score 0.6604\n",
      "00:42:00 epoch 80 of 100 Train loss: 0.6946 Train f_score 0.6176 Xval loss: 0.6941 Xval f_score 0.6604\n",
      "00:42:00 epoch 90 of 100 Train loss: 0.6922 Train f_score 0.6574 Xval loss: 0.6939 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.693858\n",
      "NN units 8\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.581, Train F1 0.551\n",
      "[[68 53]\n",
      " [35 54]]\n",
      "Final Xval Accuracy 0.592, Xval F1 0.603\n",
      "[[20 13]\n",
      " [16 22]]\n",
      "00:42:04 Starting\n",
      "00:42:06 epoch 0 of 100 Train loss: 0.7204 Train f_score 0.4854 Xval loss: 0.7187 Xval f_score 0.4687\n",
      "00:42:06 epoch 10 of 100 Train loss: 0.7120 Train f_score 0.5946 Xval loss: 0.7124 Xval f_score 0.6604\n",
      "00:42:06 epoch 20 of 100 Train loss: 0.7056 Train f_score 0.6380 Xval loss: 0.7073 Xval f_score 0.6604\n",
      "00:42:06 epoch 30 of 100 Train loss: 0.7031 Train f_score 0.6486 Xval loss: 0.7032 Xval f_score 0.6604\n",
      "00:42:06 epoch 40 of 100 Train loss: 0.7006 Train f_score 0.6373 Xval loss: 0.7002 Xval f_score 0.6604\n",
      "00:42:06 epoch 50 of 100 Train loss: 0.6971 Train f_score 0.6528 Xval loss: 0.6979 Xval f_score 0.6604\n",
      "00:42:06 epoch 60 of 100 Train loss: 0.6955 Train f_score 0.6505 Xval loss: 0.6963 Xval f_score 0.6604\n",
      "00:42:06 epoch 70 of 100 Train loss: 0.6947 Train f_score 0.6438 Xval loss: 0.6953 Xval f_score 0.6604\n",
      "00:42:07 epoch 80 of 100 Train loss: 0.6936 Train f_score 0.6752 Xval loss: 0.6946 Xval f_score 0.6604\n",
      "00:42:07 epoch 90 of 100 Train loss: 0.6930 Train f_score 0.6535 Xval loss: 0.6941 Xval f_score 0.6604\n",
      "Best Xval loss epoch 98, value 0.693945\n",
      "NN units 8\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.557, Train F1 0.652\n",
      "[[30 20]\n",
      " [73 87]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.400\n",
      "[[31 25]\n",
      " [ 5 10]]\n",
      "00:42:10 Starting\n",
      "00:42:13 epoch 0 of 100 Train loss: 0.6899 Train f_score 0.6131 Xval loss: 0.6946 Xval f_score 0.6604\n",
      "00:42:13 epoch 10 of 100 Train loss: 0.6950 Train f_score 0.5882 Xval loss: 0.6938 Xval f_score 0.6476\n",
      "00:42:13 epoch 20 of 100 Train loss: 0.6915 Train f_score 0.6360 Xval loss: 0.6935 Xval f_score 0.6476\n",
      "00:42:13 epoch 30 of 100 Train loss: 0.6946 Train f_score 0.5514 Xval loss: 0.6934 Xval f_score 0.6408\n",
      "00:42:13 epoch 40 of 100 Train loss: 0.6892 Train f_score 0.6295 Xval loss: 0.6933 Xval f_score 0.6263\n",
      "00:42:13 epoch 50 of 100 Train loss: 0.6939 Train f_score 0.6279 Xval loss: 0.6932 Xval f_score 0.6263\n",
      "00:42:13 epoch 60 of 100 Train loss: 0.6905 Train f_score 0.6073 Xval loss: 0.6931 Xval f_score 0.6105\n",
      "00:42:13 epoch 70 of 100 Train loss: 0.6913 Train f_score 0.5570 Xval loss: 0.6928 Xval f_score 0.6222\n",
      "00:42:13 epoch 80 of 100 Train loss: 0.6917 Train f_score 0.5837 Xval loss: 0.6927 Xval f_score 0.6222\n",
      "00:42:13 epoch 90 of 100 Train loss: 0.6845 Train f_score 0.6073 Xval loss: 0.6925 Xval f_score 0.5814\n",
      "Best Xval loss epoch 99, value 0.692378\n",
      "NN units 16\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.595, Train F1 0.629\n",
      "[[53 35]\n",
      " [50 72]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.617\n",
      "[[15 10]\n",
      " [21 25]]\n",
      "00:42:16 Starting\n",
      "00:42:19 epoch 0 of 100 Train loss: 0.6957 Train f_score 0.4239 Xval loss: 0.6946 Xval f_score 0.0000\n",
      "00:42:19 epoch 10 of 100 Train loss: 0.6963 Train f_score 0.5625 Xval loss: 0.6945 Xval f_score 0.6408\n",
      "00:42:19 epoch 20 of 100 Train loss: 0.6961 Train f_score 0.6212 Xval loss: 0.6946 Xval f_score 0.6604\n",
      "00:42:19 epoch 30 of 100 Train loss: 0.6967 Train f_score 0.6038 Xval loss: 0.6945 Xval f_score 0.6604\n",
      "00:42:19 epoch 40 of 100 Train loss: 0.6925 Train f_score 0.6255 Xval loss: 0.6944 Xval f_score 0.6476\n",
      "00:42:19 epoch 50 of 100 Train loss: 0.6883 Train f_score 0.6565 Xval loss: 0.6943 Xval f_score 0.6408\n",
      "00:42:19 epoch 60 of 100 Train loss: 0.6930 Train f_score 0.5796 Xval loss: 0.6942 Xval f_score 0.6200\n",
      "00:42:19 epoch 70 of 100 Train loss: 0.6919 Train f_score 0.5725 Xval loss: 0.6941 Xval f_score 0.6186\n",
      "00:42:19 epoch 80 of 100 Train loss: 0.6909 Train f_score 0.6324 Xval loss: 0.6942 Xval f_score 0.6105\n",
      "00:42:19 epoch 90 of 100 Train loss: 0.6930 Train f_score 0.5714 Xval loss: 0.6942 Xval f_score 0.6105\n",
      "Best Xval loss epoch 73, value 0.694108\n",
      "NN units 16\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.581, Train F1 0.659\n",
      "[[37 22]\n",
      " [66 85]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.475\n",
      "[[26 21]\n",
      " [10 14]]\n",
      "00:42:23 Starting\n",
      "00:42:26 epoch 0 of 100 Train loss: 0.7010 Train f_score 0.4793 Xval loss: 0.6999 Xval f_score 0.5116\n",
      "00:42:26 epoch 10 of 100 Train loss: 0.6974 Train f_score 0.5826 Xval loss: 0.6991 Xval f_score 0.6476\n",
      "00:42:26 epoch 20 of 100 Train loss: 0.6963 Train f_score 0.5844 Xval loss: 0.6985 Xval f_score 0.6476\n",
      "00:42:26 epoch 30 of 100 Train loss: 0.6964 Train f_score 0.6054 Xval loss: 0.6978 Xval f_score 0.6476\n",
      "00:42:26 epoch 40 of 100 Train loss: 0.6983 Train f_score 0.5591 Xval loss: 0.6973 Xval f_score 0.6476\n",
      "00:42:26 epoch 50 of 100 Train loss: 0.6994 Train f_score 0.5333 Xval loss: 0.6969 Xval f_score 0.6476\n",
      "00:42:26 epoch 60 of 100 Train loss: 0.6927 Train f_score 0.5961 Xval loss: 0.6966 Xval f_score 0.6408\n",
      "00:42:26 epoch 70 of 100 Train loss: 0.6937 Train f_score 0.6084 Xval loss: 0.6964 Xval f_score 0.6408\n",
      "00:42:26 epoch 80 of 100 Train loss: 0.6918 Train f_score 0.5976 Xval loss: 0.6962 Xval f_score 0.6400\n",
      "00:42:26 epoch 90 of 100 Train loss: 0.6946 Train f_score 0.5620 Xval loss: 0.6960 Xval f_score 0.5745\n",
      "Best Xval loss epoch 97, value 0.695944\n",
      "NN units 16\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.590, Train F1 0.632\n",
      "[[50 33]\n",
      " [53 74]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.608\n",
      "[[16 11]\n",
      " [20 24]]\n",
      "00:42:29 Starting\n",
      "00:42:32 epoch 0 of 100 Train loss: 0.7145 Train f_score 0.3007 Xval loss: 0.7106 Xval f_score 0.0000\n",
      "00:42:32 epoch 10 of 100 Train loss: 0.7131 Train f_score 0.4100 Xval loss: 0.7066 Xval f_score 0.1053\n",
      "00:42:32 epoch 20 of 100 Train loss: 0.6999 Train f_score 0.6044 Xval loss: 0.7037 Xval f_score 0.6604\n",
      "00:42:32 epoch 30 of 100 Train loss: 0.7011 Train f_score 0.5911 Xval loss: 0.7016 Xval f_score 0.6604\n",
      "00:42:32 epoch 40 of 100 Train loss: 0.7004 Train f_score 0.6094 Xval loss: 0.6997 Xval f_score 0.6604\n",
      "00:42:32 epoch 50 of 100 Train loss: 0.6968 Train f_score 0.5984 Xval loss: 0.6981 Xval f_score 0.6604\n",
      "00:42:32 epoch 60 of 100 Train loss: 0.7007 Train f_score 0.5591 Xval loss: 0.6969 Xval f_score 0.6604\n",
      "00:42:32 epoch 70 of 100 Train loss: 0.7000 Train f_score 0.6023 Xval loss: 0.6961 Xval f_score 0.6604\n",
      "00:42:32 epoch 80 of 100 Train loss: 0.6924 Train f_score 0.6260 Xval loss: 0.6954 Xval f_score 0.6604\n",
      "00:42:33 epoch 90 of 100 Train loss: 0.6976 Train f_score 0.5425 Xval loss: 0.6950 Xval f_score 0.6604\n",
      "Best Xval loss epoch 98, value 0.694885\n",
      "NN units 16\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.586, Train F1 0.659\n",
      "[[39 23]\n",
      " [64 84]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.634\n",
      "[[15  9]\n",
      " [21 26]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:42:36 Starting\n",
      "00:42:39 epoch 0 of 100 Train loss: 0.7478 Train f_score 0.3871 Xval loss: 0.7457 Xval f_score 0.0000\n",
      "00:42:39 epoch 10 of 100 Train loss: 0.7364 Train f_score 0.4456 Xval loss: 0.7326 Xval f_score 0.1081\n",
      "00:42:39 epoch 20 of 100 Train loss: 0.7296 Train f_score 0.4476 Xval loss: 0.7225 Xval f_score 0.6604\n",
      "00:42:39 epoch 30 of 100 Train loss: 0.7159 Train f_score 0.5868 Xval loss: 0.7149 Xval f_score 0.6604\n",
      "00:42:39 epoch 40 of 100 Train loss: 0.7066 Train f_score 0.5897 Xval loss: 0.7088 Xval f_score 0.6604\n",
      "00:42:39 epoch 50 of 100 Train loss: 0.7057 Train f_score 0.5891 Xval loss: 0.7041 Xval f_score 0.6604\n",
      "00:42:39 epoch 60 of 100 Train loss: 0.7016 Train f_score 0.5714 Xval loss: 0.7006 Xval f_score 0.6604\n",
      "00:42:39 epoch 70 of 100 Train loss: 0.6990 Train f_score 0.5669 Xval loss: 0.6980 Xval f_score 0.6604\n",
      "00:42:39 epoch 80 of 100 Train loss: 0.6970 Train f_score 0.6016 Xval loss: 0.6964 Xval f_score 0.6604\n",
      "00:42:39 epoch 90 of 100 Train loss: 0.6966 Train f_score 0.5814 Xval loss: 0.6957 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.695279\n",
      "NN units 16\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.595, Train F1 0.585\n",
      "[[65 47]\n",
      " [38 60]]\n",
      "Final Xval Accuracy 0.563, Xval F1 0.537\n",
      "[[22 17]\n",
      " [14 18]]\n",
      "00:42:43 Starting\n",
      "00:42:46 epoch 0 of 100 Train loss: 0.7006 Train f_score 0.3550 Xval loss: 0.6930 Xval f_score 0.0000\n",
      "00:42:46 epoch 10 of 100 Train loss: 0.6975 Train f_score 0.4907 Xval loss: 0.6926 Xval f_score 0.6250\n",
      "00:42:46 epoch 20 of 100 Train loss: 0.6875 Train f_score 0.6587 Xval loss: 0.6931 Xval f_score 0.6538\n",
      "00:42:46 epoch 30 of 100 Train loss: 0.6889 Train f_score 0.5935 Xval loss: 0.6928 Xval f_score 0.6061\n",
      "00:42:46 epoch 40 of 100 Train loss: 0.6956 Train f_score 0.5787 Xval loss: 0.6922 Xval f_score 0.5977\n",
      "00:42:46 epoch 50 of 100 Train loss: 0.6838 Train f_score 0.6207 Xval loss: 0.6919 Xval f_score 0.5570\n",
      "00:42:46 epoch 60 of 100 Train loss: 0.6927 Train f_score 0.5847 Xval loss: 0.6919 Xval f_score 0.5600\n",
      "00:42:46 epoch 70 of 100 Train loss: 0.6900 Train f_score 0.5665 Xval loss: 0.6919 Xval f_score 0.5600\n",
      "00:42:46 epoch 80 of 100 Train loss: 0.6836 Train f_score 0.5505 Xval loss: 0.6919 Xval f_score 0.5789\n",
      "00:42:46 epoch 90 of 100 Train loss: 0.6805 Train f_score 0.5957 Xval loss: 0.6920 Xval f_score 0.5600\n",
      "Best Xval loss epoch 64, value 0.691843\n",
      "NN units 32\n",
      "Reg_penalty 0.00000000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.586, Train F1 0.630\n",
      "[[49 33]\n",
      " [54 74]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.318\n",
      "[[34 28]\n",
      " [ 2  7]]\n",
      "00:42:49 Starting\n",
      "00:42:52 epoch 0 of 100 Train loss: 0.7043 Train f_score 0.2319 Xval loss: 0.6973 Xval f_score 0.0000\n",
      "00:42:52 epoch 10 of 100 Train loss: 0.6987 Train f_score 0.5126 Xval loss: 0.6960 Xval f_score 0.5915\n",
      "00:42:52 epoch 20 of 100 Train loss: 0.6935 Train f_score 0.6080 Xval loss: 0.6963 Xval f_score 0.6476\n",
      "00:42:52 epoch 30 of 100 Train loss: 0.6972 Train f_score 0.5873 Xval loss: 0.6959 Xval f_score 0.6471\n",
      "00:42:52 epoch 40 of 100 Train loss: 0.6933 Train f_score 0.5583 Xval loss: 0.6954 Xval f_score 0.5957\n",
      "00:42:52 epoch 50 of 100 Train loss: 0.6931 Train f_score 0.5664 Xval loss: 0.6951 Xval f_score 0.6190\n",
      "00:42:53 epoch 60 of 100 Train loss: 0.6922 Train f_score 0.5739 Xval loss: 0.6951 Xval f_score 0.6098\n",
      "00:42:53 epoch 70 of 100 Train loss: 0.6897 Train f_score 0.6043 Xval loss: 0.6953 Xval f_score 0.6265\n",
      "00:42:53 epoch 80 of 100 Train loss: 0.6967 Train f_score 0.5000 Xval loss: 0.6955 Xval f_score 0.5926\n",
      "00:42:53 epoch 90 of 100 Train loss: 0.6837 Train f_score 0.5841 Xval loss: 0.6958 Xval f_score 0.5926\n",
      "Best Xval loss epoch 55, value 0.695058\n",
      "NN units 32\n",
      "Reg_penalty 0.00010000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.600, Train F1 0.644\n",
      "[[50 31]\n",
      " [53 76]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.615\n",
      "[[17 11]\n",
      " [19 24]]\n",
      "00:42:56 Starting\n",
      "00:42:59 epoch 0 of 100 Train loss: 0.7098 Train f_score 0.4906 Xval loss: 0.7054 Xval f_score 0.5591\n",
      "00:42:59 epoch 10 of 100 Train loss: 0.6976 Train f_score 0.5798 Xval loss: 0.7039 Xval f_score 0.6538\n",
      "00:42:59 epoch 20 of 100 Train loss: 0.6966 Train f_score 0.6230 Xval loss: 0.7023 Xval f_score 0.5918\n",
      "00:42:59 epoch 30 of 100 Train loss: 0.7006 Train f_score 0.5259 Xval loss: 0.7010 Xval f_score 0.5778\n",
      "00:42:59 epoch 40 of 100 Train loss: 0.6964 Train f_score 0.5858 Xval loss: 0.7001 Xval f_score 0.5909\n",
      "00:42:59 epoch 50 of 100 Train loss: 0.6962 Train f_score 0.5316 Xval loss: 0.6995 Xval f_score 0.5814\n",
      "00:42:59 epoch 60 of 100 Train loss: 0.6939 Train f_score 0.5957 Xval loss: 0.6991 Xval f_score 0.5747\n",
      "00:42:59 epoch 70 of 100 Train loss: 0.6994 Train f_score 0.5551 Xval loss: 0.6988 Xval f_score 0.5814\n",
      "00:42:59 epoch 80 of 100 Train loss: 0.7005 Train f_score 0.5217 Xval loss: 0.6986 Xval f_score 0.6024\n",
      "00:42:59 epoch 90 of 100 Train loss: 0.6916 Train f_score 0.5727 Xval loss: 0.6986 Xval f_score 0.5814\n",
      "Best Xval loss epoch 81, value 0.698572\n",
      "NN units 32\n",
      "Reg_penalty 0.00030000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.581, Train F1 0.648\n",
      "[[41 26]\n",
      " [62 81]]\n",
      "Final Xval Accuracy 0.549, Xval F1 0.600\n",
      "[[15 11]\n",
      " [21 24]]\n",
      "00:43:03 Starting\n",
      "00:43:06 epoch 0 of 100 Train loss: 0.7316 Train f_score 0.4766 Xval loss: 0.7271 Xval f_score 0.6444\n",
      "00:43:06 epoch 10 of 100 Train loss: 0.7218 Train f_score 0.5817 Xval loss: 0.7196 Xval f_score 0.6604\n",
      "00:43:06 epoch 20 of 100 Train loss: 0.7148 Train f_score 0.5417 Xval loss: 0.7130 Xval f_score 0.6476\n",
      "00:43:06 epoch 30 of 100 Train loss: 0.7085 Train f_score 0.5487 Xval loss: 0.7080 Xval f_score 0.6476\n",
      "00:43:06 epoch 40 of 100 Train loss: 0.7045 Train f_score 0.5760 Xval loss: 0.7043 Xval f_score 0.6604\n",
      "00:43:06 epoch 50 of 100 Train loss: 0.7035 Train f_score 0.5749 Xval loss: 0.7014 Xval f_score 0.6604\n",
      "00:43:06 epoch 60 of 100 Train loss: 0.6872 Train f_score 0.6527 Xval loss: 0.6992 Xval f_score 0.6604\n",
      "00:43:06 epoch 70 of 100 Train loss: 0.7018 Train f_score 0.5760 Xval loss: 0.6977 Xval f_score 0.6604\n",
      "00:43:06 epoch 80 of 100 Train loss: 0.6977 Train f_score 0.5492 Xval loss: 0.6964 Xval f_score 0.6604\n",
      "00:43:06 epoch 90 of 100 Train loss: 0.6875 Train f_score 0.6167 Xval loss: 0.6957 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.695466\n",
      "NN units 32\n",
      "Reg_penalty 0.00100000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.571, Train F1 0.685\n",
      "[[22  9]\n",
      " [81 98]]\n",
      "Final Xval Accuracy 0.620, Xval F1 0.675\n",
      "[[16  7]\n",
      " [20 28]]\n",
      "00:43:10 Starting\n",
      "00:43:13 epoch 0 of 100 Train loss: 0.8061 Train f_score 0.6752 Xval loss: 0.8106 Xval f_score 0.6604\n",
      "00:43:13 epoch 10 of 100 Train loss: 0.7739 Train f_score 0.6758 Xval loss: 0.7776 Xval f_score 0.6604\n",
      "00:43:13 epoch 20 of 100 Train loss: 0.7562 Train f_score 0.5763 Xval loss: 0.7534 Xval f_score 0.6604\n",
      "00:43:13 epoch 30 of 100 Train loss: 0.7383 Train f_score 0.5463 Xval loss: 0.7356 Xval f_score 0.6383\n",
      "00:43:13 epoch 40 of 100 Train loss: 0.7253 Train f_score 0.5098 Xval loss: 0.7225 Xval f_score 0.6604\n",
      "00:43:13 epoch 50 of 100 Train loss: 0.7100 Train f_score 0.6066 Xval loss: 0.7130 Xval f_score 0.6604\n",
      "00:43:13 epoch 60 of 100 Train loss: 0.7017 Train f_score 0.5749 Xval loss: 0.7063 Xval f_score 0.6604\n",
      "00:43:13 epoch 70 of 100 Train loss: 0.7009 Train f_score 0.5891 Xval loss: 0.7015 Xval f_score 0.6604\n",
      "00:43:13 epoch 80 of 100 Train loss: 0.7041 Train f_score 0.5148 Xval loss: 0.6984 Xval f_score 0.6604\n",
      "00:43:13 epoch 90 of 100 Train loss: 0.6966 Train f_score 0.6235 Xval loss: 0.6968 Xval f_score 0.6604\n",
      "Best Xval loss epoch 99, value 0.696109\n",
      "NN units 32\n",
      "Reg_penalty 0.00300000\n",
      "Dropout 0.5000\n",
      "Activation sigmoid\n",
      "Final Train Accuracy 0.567, Train F1 0.689\n",
      "[[ 18   6]\n",
      " [ 85 101]]\n",
      "Final Xval Accuracy 0.577, Xval F1 0.545\n",
      "[[23 17]\n",
      " [13 18]]\n",
      "00:43:17 Finishing\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter options to try\n",
    "hidden_layer_hp = [4, 8, 16, 32]\n",
    "dropout_hp = [0, 0.333, 0.5]\n",
    "reg_penalty_hp = [0, 0.0001, 0.0003, 0.001, 0.003]\n",
    "activation_hp = ['relu', 'sigmoid']\n",
    "\n",
    "fscores = {}\n",
    "\n",
    "for dr in dropout_hp:\n",
    "    for ac in activation_hp:\n",
    "        for hl in hidden_layer_hp:\n",
    "            for rp in reg_penalty_hp:\n",
    "                # print(\"\\n %s\\n\" % (time.strftime(\"%H:%M:%S\")))               \n",
    "\n",
    "                model = create_model(num_components=num_features,\n",
    "                                     hidden_layer_size=hl, \n",
    "                                     dropout=dr, \n",
    "                                     reg_penalty=rp,\n",
    "                                     activation=ac)\n",
    "                \n",
    "                models = []\n",
    "                losses = []\n",
    "    \n",
    "    \n",
    "                epochs = 100 # increase this if Xval_loss doesn't always reach a minimum in this many epochs\n",
    "                print('%s Starting' % time.strftime(\"%H:%M:%S\"))             \n",
    "                \n",
    "                for i in range(epochs):\n",
    "                    fit = model.fit(X_train, y_train, validation_data=(X_xval, y_xval), \n",
    "                                    epochs=1, \n",
    "                                    batch_size=X_train.shape[0], # small data, full batch, no sgd \n",
    "                                    verbose=False)\n",
    "                    train_loss = fit.history['loss'][-1]\n",
    "                    train_acc = fit.history['acc'][-1]\n",
    "                    train_f_score = fit.history['f_score'][-1]\n",
    "                    current_loss = fit.history['val_loss'][-1]\n",
    "                    current_acc = fit.history['val_acc'][-1]\n",
    "                    current_f_score = fit.history['val_f_score'][-1]\n",
    "                    if i % 10 == 0:\n",
    "                        print('%s epoch %d of %d Train loss: %.4f Train f_score %.4f Xval loss: %.4f Xval f_score %.4f' % \n",
    "                              (time.strftime(\"%H:%M:%S\"), i, epochs,\n",
    "                               train_loss, train_f_score,\n",
    "                               current_loss, current_f_score))\n",
    "                    \n",
    "                    losses.append(current_loss)\n",
    "                    models.append(copy.copy(model))\n",
    "                    \n",
    "                    bestloss_index = np.argmin(losses)\n",
    "                    bestloss_value = losses[bestloss_index]\n",
    "                    # stop if loss rises by 10% from best\n",
    "                    if current_loss / bestloss_value > 1.1:\n",
    "                        break\n",
    "                    \n",
    "                # keep model from epoch with best xval loss\n",
    "                print (\"Best Xval loss epoch %d, value %f\" % (bestloss_index, bestloss_value))\n",
    "                model = models[bestloss_index]\n",
    "\n",
    "                # evaluate model\n",
    "                print (\"NN units %d\" % hl)\n",
    "                print (\"Reg_penalty %.8f\" % rp)\n",
    "                print (\"Dropout %.4f\" %  dr)\n",
    "                print (\"Activation %s\" %  ac)\n",
    "                \n",
    "                y_train_prob = model.predict(X_train)\n",
    "                \n",
    "                thresh, score = selectThresholdAcc(y_train_prob, y_train)\n",
    "                y_train_pred = y_train_prob >= thresh\n",
    "                \n",
    "                print(\"Final Train Accuracy %.3f, Train F1 %.3f\" % \n",
    "                      (sklearn.metrics.accuracy_score(y_train_pred, y_train), \n",
    "                       sklearn.metrics.f1_score(y_train_pred, y_train)))\n",
    "                \n",
    "                print(sklearn.metrics.confusion_matrix(y_train_pred, y_train))\n",
    "\n",
    "                y_xval_prob = model.predict(X_xval)\n",
    "                \n",
    "                thresh, score = selectThresholdAcc(y_xval_prob, y_xval)\n",
    "                y_xval_pred = y_xval_prob >= thresh\n",
    "                \n",
    "                print(\"Final Xval Accuracy %.3f, Xval F1 %.3f\" % \n",
    "                      (sklearn.metrics.accuracy_score(y_xval_pred, y_xval), \n",
    "                       sklearn.metrics.f1_score(y_xval_pred, y_xval)))\n",
    "                                \n",
    "                confusion_matrix = sklearn.metrics.confusion_matrix(y_xval_pred, y_xval)\n",
    "                print(confusion_matrix)\n",
    "                true_negative = confusion_matrix[0][0]\n",
    "                false_negative = confusion_matrix[0][1]\n",
    "                false_positive = confusion_matrix[1][0]\n",
    "                true_positive = confusion_matrix[1][1]\n",
    "\n",
    "                fscores[(hl, rp, dr, ac)] = score\n",
    "\n",
    "                # save model to disk\n",
    "                modelname = \"model_%s_%d_%.3f_%.6f\" % (ac, hl, dr, rp)\n",
    "                model.save(\"%s.h5\" % modelname)\n",
    "                model.save_weights(\"%s_weights.h5\" % modelname)\n",
    "                with open(\"%s.json\" % modelname, \"wb\") as fjson:\n",
    "                    fjson.write(model.to_json())                \n",
    "\n",
    "print('%s Finishing' % time.strftime(\"%H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATAAAAD+CAYAAAC9blmDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvt0K8xkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3773225a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dropout 0.000000 Activation: relu \n",
      "array([[ 0.5915493 ,  0.54929577,  0.6056338 ,  0.63380282,  0.57746479],\n",
      "       [ 0.5915493 ,  0.57746479,  0.5915493 ,  0.57746479,  0.54929577],\n",
      "       [ 0.54929577,  0.56338028,  0.54929577,  0.54929577,  0.57746479],\n",
      "       [ 0.56338028,  0.53521127,  0.53521127,  0.53521127,  0.56338028]])\n",
      "\n",
      "Max: 0.633803\n",
      "Avg: 0.568310\n",
      "Col Avg: array([ 0.57394366,  0.55633803,  0.57042254,  0.57394366,  0.56690141])\n",
      "Row Avg: array([ 0.5915493 ,  0.57746479,  0.55774648,  0.54647887])\n",
      "\n",
      "\n",
      "Dropout 0.000000 Activation: sigmoid \n",
      "array([[ 0.56338028,  0.57746479,  0.57746479,  0.53521127,  0.64788732],\n",
      "       [ 0.57746479,  0.57746479,  0.54929577,  0.54929577,  0.54929577],\n",
      "       [ 0.56338028,  0.6056338 ,  0.57746479,  0.57746479,  0.54929577],\n",
      "       [ 0.57746479,  0.5915493 ,  0.6056338 ,  0.54929577,  0.57746479]])\n",
      "\n",
      "Max: 0.647887\n",
      "Avg: 0.573944\n",
      "Col Avg: array([ 0.57042254,  0.58802817,  0.57746479,  0.5528169 ,  0.58098592])\n",
      "Row Avg: array([ 0.58028169,  0.56056338,  0.57464789,  0.58028169])\n",
      "\n",
      "\n",
      "Dropout 0.333000 Activation: relu \n",
      "array([[ 0.54929577,  0.57746479,  0.54929577,  0.53521127,  0.57746479],\n",
      "       [ 0.6056338 ,  0.53521127,  0.57746479,  0.6056338 ,  0.6056338 ],\n",
      "       [ 0.54929577,  0.54929577,  0.54929577,  0.54929577,  0.5915493 ],\n",
      "       [ 0.56338028,  0.53521127,  0.54929577,  0.57746479,  0.56338028]])\n",
      "\n",
      "Max: 0.605634\n",
      "Avg: 0.564789\n",
      "Col Avg: array([ 0.56690141,  0.54929577,  0.55633803,  0.56690141,  0.58450704])\n",
      "Row Avg: array([ 0.55774648,  0.58591549,  0.55774648,  0.55774648])\n",
      "\n",
      "\n",
      "Dropout 0.333000 Activation: sigmoid \n",
      "array([[ 0.56338028,  0.56338028,  0.6056338 ,  0.61971831,  0.53521127],\n",
      "       [ 0.57746479,  0.57746479,  0.57746479,  0.6056338 ,  0.54929577],\n",
      "       [ 0.57746479,  0.56338028,  0.56338028,  0.57746479,  0.6056338 ],\n",
      "       [ 0.54929577,  0.56338028,  0.56338028,  0.53521127,  0.6056338 ]])\n",
      "\n",
      "Max: 0.619718\n",
      "Avg: 0.573944\n",
      "Col Avg: array([ 0.56690141,  0.56690141,  0.57746479,  0.58450704,  0.57394366])\n",
      "Row Avg: array([ 0.57746479,  0.57746479,  0.57746479,  0.56338028])\n",
      "\n",
      "\n",
      "Dropout 0.500000 Activation: relu \n",
      "array([[ 0.54929577,  0.5915493 ,  0.57746479,  0.5915493 ,  0.56338028],\n",
      "       [ 0.5915493 ,  0.5915493 ,  0.5915493 ,  0.61971831,  0.63380282],\n",
      "       [ 0.54929577,  0.56338028,  0.54929577,  0.56338028,  0.52112676],\n",
      "       [ 0.54929577,  0.54929577,  0.54929577,  0.57746479,  0.54929577]])\n",
      "\n",
      "Max: 0.633803\n",
      "Avg: 0.571127\n",
      "Col Avg: array([ 0.55985915,  0.57394366,  0.56690141,  0.58802817,  0.56690141])\n",
      "Row Avg: array([ 0.57464789,  0.6056338 ,  0.54929577,  0.55492958])\n",
      "\n",
      "\n",
      "Dropout 0.500000 Activation: sigmoid \n",
      "array([[ 0.5915493 ,  0.56338028,  0.54929577,  0.6056338 ,  0.56338028],\n",
      "       [ 0.57746479,  0.57746479,  0.57746479,  0.5915493 ,  0.57746479],\n",
      "       [ 0.56338028,  0.56338028,  0.56338028,  0.57746479,  0.56338028],\n",
      "       [ 0.57746479,  0.57746479,  0.54929577,  0.61971831,  0.57746479]])\n",
      "\n",
      "Max: 0.619718\n",
      "Avg: 0.575352\n",
      "Col Avg: array([ 0.57746479,  0.57042254,  0.55985915,  0.59859155,  0.57042254])\n",
      "Row Avg: array([ 0.57464789,  0.58028169,  0.56619718,  0.58028169])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#visualize evaluations in a grid to pick one\n",
    "#hidden_layer_hp = [4, 8, 16, 32]\n",
    "#dropout_hp = [0, 0.333, 0.5]\n",
    "#reg_penalty_hp = [0, 0.0001, 0.0003, 0.001, 0.003]\n",
    "#activation_hp = ['relu']\n",
    "\n",
    "f, subplots = plt.subplots(len(dropout_hp), len(activation_hp))\n",
    "\n",
    "outstr = \"\"\n",
    "\n",
    "rownum=0\n",
    "for dr in dropout_hp:\n",
    "    colnum = 0\n",
    "    for ac in activation_hp:\n",
    "        matrix1 = []\n",
    "        for hl in hidden_layer_hp:\n",
    "            row = []\n",
    "            for reg_penalty in reg_penalty_hp:\n",
    "                try:\n",
    "                    row.append(fscores[(hl, reg_penalty, dr, ac)])\n",
    "                except KeyError:\n",
    "                    row.append(0.00)\n",
    "            matrix1.append(row)\n",
    "            \n",
    "        outstr += \"\\n\\nDropout %f Activation: %s \\n\" % (dr, ac)\n",
    "        outstr += repr(np.array(matrix1))\n",
    "        outstr += \"\\n\\nMax: %f\\n\" % (np.max(np.array(matrix1)))\n",
    "        outstr += \"Avg: %f\\n\" % (np.average(np.array(matrix1)))\n",
    "        outstr += \"Col Avg: %s\\n\" % repr(np.average(np.array(matrix1), axis=0))\n",
    "        outstr += \"Row Avg: %s\\n\" % repr(np.average(np.array(matrix1), axis=1))\n",
    "\n",
    "        subplots[rownum, colnum].matshow(matrix1, cmap='hot', vmin=0.5, vmax=0.7, interpolation='nearest')\n",
    "        subplots[rownum, colnum].set_title('%s %f' % (ac, dr))\n",
    "\n",
    "        colnum += 1  \n",
    "    rownum += 1\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(outstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# pick this model, load from file\n",
    "ac='sigmoid'\n",
    "hl = 8\n",
    "dr = 0.5\n",
    "rp = 0.001\n",
    "modelname = \"model_%s_%d_%.3f_%.6f\" % (ac, hl, dr, rp)\n",
    "\n",
    "# doesn't work because of some custom metric BS\n",
    "#keras.models.load_model(\"%s.h5\" % modelname)\n",
    "from keras.models import model_from_json\n",
    "\n",
    "with open(\"%s.json\" % modelname, 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"%s_weights.h5\" % modelname)\n",
    "print(\"Loaded model from disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Xval Accuracy 0.592, Xval F1 0.603, f_score 0.592\n",
      "[[20 13]\n",
      " [16 22]]\n"
     ]
    }
   ],
   "source": [
    "# re-evaluate in xval to confirm, pick threshold value\n",
    "y_xval_prob = model.predict(X_xval)\n",
    "                \n",
    "thresh, score = selectThresholdAcc(y_xval_prob, y_xval)\n",
    "y_xval_pred = y_xval_prob >= thresh\n",
    "                \n",
    "print(\"Final Xval Accuracy %.3f, Xval F1 %.3f, f_score %.3f\" % \n",
    "      (sklearn.metrics.accuracy_score(y_xval_pred, y_xval), \n",
    "       sklearn.metrics.f1_score(y_xval_pred, y_xval),\n",
    "       score))\n",
    "                                \n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_xval_pred, y_xval)\n",
    "print(confusion_matrix)\n",
    "false_positive = confusion_matrix[1][0]\n",
    "false_negative = confusion_matrix[0][1]\n",
    "true_positive = confusion_matrix[1][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot import classifier_factory\n",
    "classifier_factory(model)\n",
    "model.plot_roc_curve(X_xval, y_xval, random_state=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 0.535, Test F1 0.602\n",
      "[[13  9]\n",
      " [24 25]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate in test set\n",
    "# xval not a good measure of expected performance since we used it to pick threshold\n",
    "# also tested many times in xval and picked best model, which is probably model that's lucky in xval\n",
    "y_test_prob = model.predict(X_test)\n",
    "\n",
    "y_test_pred = y_test_prob >= thresh\n",
    "\n",
    "print(\"Test Accuracy %.3f, Test F1 %.3f\" % \n",
    "      (sklearn.metrics.accuracy_score(y_test_pred, y_test), \n",
    "       sklearn.metrics.f1_score(y_test_pred, y_test)))\n",
    "\n",
    "print(sklearn.metrics.confusion_matrix(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"finalmodel.h5\")\n",
    "model.save_weights(\"modelweights.h5\")\n",
    "\n",
    "with open(\"finalmodel.json\", \"wb\") as fjson:\n",
    "    fjson.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slotnames__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_inbound_node',\n",
       " '_feed_input_names',\n",
       " '_feed_inputs',\n",
       " '_fit_loop',\n",
       " '_flattened_layers',\n",
       " '_gather_list_attr',\n",
       " '_get_node_attribute_at_index',\n",
       " '_initial_weights',\n",
       " '_make_predict_function',\n",
       " '_make_test_function',\n",
       " '_make_train_function',\n",
       " '_output_mask_cache',\n",
       " '_output_shape_cache',\n",
       " '_output_tensor_cache',\n",
       " '_predict_loop',\n",
       " '_standardize_user_data',\n",
       " '_test_loop',\n",
       " '_trainable',\n",
       " '_updated_config',\n",
       " 'add',\n",
       " 'add_loss',\n",
       " 'add_update',\n",
       " 'add_weight',\n",
       " 'assert_input_compatibility',\n",
       " 'build',\n",
       " 'built',\n",
       " 'call',\n",
       " 'compile',\n",
       " 'compute_mask',\n",
       " 'compute_output_shape',\n",
       " 'constraints',\n",
       " 'container_nodes',\n",
       " 'count_params',\n",
       " 'evaluate',\n",
       " 'evaluate_generator',\n",
       " 'fit',\n",
       " 'fit_generator',\n",
       " 'from_config',\n",
       " 'get_config',\n",
       " 'get_input_at',\n",
       " 'get_input_mask_at',\n",
       " 'get_input_shape_at',\n",
       " 'get_layer',\n",
       " 'get_losses_for',\n",
       " 'get_output_at',\n",
       " 'get_output_mask_at',\n",
       " 'get_output_shape_at',\n",
       " 'get_updates_for',\n",
       " 'get_weights',\n",
       " 'inbound_nodes',\n",
       " 'input',\n",
       " 'input_layers',\n",
       " 'input_layers_node_indices',\n",
       " 'input_layers_tensor_indices',\n",
       " 'input_mask',\n",
       " 'input_names',\n",
       " 'input_shape',\n",
       " 'input_spec',\n",
       " 'inputs',\n",
       " 'layers',\n",
       " 'legacy_from_config',\n",
       " 'legacy_get_config',\n",
       " 'load_weights',\n",
       " 'loss',\n",
       " 'loss_weights',\n",
       " 'losses',\n",
       " 'metrics',\n",
       " 'metrics_names',\n",
       " 'metrics_tensors',\n",
       " 'model',\n",
       " 'name',\n",
       " 'nodes_by_depth',\n",
       " 'non_trainable_weights',\n",
       " 'optimizer',\n",
       " 'outbound_nodes',\n",
       " 'output',\n",
       " 'output_layers',\n",
       " 'output_layers_node_indices',\n",
       " 'output_layers_tensor_indices',\n",
       " 'output_mask',\n",
       " 'output_names',\n",
       " 'output_shape',\n",
       " 'outputs',\n",
       " 'pop',\n",
       " 'predict',\n",
       " 'predict_classes',\n",
       " 'predict_generator',\n",
       " 'predict_on_batch',\n",
       " 'predict_proba',\n",
       " 'regularizers',\n",
       " 'reset_states',\n",
       " 'run_internal_graph',\n",
       " 'sample_weight_mode',\n",
       " 'save',\n",
       " 'save_weights',\n",
       " 'set_weights',\n",
       " 'state_updates',\n",
       " 'stateful',\n",
       " 'stop_training',\n",
       " 'summary',\n",
       " 'supports_masking',\n",
       " 'test_on_batch',\n",
       " 'to_json',\n",
       " 'to_yaml',\n",
       " 'train_on_batch',\n",
       " 'trainable',\n",
       " 'trainable_weights',\n",
       " 'updates',\n",
       " 'uses_learning_phase',\n",
       " 'weights']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(grid.best_estimator_.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
